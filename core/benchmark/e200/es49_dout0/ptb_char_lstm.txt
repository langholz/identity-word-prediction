Loaded corpus from 'ptb_char_corpus.pkl'...
['<eos>', 'N', '<unk>', 'a', 'e', 'r', ' ', 'b', 'n', 'k', 'o', 't', 'l', 'i', 'z', 'c', 'w', 'y', 'u', 's', 'f', 'm', 'g', 'h', 'd', '-', 'q', 'p', 'x', 'j', 'v', '.', "'", '1', '9', '5', '0', '&', '$', '3', '2', '4', '8', '6', '7', '#', '\\', '/', '*']

[batch =  100] train_perplexity =    24.53, train_loss =  3.20, learning_rate = 20.00000000


[batch =  200] train_perplexity =    21.15, train_loss =  3.05, learning_rate = 20.00000000


[batch =  300] train_perplexity =    14.38, train_loss =  2.67, learning_rate = 20.00000000


[batch =  400] train_perplexity =     9.11, train_loss =  2.21, learning_rate = 20.00000000


[batch =  500] train_perplexity =     7.22, train_loss =  1.98, learning_rate = 20.00000000


[batch =  600] train_perplexity =     6.27, train_loss =  1.84, learning_rate = 20.00000000


[batch =  700] train_perplexity =     5.75, train_loss =  1.75, learning_rate = 20.00000000


[batch =  800] train_perplexity =     5.68, train_loss =  1.74, learning_rate = 20.00000000


[batch =  900] train_perplexity =     5.50, train_loss =  1.70, learning_rate = 20.00000000


[batch = 1000] train_perplexity =     5.51, train_loss =  1.71, learning_rate = 20.00000000


[batch = 1100] train_perplexity =     5.39, train_loss =  1.68, learning_rate = 20.00000000


[batch = 1200] train_perplexity =     5.21, train_loss =  1.65, learning_rate = 20.00000000


[batch = 1300] train_perplexity =     5.08, train_loss =  1.63, learning_rate = 20.00000000


[batch = 1400] train_perplexity =     5.01, train_loss =  1.61, learning_rate = 20.00000000


[batch = 1500] train_perplexity =     4.84, train_loss =  1.58, learning_rate = 20.00000000


[batch = 1600] train_perplexity =     4.86, train_loss =  1.58, learning_rate = 20.00000000


[batch = 1700] train_perplexity =     4.80, train_loss =  1.57, learning_rate = 20.00000000


[batch = 1800] train_perplexity =     4.80, train_loss =  1.57, learning_rate = 20.00000000


[batch = 1900] train_perplexity =     4.79, train_loss =  1.57, learning_rate = 20.00000000


[batch = 2000] train_perplexity =     4.92, train_loss =  1.59, learning_rate = 20.00000000


[batch = 2100] train_perplexity =     4.72, train_loss =  1.55, learning_rate = 20.00000000


[batch = 2200] train_perplexity =     4.82, train_loss =  1.57, learning_rate = 20.00000000


[batch = 2300] train_perplexity =     4.95, train_loss =  1.60, learning_rate = 20.00000000


[batch = 2400] train_perplexity =     4.79, train_loss =  1.57, learning_rate = 20.00000000


[batch = 2500] train_perplexity =     4.78, train_loss =  1.57, learning_rate = 20.00000000


[batch = 2600] train_perplexity =     4.76, train_loss =  1.56, learning_rate = 20.00000000


[batch = 2700] train_perplexity =     4.77, train_loss =  1.56, learning_rate = 20.00000000


[batch = 2800] train_perplexity =     4.76, train_loss =  1.56, learning_rate = 20.00000000


[batch = 2900] train_perplexity =     4.69, train_loss =  1.54, learning_rate = 20.00000000


[batch = 3000] train_perplexity =     4.57, train_loss =  1.52, learning_rate = 20.00000000


[batch = 3100] train_perplexity =     4.73, train_loss =  1.55, learning_rate = 20.00000000


[batch = 3200] train_perplexity =     4.83, train_loss =  1.57, learning_rate = 20.00000000


[batch = 3300] train_perplexity =     4.73, train_loss =  1.55, learning_rate = 20.00000000


[batch = 3400] train_perplexity =     4.53, train_loss =  1.51, learning_rate = 20.00000000


[batch = 3500] train_perplexity =     4.66, train_loss =  1.54, learning_rate = 20.00000000


[batch = 3600] train_perplexity =     4.66, train_loss =  1.54, learning_rate = 20.00000000


[batch = 3700] train_perplexity =     4.70, train_loss =  1.55, learning_rate = 20.00000000


[batch = 3800] train_perplexity =     4.64, train_loss =  1.54, learning_rate = 20.00000000


[batch = 3900] train_perplexity =     4.57, train_loss =  1.52, learning_rate = 20.00000000


[batch = 4000] train_perplexity =     4.67, train_loss =  1.54, learning_rate = 20.00000000


[batch = 4100] train_perplexity =     4.46, train_loss =  1.49, learning_rate = 20.00000000


[batch = 4200] train_perplexity =     4.55, train_loss =  1.51, learning_rate = 20.00000000


[batch = 4300] train_perplexity =     4.45, train_loss =  1.49, learning_rate = 20.00000000


[batch = 4400] train_perplexity =     4.58, train_loss =  1.52, learning_rate = 20.00000000


[batch = 4500] train_perplexity =     4.53, train_loss =  1.51, learning_rate = 20.00000000


[batch = 4600] train_perplexity =     4.52, train_loss =  1.51, learning_rate = 20.00000000


[batch = 4700] train_perplexity =     4.67, train_loss =  1.54, learning_rate = 20.00000000


[batch = 4800] train_perplexity =     4.57, train_loss =  1.52, learning_rate = 20.00000000


[batch = 4900] train_perplexity =     4.61, train_loss =  1.53, learning_rate = 20.00000000


[batch = 5000] train_perplexity =     4.53, train_loss =  1.51, learning_rate = 20.00000000


[batch = 5100] train_perplexity =     4.62, train_loss =  1.53, learning_rate = 20.00000000


[batch = 5200] train_perplexity =     4.66, train_loss =  1.54, learning_rate = 20.00000000


[batch = 5300] train_perplexity =     4.66, train_loss =  1.54, learning_rate = 20.00000000


[batch = 5400] train_perplexity =     4.57, train_loss =  1.52, learning_rate = 20.00000000


[batch = 5500] train_perplexity =     4.65, train_loss =  1.54, learning_rate = 20.00000000


[batch = 5600] train_perplexity =     4.66, train_loss =  1.54, learning_rate = 20.00000000


[batch = 5700] train_perplexity =     4.36, train_loss =  1.47, learning_rate = 20.00000000


[batch = 5800] train_perplexity =     4.45, train_loss =  1.49, learning_rate = 20.00000000


[batch = 5900] train_perplexity =     4.58, train_loss =  1.52, learning_rate = 20.00000000


[batch = 6000] train_perplexity =     4.57, train_loss =  1.52, learning_rate = 20.00000000


[batch = 6100] train_perplexity =     4.58, train_loss =  1.52, learning_rate = 20.00000000


[batch = 6200] train_perplexity =     4.52, train_loss =  1.51, learning_rate = 20.00000000


[batch = 6300] train_perplexity =     4.56, train_loss =  1.52, learning_rate = 20.00000000


[batch = 6400] train_perplexity =     4.55, train_loss =  1.52, learning_rate = 20.00000000


[batch = 6500] train_perplexity =     4.40, train_loss =  1.48, learning_rate = 20.00000000


[batch = 6600] train_perplexity =     4.59, train_loss =  1.52, learning_rate = 20.00000000


[batch = 6700] train_perplexity =     4.50, train_loss =  1.50, learning_rate = 20.00000000


[batch = 6800] train_perplexity =     4.54, train_loss =  1.51, learning_rate = 20.00000000


[batch = 6900] train_perplexity =     4.62, train_loss =  1.53, learning_rate = 20.00000000


[epoch =   1] validation_perplexity =     4.92, validation_loss =  1.59

Saved model to 'e200/ptb_char_lstm.pt'...
[epoch =   1] min_validation_perplexity =     4.92, min_validation_loss =  1.59


[batch =  100] train_perplexity =     4.92, train_loss =  1.59, learning_rate = 20.00000000


[batch =  200] train_perplexity =     4.62, train_loss =  1.53, learning_rate = 20.00000000


[batch =  300] train_perplexity =     4.52, train_loss =  1.51, learning_rate = 20.00000000


[batch =  400] train_perplexity =     4.32, train_loss =  1.46, learning_rate = 20.00000000


[batch =  500] train_perplexity =     4.38, train_loss =  1.48, learning_rate = 20.00000000


[batch =  600] train_perplexity =     4.30, train_loss =  1.46, learning_rate = 20.00000000


[batch =  700] train_perplexity =     4.26, train_loss =  1.45, learning_rate = 20.00000000


[batch =  800] train_perplexity =     4.62, train_loss =  1.53, learning_rate = 20.00000000


[batch =  900] train_perplexity =     4.54, train_loss =  1.51, learning_rate = 20.00000000


[batch = 1000] train_perplexity =     4.71, train_loss =  1.55, learning_rate = 20.00000000


[batch = 1100] train_perplexity =     4.73, train_loss =  1.55, learning_rate = 20.00000000


[batch = 1200] train_perplexity =     4.60, train_loss =  1.53, learning_rate = 20.00000000


[batch = 1300] train_perplexity =     4.53, train_loss =  1.51, learning_rate = 20.00000000


[batch = 1400] train_perplexity =     4.56, train_loss =  1.52, learning_rate = 20.00000000


[batch = 1500] train_perplexity =     4.56, train_loss =  1.52, learning_rate = 20.00000000


[batch = 1600] train_perplexity =     4.51, train_loss =  1.51, learning_rate = 20.00000000


[batch = 1700] train_perplexity =     4.50, train_loss =  1.50, learning_rate = 20.00000000


[batch = 1800] train_perplexity =     4.56, train_loss =  1.52, learning_rate = 20.00000000


[batch = 1900] train_perplexity =     4.60, train_loss =  1.53, learning_rate = 20.00000000


[batch = 2000] train_perplexity =     4.70, train_loss =  1.55, learning_rate = 20.00000000


[batch = 2100] train_perplexity =     4.52, train_loss =  1.51, learning_rate = 20.00000000


[batch = 2200] train_perplexity =     4.61, train_loss =  1.53, learning_rate = 20.00000000


[batch = 2300] train_perplexity =     4.74, train_loss =  1.56, learning_rate = 20.00000000


[batch = 2400] train_perplexity =     4.54, train_loss =  1.51, learning_rate = 20.00000000


[batch = 2500] train_perplexity =     4.63, train_loss =  1.53, learning_rate = 20.00000000


[batch = 2600] train_perplexity =     4.62, train_loss =  1.53, learning_rate = 20.00000000


[batch = 2700] train_perplexity =     4.72, train_loss =  1.55, learning_rate = 20.00000000


[batch = 2800] train_perplexity =     4.63, train_loss =  1.53, learning_rate = 20.00000000


[batch = 2900] train_perplexity =     4.63, train_loss =  1.53, learning_rate = 20.00000000


[batch = 3000] train_perplexity =     4.48, train_loss =  1.50, learning_rate = 20.00000000


[batch = 3100] train_perplexity =     4.65, train_loss =  1.54, learning_rate = 20.00000000


[batch = 3200] train_perplexity =     4.80, train_loss =  1.57, learning_rate = 20.00000000


[batch = 3300] train_perplexity =     4.64, train_loss =  1.54, learning_rate = 20.00000000


[batch = 3400] train_perplexity =     4.52, train_loss =  1.51, learning_rate = 20.00000000


[batch = 3500] train_perplexity =     4.53, train_loss =  1.51, learning_rate = 20.00000000


[batch = 3600] train_perplexity =     4.65, train_loss =  1.54, learning_rate = 20.00000000


[batch = 3700] train_perplexity =     4.67, train_loss =  1.54, learning_rate = 20.00000000


[batch = 3800] train_perplexity =     4.54, train_loss =  1.51, learning_rate = 20.00000000


[batch = 3900] train_perplexity =     4.47, train_loss =  1.50, learning_rate = 20.00000000


[batch = 4000] train_perplexity =     4.61, train_loss =  1.53, learning_rate = 20.00000000


[batch = 4100] train_perplexity =     4.49, train_loss =  1.50, learning_rate = 20.00000000


[batch = 4200] train_perplexity =     4.51, train_loss =  1.51, learning_rate = 20.00000000


[batch = 4300] train_perplexity =     4.50, train_loss =  1.50, learning_rate = 20.00000000


[batch = 4400] train_perplexity =     4.67, train_loss =  1.54, learning_rate = 20.00000000


[batch = 4500] train_perplexity =     4.61, train_loss =  1.53, learning_rate = 20.00000000


[batch = 4600] train_perplexity =     4.55, train_loss =  1.52, learning_rate = 20.00000000


[batch = 4700] train_perplexity =     4.61, train_loss =  1.53, learning_rate = 20.00000000


[batch = 4800] train_perplexity =     4.62, train_loss =  1.53, learning_rate = 20.00000000


[batch = 4900] train_perplexity =     4.73, train_loss =  1.55, learning_rate = 20.00000000


[batch = 5000] train_perplexity =     4.57, train_loss =  1.52, learning_rate = 20.00000000


[batch = 5100] train_perplexity =     4.60, train_loss =  1.53, learning_rate = 20.00000000


[batch = 5200] train_perplexity =     4.64, train_loss =  1.53, learning_rate = 20.00000000


[batch = 5300] train_perplexity =     4.62, train_loss =  1.53, learning_rate = 20.00000000


[batch = 5400] train_perplexity =     4.54, train_loss =  1.51, learning_rate = 20.00000000


[batch = 5500] train_perplexity =     4.66, train_loss =  1.54, learning_rate = 20.00000000


[batch = 5600] train_perplexity =     4.72, train_loss =  1.55, learning_rate = 20.00000000


[batch = 5700] train_perplexity =     4.38, train_loss =  1.48, learning_rate = 20.00000000


[batch = 5800] train_perplexity =     4.55, train_loss =  1.52, learning_rate = 20.00000000


[batch = 5900] train_perplexity =     4.64, train_loss =  1.54, learning_rate = 20.00000000


[batch = 6000] train_perplexity =     4.57, train_loss =  1.52, learning_rate = 20.00000000


[batch = 6100] train_perplexity =     4.59, train_loss =  1.52, learning_rate = 20.00000000


[batch = 6200] train_perplexity =     4.58, train_loss =  1.52, learning_rate = 20.00000000


[batch = 6300] train_perplexity =     4.63, train_loss =  1.53, learning_rate = 20.00000000


[batch = 6400] train_perplexity =     4.67, train_loss =  1.54, learning_rate = 20.00000000


[batch = 6500] train_perplexity =     4.59, train_loss =  1.52, learning_rate = 20.00000000


[batch = 6600] train_perplexity =     4.72, train_loss =  1.55, learning_rate = 20.00000000


[batch = 6700] train_perplexity =     4.59, train_loss =  1.52, learning_rate = 20.00000000


[batch = 6800] train_perplexity =     4.61, train_loss =  1.53, learning_rate = 20.00000000


[batch = 6900] train_perplexity =     4.79, train_loss =  1.57, learning_rate = 20.00000000


[epoch =   2] validation_perplexity =     4.83, validation_loss =  1.57

Saved model to 'e200/ptb_char_lstm.pt'...
[epoch =   2] min_validation_perplexity =     4.83, min_validation_loss =  1.57


[batch =  100] train_perplexity =     5.08, train_loss =  1.63, learning_rate = 20.00000000


[batch =  200] train_perplexity =     4.70, train_loss =  1.55, learning_rate = 20.00000000


[batch =  300] train_perplexity =     4.63, train_loss =  1.53, learning_rate = 20.00000000


[batch =  400] train_perplexity =     4.51, train_loss =  1.51, learning_rate = 20.00000000


[batch =  500] train_perplexity =     4.51, train_loss =  1.51, learning_rate = 20.00000000


[batch =  600] train_perplexity =     4.49, train_loss =  1.50, learning_rate = 20.00000000


[batch =  700] train_perplexity =     4.48, train_loss =  1.50, learning_rate = 20.00000000


[batch =  800] train_perplexity =     4.68, train_loss =  1.54, learning_rate = 20.00000000


[batch =  900] train_perplexity =     4.59, train_loss =  1.52, learning_rate = 20.00000000


[batch = 1000] train_perplexity =     4.75, train_loss =  1.56, learning_rate = 20.00000000


[batch = 1100] train_perplexity =     4.85, train_loss =  1.58, learning_rate = 20.00000000


[batch = 1200] train_perplexity =     4.70, train_loss =  1.55, learning_rate = 20.00000000


[batch = 1300] train_perplexity =     4.62, train_loss =  1.53, learning_rate = 20.00000000


[batch = 1400] train_perplexity =     4.59, train_loss =  1.52, learning_rate = 20.00000000


[batch = 1500] train_perplexity =     4.49, train_loss =  1.50, learning_rate = 20.00000000


[batch = 1600] train_perplexity =     4.55, train_loss =  1.52, learning_rate = 20.00000000


[batch = 1700] train_perplexity =     4.56, train_loss =  1.52, learning_rate = 20.00000000


[batch = 1800] train_perplexity =     4.68, train_loss =  1.54, learning_rate = 20.00000000


[batch = 1900] train_perplexity =     4.66, train_loss =  1.54, learning_rate = 20.00000000


[batch = 2000] train_perplexity =     4.81, train_loss =  1.57, learning_rate = 20.00000000


[batch = 2100] train_perplexity =     4.52, train_loss =  1.51, learning_rate = 20.00000000


[batch = 2200] train_perplexity =     4.64, train_loss =  1.53, learning_rate = 20.00000000


[batch = 2300] train_perplexity =     4.94, train_loss =  1.60, learning_rate = 20.00000000


[batch = 2400] train_perplexity =     4.81, train_loss =  1.57, learning_rate = 20.00000000


[batch = 2500] train_perplexity =     4.78, train_loss =  1.56, learning_rate = 20.00000000


[batch = 2600] train_perplexity =     4.81, train_loss =  1.57, learning_rate = 20.00000000


[batch = 2700] train_perplexity =     4.84, train_loss =  1.58, learning_rate = 20.00000000


[batch = 2800] train_perplexity =     4.80, train_loss =  1.57, learning_rate = 20.00000000


[batch = 2900] train_perplexity =     4.69, train_loss =  1.55, learning_rate = 20.00000000


[batch = 3000] train_perplexity =     4.59, train_loss =  1.52, learning_rate = 20.00000000


[batch = 3100] train_perplexity =     4.90, train_loss =  1.59, learning_rate = 20.00000000


[batch = 3200] train_perplexity =     5.06, train_loss =  1.62, learning_rate = 20.00000000


[batch = 3300] train_perplexity =     4.97, train_loss =  1.60, learning_rate = 20.00000000


[batch = 3400] train_perplexity =     4.72, train_loss =  1.55, learning_rate = 20.00000000


[batch = 3500] train_perplexity =     4.90, train_loss =  1.59, learning_rate = 20.00000000


[batch = 3600] train_perplexity =     4.87, train_loss =  1.58, learning_rate = 20.00000000


[batch = 3700] train_perplexity =     4.82, train_loss =  1.57, learning_rate = 20.00000000


[batch = 3800] train_perplexity =     4.85, train_loss =  1.58, learning_rate = 20.00000000


[batch = 3900] train_perplexity =     4.89, train_loss =  1.59, learning_rate = 20.00000000


[batch = 4000] train_perplexity =     4.88, train_loss =  1.59, learning_rate = 20.00000000


[batch = 4100] train_perplexity =     4.63, train_loss =  1.53, learning_rate = 20.00000000


[batch = 4200] train_perplexity =     4.73, train_loss =  1.55, learning_rate = 20.00000000


[batch = 4300] train_perplexity =     4.83, train_loss =  1.58, learning_rate = 20.00000000


[batch = 4400] train_perplexity =     4.75, train_loss =  1.56, learning_rate = 20.00000000


[batch = 4500] train_perplexity =     4.73, train_loss =  1.55, learning_rate = 20.00000000


[batch = 4600] train_perplexity =     4.71, train_loss =  1.55, learning_rate = 20.00000000


[batch = 4700] train_perplexity =     4.77, train_loss =  1.56, learning_rate = 20.00000000


[batch = 4800] train_perplexity =     4.72, train_loss =  1.55, learning_rate = 20.00000000


[batch = 4900] train_perplexity =     4.84, train_loss =  1.58, learning_rate = 20.00000000


[batch = 5000] train_perplexity =     4.70, train_loss =  1.55, learning_rate = 20.00000000


[batch = 5100] train_perplexity =     4.75, train_loss =  1.56, learning_rate = 20.00000000


[batch = 5200] train_perplexity =     4.78, train_loss =  1.56, learning_rate = 20.00000000


[batch = 5300] train_perplexity =     4.76, train_loss =  1.56, learning_rate = 20.00000000


[batch = 5400] train_perplexity =     4.53, train_loss =  1.51, learning_rate = 20.00000000


[batch = 5500] train_perplexity =     4.66, train_loss =  1.54, learning_rate = 20.00000000


[batch = 5600] train_perplexity =     4.67, train_loss =  1.54, learning_rate = 20.00000000


[batch = 5700] train_perplexity =     4.39, train_loss =  1.48, learning_rate = 20.00000000


[batch = 5800] train_perplexity =     4.62, train_loss =  1.53, learning_rate = 20.00000000


[batch = 5900] train_perplexity =     4.67, train_loss =  1.54, learning_rate = 20.00000000


[batch = 6000] train_perplexity =     4.72, train_loss =  1.55, learning_rate = 20.00000000


[batch = 6100] train_perplexity =     4.65, train_loss =  1.54, learning_rate = 20.00000000


[batch = 6200] train_perplexity =     4.60, train_loss =  1.53, learning_rate = 20.00000000


[batch = 6300] train_perplexity =     4.63, train_loss =  1.53, learning_rate = 20.00000000


[batch = 6400] train_perplexity =     4.60, train_loss =  1.53, learning_rate = 20.00000000


[batch = 6500] train_perplexity =     4.49, train_loss =  1.50, learning_rate = 20.00000000


[batch = 6600] train_perplexity =     4.76, train_loss =  1.56, learning_rate = 20.00000000


[batch = 6700] train_perplexity =     4.70, train_loss =  1.55, learning_rate = 20.00000000


[batch = 6800] train_perplexity =     4.65, train_loss =  1.54, learning_rate = 20.00000000


[batch = 6900] train_perplexity =     4.79, train_loss =  1.57, learning_rate = 20.00000000


[epoch =   3] validation_perplexity =     4.95, validation_loss =  1.60

[epoch =   3] annealing learning_rate = 5.00000000

[batch =  100] train_perplexity =     4.74, train_loss =  1.56, learning_rate = 5.00000000


[batch =  200] train_perplexity =     4.35, train_loss =  1.47, learning_rate = 5.00000000


[batch =  300] train_perplexity =     4.23, train_loss =  1.44, learning_rate = 5.00000000


[batch =  400] train_perplexity =     4.04, train_loss =  1.40, learning_rate = 5.00000000


[batch =  500] train_perplexity =     4.03, train_loss =  1.39, learning_rate = 5.00000000


[batch =  600] train_perplexity =     3.97, train_loss =  1.38, learning_rate = 5.00000000


[batch =  700] train_perplexity =     3.99, train_loss =  1.38, learning_rate = 5.00000000


[batch =  800] train_perplexity =     4.16, train_loss =  1.42, learning_rate = 5.00000000


[batch =  900] train_perplexity =     4.06, train_loss =  1.40, learning_rate = 5.00000000


[batch = 1000] train_perplexity =     4.21, train_loss =  1.44, learning_rate = 5.00000000


[batch = 1100] train_perplexity =     4.23, train_loss =  1.44, learning_rate = 5.00000000


[batch = 1200] train_perplexity =     4.11, train_loss =  1.41, learning_rate = 5.00000000


[batch = 1300] train_perplexity =     4.10, train_loss =  1.41, learning_rate = 5.00000000


[batch = 1400] train_perplexity =     4.02, train_loss =  1.39, learning_rate = 5.00000000


[batch = 1500] train_perplexity =     4.02, train_loss =  1.39, learning_rate = 5.00000000


[batch = 1600] train_perplexity =     3.99, train_loss =  1.38, learning_rate = 5.00000000


[batch = 1700] train_perplexity =     4.00, train_loss =  1.39, learning_rate = 5.00000000


[batch = 1800] train_perplexity =     4.01, train_loss =  1.39, learning_rate = 5.00000000


[batch = 1900] train_perplexity =     4.02, train_loss =  1.39, learning_rate = 5.00000000


[batch = 2000] train_perplexity =     4.13, train_loss =  1.42, learning_rate = 5.00000000


[batch = 2100] train_perplexity =     3.98, train_loss =  1.38, learning_rate = 5.00000000


[batch = 2200] train_perplexity =     4.03, train_loss =  1.39, learning_rate = 5.00000000


[batch = 2300] train_perplexity =     4.20, train_loss =  1.44, learning_rate = 5.00000000


[batch = 2400] train_perplexity =     4.04, train_loss =  1.40, learning_rate = 5.00000000


[batch = 2500] train_perplexity =     4.06, train_loss =  1.40, learning_rate = 5.00000000


[batch = 2600] train_perplexity =     4.08, train_loss =  1.41, learning_rate = 5.00000000


[batch = 2700] train_perplexity =     4.10, train_loss =  1.41, learning_rate = 5.00000000


[batch = 2800] train_perplexity =     4.01, train_loss =  1.39, learning_rate = 5.00000000


[batch = 2900] train_perplexity =     3.96, train_loss =  1.38, learning_rate = 5.00000000


[batch = 3000] train_perplexity =     3.90, train_loss =  1.36, learning_rate = 5.00000000


[batch = 3100] train_perplexity =     4.04, train_loss =  1.40, learning_rate = 5.00000000


[batch = 3200] train_perplexity =     4.16, train_loss =  1.43, learning_rate = 5.00000000


[batch = 3300] train_perplexity =     4.04, train_loss =  1.40, learning_rate = 5.00000000


[batch = 3400] train_perplexity =     3.94, train_loss =  1.37, learning_rate = 5.00000000


[batch = 3500] train_perplexity =     4.01, train_loss =  1.39, learning_rate = 5.00000000


[batch = 3600] train_perplexity =     4.02, train_loss =  1.39, learning_rate = 5.00000000


[batch = 3700] train_perplexity =     4.00, train_loss =  1.39, learning_rate = 5.00000000


[batch = 3800] train_perplexity =     3.99, train_loss =  1.38, learning_rate = 5.00000000


[batch = 3900] train_perplexity =     3.95, train_loss =  1.37, learning_rate = 5.00000000


[batch = 4000] train_perplexity =     4.00, train_loss =  1.39, learning_rate = 5.00000000


[batch = 4100] train_perplexity =     3.83, train_loss =  1.34, learning_rate = 5.00000000


[batch = 4200] train_perplexity =     3.92, train_loss =  1.37, learning_rate = 5.00000000


[batch = 4300] train_perplexity =     3.90, train_loss =  1.36, learning_rate = 5.00000000


[batch = 4400] train_perplexity =     3.93, train_loss =  1.37, learning_rate = 5.00000000


[batch = 4500] train_perplexity =     3.93, train_loss =  1.37, learning_rate = 5.00000000


[batch = 4600] train_perplexity =     3.92, train_loss =  1.37, learning_rate = 5.00000000


[batch = 4700] train_perplexity =     3.96, train_loss =  1.38, learning_rate = 5.00000000


[batch = 4800] train_perplexity =     3.96, train_loss =  1.38, learning_rate = 5.00000000


[batch = 4900] train_perplexity =     3.99, train_loss =  1.38, learning_rate = 5.00000000


[batch = 5000] train_perplexity =     3.95, train_loss =  1.37, learning_rate = 5.00000000


[batch = 5100] train_perplexity =     3.99, train_loss =  1.38, learning_rate = 5.00000000


[batch = 5200] train_perplexity =     4.00, train_loss =  1.39, learning_rate = 5.00000000


[batch = 5300] train_perplexity =     3.97, train_loss =  1.38, learning_rate = 5.00000000


[batch = 5400] train_perplexity =     3.88, train_loss =  1.36, learning_rate = 5.00000000


[batch = 5500] train_perplexity =     3.95, train_loss =  1.37, learning_rate = 5.00000000


[batch = 5600] train_perplexity =     3.97, train_loss =  1.38, learning_rate = 5.00000000


[batch = 5700] train_perplexity =     3.74, train_loss =  1.32, learning_rate = 5.00000000


[batch = 5800] train_perplexity =     3.85, train_loss =  1.35, learning_rate = 5.00000000


[batch = 5900] train_perplexity =     3.95, train_loss =  1.37, learning_rate = 5.00000000


[batch = 6000] train_perplexity =     3.92, train_loss =  1.37, learning_rate = 5.00000000


[batch = 6100] train_perplexity =     3.86, train_loss =  1.35, learning_rate = 5.00000000


[batch = 6200] train_perplexity =     3.83, train_loss =  1.34, learning_rate = 5.00000000


[batch = 6300] train_perplexity =     3.86, train_loss =  1.35, learning_rate = 5.00000000


[batch = 6400] train_perplexity =     3.85, train_loss =  1.35, learning_rate = 5.00000000


[batch = 6500] train_perplexity =     3.76, train_loss =  1.32, learning_rate = 5.00000000


[batch = 6600] train_perplexity =     3.86, train_loss =  1.35, learning_rate = 5.00000000


[batch = 6700] train_perplexity =     3.81, train_loss =  1.34, learning_rate = 5.00000000


[batch = 6800] train_perplexity =     3.85, train_loss =  1.35, learning_rate = 5.00000000


[batch = 6900] train_perplexity =     3.93, train_loss =  1.37, learning_rate = 5.00000000


[epoch =   4] validation_perplexity =     4.02, validation_loss =  1.39

Saved model to 'e200/ptb_char_lstm.pt'...
[epoch =   4] min_validation_perplexity =     4.02, min_validation_loss =  1.39


[batch =  100] train_perplexity =     4.18, train_loss =  1.43, learning_rate = 5.00000000


[batch =  200] train_perplexity =     3.93, train_loss =  1.37, learning_rate = 5.00000000


[batch =  300] train_perplexity =     3.87, train_loss =  1.35, learning_rate = 5.00000000


[batch =  400] train_perplexity =     3.74, train_loss =  1.32, learning_rate = 5.00000000


[batch =  500] train_perplexity =     3.72, train_loss =  1.31, learning_rate = 5.00000000


[batch =  600] train_perplexity =     3.70, train_loss =  1.31, learning_rate = 5.00000000


[batch =  700] train_perplexity =     3.73, train_loss =  1.32, learning_rate = 5.00000000


[batch =  800] train_perplexity =     3.90, train_loss =  1.36, learning_rate = 5.00000000


[batch =  900] train_perplexity =     3.82, train_loss =  1.34, learning_rate = 5.00000000


[batch = 1000] train_perplexity =     3.95, train_loss =  1.37, learning_rate = 5.00000000


[batch = 1100] train_perplexity =     4.00, train_loss =  1.39, learning_rate = 5.00000000


[batch = 1200] train_perplexity =     3.88, train_loss =  1.36, learning_rate = 5.00000000


[batch = 1300] train_perplexity =     3.89, train_loss =  1.36, learning_rate = 5.00000000


[batch = 1400] train_perplexity =     3.81, train_loss =  1.34, learning_rate = 5.00000000


[batch = 1500] train_perplexity =     3.83, train_loss =  1.34, learning_rate = 5.00000000


[batch = 1600] train_perplexity =     3.80, train_loss =  1.33, learning_rate = 5.00000000


[batch = 1700] train_perplexity =     3.81, train_loss =  1.34, learning_rate = 5.00000000


[batch = 1800] train_perplexity =     3.83, train_loss =  1.34, learning_rate = 5.00000000


[batch = 1900] train_perplexity =     3.84, train_loss =  1.34, learning_rate = 5.00000000


[batch = 2000] train_perplexity =     3.96, train_loss =  1.38, learning_rate = 5.00000000


[batch = 2100] train_perplexity =     3.80, train_loss =  1.34, learning_rate = 5.00000000


[batch = 2200] train_perplexity =     3.85, train_loss =  1.35, learning_rate = 5.00000000


[batch = 2300] train_perplexity =     4.00, train_loss =  1.39, learning_rate = 5.00000000


[batch = 2400] train_perplexity =     3.85, train_loss =  1.35, learning_rate = 5.00000000


[batch = 2500] train_perplexity =     3.89, train_loss =  1.36, learning_rate = 5.00000000


[batch = 2600] train_perplexity =     3.93, train_loss =  1.37, learning_rate = 5.00000000


[batch = 2700] train_perplexity =     3.95, train_loss =  1.37, learning_rate = 5.00000000


[batch = 2800] train_perplexity =     3.87, train_loss =  1.35, learning_rate = 5.00000000


[batch = 2900] train_perplexity =     3.82, train_loss =  1.34, learning_rate = 5.00000000


[batch = 3000] train_perplexity =     3.77, train_loss =  1.33, learning_rate = 5.00000000


[batch = 3100] train_perplexity =     3.90, train_loss =  1.36, learning_rate = 5.00000000


[batch = 3200] train_perplexity =     4.01, train_loss =  1.39, learning_rate = 5.00000000


[batch = 3300] train_perplexity =     3.90, train_loss =  1.36, learning_rate = 5.00000000


[batch = 3400] train_perplexity =     3.80, train_loss =  1.34, learning_rate = 5.00000000


[batch = 3500] train_perplexity =     3.88, train_loss =  1.36, learning_rate = 5.00000000


[batch = 3600] train_perplexity =     3.90, train_loss =  1.36, learning_rate = 5.00000000


[batch = 3700] train_perplexity =     3.87, train_loss =  1.35, learning_rate = 5.00000000


[batch = 3800] train_perplexity =     3.85, train_loss =  1.35, learning_rate = 5.00000000


[batch = 3900] train_perplexity =     3.82, train_loss =  1.34, learning_rate = 5.00000000


[batch = 4000] train_perplexity =     3.87, train_loss =  1.35, learning_rate = 5.00000000


[batch = 4100] train_perplexity =     3.72, train_loss =  1.31, learning_rate = 5.00000000


[batch = 4200] train_perplexity =     3.82, train_loss =  1.34, learning_rate = 5.00000000


[batch = 4300] train_perplexity =     3.79, train_loss =  1.33, learning_rate = 5.00000000


[batch = 4400] train_perplexity =     3.83, train_loss =  1.34, learning_rate = 5.00000000


[batch = 4500] train_perplexity =     3.82, train_loss =  1.34, learning_rate = 5.00000000


[batch = 4600] train_perplexity =     3.82, train_loss =  1.34, learning_rate = 5.00000000


[batch = 4700] train_perplexity =     3.83, train_loss =  1.34, learning_rate = 5.00000000


[batch = 4800] train_perplexity =     3.86, train_loss =  1.35, learning_rate = 5.00000000


[batch = 4900] train_perplexity =     3.89, train_loss =  1.36, learning_rate = 5.00000000


[batch = 5000] train_perplexity =     3.85, train_loss =  1.35, learning_rate = 5.00000000


[batch = 5100] train_perplexity =     3.89, train_loss =  1.36, learning_rate = 5.00000000


[batch = 5200] train_perplexity =     3.90, train_loss =  1.36, learning_rate = 5.00000000


[batch = 5300] train_perplexity =     3.87, train_loss =  1.35, learning_rate = 5.00000000


[batch = 5400] train_perplexity =     3.78, train_loss =  1.33, learning_rate = 5.00000000


[batch = 5500] train_perplexity =     3.85, train_loss =  1.35, learning_rate = 5.00000000


[batch = 5600] train_perplexity =     3.86, train_loss =  1.35, learning_rate = 5.00000000


[batch = 5700] train_perplexity =     3.66, train_loss =  1.30, learning_rate = 5.00000000


[batch = 5800] train_perplexity =     3.75, train_loss =  1.32, learning_rate = 5.00000000


[batch = 5900] train_perplexity =     3.85, train_loss =  1.35, learning_rate = 5.00000000


[batch = 6000] train_perplexity =     3.82, train_loss =  1.34, learning_rate = 5.00000000


[batch = 6100] train_perplexity =     3.76, train_loss =  1.32, learning_rate = 5.00000000


[batch = 6200] train_perplexity =     3.75, train_loss =  1.32, learning_rate = 5.00000000


[batch = 6300] train_perplexity =     3.77, train_loss =  1.33, learning_rate = 5.00000000


[batch = 6400] train_perplexity =     3.74, train_loss =  1.32, learning_rate = 5.00000000


[batch = 6500] train_perplexity =     3.67, train_loss =  1.30, learning_rate = 5.00000000


[batch = 6600] train_perplexity =     3.77, train_loss =  1.33, learning_rate = 5.00000000


[batch = 6700] train_perplexity =     3.72, train_loss =  1.31, learning_rate = 5.00000000


[batch = 6800] train_perplexity =     3.77, train_loss =  1.33, learning_rate = 5.00000000


[batch = 6900] train_perplexity =     3.86, train_loss =  1.35, learning_rate = 5.00000000


[epoch =   5] validation_perplexity =     3.93, validation_loss =  1.37

Saved model to 'e200/ptb_char_lstm.pt'...
[epoch =   5] min_validation_perplexity =     3.93, min_validation_loss =  1.37


[batch =  100] train_perplexity =     4.09, train_loss =  1.41, learning_rate = 5.00000000


[batch =  200] train_perplexity =     3.84, train_loss =  1.35, learning_rate = 5.00000000


[batch =  300] train_perplexity =     3.78, train_loss =  1.33, learning_rate = 5.00000000


[batch =  400] train_perplexity =     3.65, train_loss =  1.29, learning_rate = 5.00000000


[batch =  500] train_perplexity =     3.66, train_loss =  1.30, learning_rate = 5.00000000


[batch =  600] train_perplexity =     3.63, train_loss =  1.29, learning_rate = 5.00000000


[batch =  700] train_perplexity =     3.65, train_loss =  1.30, learning_rate = 5.00000000


[batch =  800] train_perplexity =     3.82, train_loss =  1.34, learning_rate = 5.00000000


[batch =  900] train_perplexity =     3.74, train_loss =  1.32, learning_rate = 5.00000000


[batch = 1000] train_perplexity =     3.87, train_loss =  1.35, learning_rate = 5.00000000


[batch = 1100] train_perplexity =     3.92, train_loss =  1.36, learning_rate = 5.00000000


[batch = 1200] train_perplexity =     3.80, train_loss =  1.33, learning_rate = 5.00000000


[batch = 1300] train_perplexity =     3.81, train_loss =  1.34, learning_rate = 5.00000000


[batch = 1400] train_perplexity =     3.75, train_loss =  1.32, learning_rate = 5.00000000


[batch = 1500] train_perplexity =     3.77, train_loss =  1.33, learning_rate = 5.00000000


[batch = 1600] train_perplexity =     3.73, train_loss =  1.32, learning_rate = 5.00000000


[batch = 1700] train_perplexity =     3.75, train_loss =  1.32, learning_rate = 5.00000000


[batch = 1800] train_perplexity =     3.77, train_loss =  1.33, learning_rate = 5.00000000


[batch = 1900] train_perplexity =     3.76, train_loss =  1.33, learning_rate = 5.00000000


[batch = 2000] train_perplexity =     3.88, train_loss =  1.35, learning_rate = 5.00000000


[batch = 2100] train_perplexity =     3.74, train_loss =  1.32, learning_rate = 5.00000000


[batch = 2200] train_perplexity =     3.79, train_loss =  1.33, learning_rate = 5.00000000


[batch = 2300] train_perplexity =     3.91, train_loss =  1.36, learning_rate = 5.00000000


[batch = 2400] train_perplexity =     3.77, train_loss =  1.33, learning_rate = 5.00000000


[batch = 2500] train_perplexity =     3.81, train_loss =  1.34, learning_rate = 5.00000000


[batch = 2600] train_perplexity =     3.86, train_loss =  1.35, learning_rate = 5.00000000


[batch = 2700] train_perplexity =     3.89, train_loss =  1.36, learning_rate = 5.00000000


[batch = 2800] train_perplexity =     3.81, train_loss =  1.34, learning_rate = 5.00000000


[batch = 2900] train_perplexity =     3.76, train_loss =  1.32, learning_rate = 5.00000000


[batch = 3000] train_perplexity =     3.71, train_loss =  1.31, learning_rate = 5.00000000


[batch = 3100] train_perplexity =     3.83, train_loss =  1.34, learning_rate = 5.00000000


[batch = 3200] train_perplexity =     3.94, train_loss =  1.37, learning_rate = 5.00000000


[batch = 3300] train_perplexity =     3.84, train_loss =  1.35, learning_rate = 5.00000000


[batch = 3400] train_perplexity =     3.74, train_loss =  1.32, learning_rate = 5.00000000


[batch = 3500] train_perplexity =     3.82, train_loss =  1.34, learning_rate = 5.00000000


[batch = 3600] train_perplexity =     3.84, train_loss =  1.35, learning_rate = 5.00000000


[batch = 3700] train_perplexity =     3.81, train_loss =  1.34, learning_rate = 5.00000000


[batch = 3800] train_perplexity =     3.78, train_loss =  1.33, learning_rate = 5.00000000


[batch = 3900] train_perplexity =     3.76, train_loss =  1.33, learning_rate = 5.00000000


[batch = 4000] train_perplexity =     3.80, train_loss =  1.34, learning_rate = 5.00000000


[batch = 4100] train_perplexity =     3.66, train_loss =  1.30, learning_rate = 5.00000000


[batch = 4200] train_perplexity =     3.76, train_loss =  1.32, learning_rate = 5.00000000


[batch = 4300] train_perplexity =     3.74, train_loss =  1.32, learning_rate = 5.00000000


[batch = 4400] train_perplexity =     3.77, train_loss =  1.33, learning_rate = 5.00000000


[batch = 4500] train_perplexity =     3.76, train_loss =  1.32, learning_rate = 5.00000000


[batch = 4600] train_perplexity =     3.75, train_loss =  1.32, learning_rate = 5.00000000


[batch = 4700] train_perplexity =     3.78, train_loss =  1.33, learning_rate = 5.00000000


[batch = 4800] train_perplexity =     3.81, train_loss =  1.34, learning_rate = 5.00000000


[batch = 4900] train_perplexity =     3.83, train_loss =  1.34, learning_rate = 5.00000000


[batch = 5000] train_perplexity =     3.80, train_loss =  1.33, learning_rate = 5.00000000


[batch = 5100] train_perplexity =     3.83, train_loss =  1.34, learning_rate = 5.00000000


[batch = 5200] train_perplexity =     3.86, train_loss =  1.35, learning_rate = 5.00000000


[batch = 5300] train_perplexity =     3.82, train_loss =  1.34, learning_rate = 5.00000000


[batch = 5400] train_perplexity =     3.73, train_loss =  1.32, learning_rate = 5.00000000


[batch = 5500] train_perplexity =     3.81, train_loss =  1.34, learning_rate = 5.00000000


[batch = 5600] train_perplexity =     3.81, train_loss =  1.34, learning_rate = 5.00000000


[batch = 5700] train_perplexity =     3.61, train_loss =  1.28, learning_rate = 5.00000000


[batch = 5800] train_perplexity =     3.71, train_loss =  1.31, learning_rate = 5.00000000


[batch = 5900] train_perplexity =     3.79, train_loss =  1.33, learning_rate = 5.00000000


[batch = 6000] train_perplexity =     3.77, train_loss =  1.33, learning_rate = 5.00000000


[batch = 6100] train_perplexity =     3.71, train_loss =  1.31, learning_rate = 5.00000000


[batch = 6200] train_perplexity =     3.71, train_loss =  1.31, learning_rate = 5.00000000


[batch = 6300] train_perplexity =     3.73, train_loss =  1.32, learning_rate = 5.00000000


[batch = 6400] train_perplexity =     3.70, train_loss =  1.31, learning_rate = 5.00000000


[batch = 6500] train_perplexity =     3.63, train_loss =  1.29, learning_rate = 5.00000000


[batch = 6600] train_perplexity =     3.74, train_loss =  1.32, learning_rate = 5.00000000


[batch = 6700] train_perplexity =     3.68, train_loss =  1.30, learning_rate = 5.00000000


[batch = 6800] train_perplexity =     3.73, train_loss =  1.32, learning_rate = 5.00000000


[batch = 6900] train_perplexity =     3.82, train_loss =  1.34, learning_rate = 5.00000000


[epoch =   6] validation_perplexity =     3.89, validation_loss =  1.36

Saved model to 'e200/ptb_char_lstm.pt'...
[epoch =   6] min_validation_perplexity =     3.89, min_validation_loss =  1.36


[batch =  100] train_perplexity =     4.03, train_loss =  1.39, learning_rate = 5.00000000


[batch =  200] train_perplexity =     3.80, train_loss =  1.33, learning_rate = 5.00000000


[batch =  300] train_perplexity =     3.74, train_loss =  1.32, learning_rate = 5.00000000


[batch =  400] train_perplexity =     3.61, train_loss =  1.28, learning_rate = 5.00000000


[batch =  500] train_perplexity =     3.62, train_loss =  1.29, learning_rate = 5.00000000


[batch =  600] train_perplexity =     3.60, train_loss =  1.28, learning_rate = 5.00000000


[batch =  700] train_perplexity =     3.62, train_loss =  1.29, learning_rate = 5.00000000


[batch =  800] train_perplexity =     3.77, train_loss =  1.33, learning_rate = 5.00000000


[batch =  900] train_perplexity =     3.70, train_loss =  1.31, learning_rate = 5.00000000


[batch = 1000] train_perplexity =     3.82, train_loss =  1.34, learning_rate = 5.00000000


[batch = 1100] train_perplexity =     3.88, train_loss =  1.35, learning_rate = 5.00000000


[batch = 1200] train_perplexity =     3.76, train_loss =  1.33, learning_rate = 5.00000000


[batch = 1300] train_perplexity =     3.78, train_loss =  1.33, learning_rate = 5.00000000


[batch = 1400] train_perplexity =     3.70, train_loss =  1.31, learning_rate = 5.00000000


[batch = 1500] train_perplexity =     3.71, train_loss =  1.31, learning_rate = 5.00000000


[batch = 1600] train_perplexity =     3.68, train_loss =  1.30, learning_rate = 5.00000000


[batch = 1700] train_perplexity =     3.71, train_loss =  1.31, learning_rate = 5.00000000


[batch = 1800] train_perplexity =     3.73, train_loss =  1.32, learning_rate = 5.00000000


[batch = 1900] train_perplexity =     3.73, train_loss =  1.32, learning_rate = 5.00000000


[batch = 2000] train_perplexity =     3.84, train_loss =  1.34, learning_rate = 5.00000000


[batch = 2100] train_perplexity =     3.70, train_loss =  1.31, learning_rate = 5.00000000


[batch = 2200] train_perplexity =     3.76, train_loss =  1.32, learning_rate = 5.00000000


[batch = 2300] train_perplexity =     3.88, train_loss =  1.35, learning_rate = 5.00000000


[batch = 2400] train_perplexity =     3.73, train_loss =  1.32, learning_rate = 5.00000000


[batch = 2500] train_perplexity =     3.78, train_loss =  1.33, learning_rate = 5.00000000


[batch = 2600] train_perplexity =     3.83, train_loss =  1.34, learning_rate = 5.00000000


[batch = 2700] train_perplexity =     3.85, train_loss =  1.35, learning_rate = 5.00000000


[batch = 2800] train_perplexity =     3.77, train_loss =  1.33, learning_rate = 5.00000000


[batch = 2900] train_perplexity =     3.72, train_loss =  1.31, learning_rate = 5.00000000


[batch = 3000] train_perplexity =     3.67, train_loss =  1.30, learning_rate = 5.00000000


[batch = 3100] train_perplexity =     3.79, train_loss =  1.33, learning_rate = 5.00000000


[batch = 3200] train_perplexity =     3.90, train_loss =  1.36, learning_rate = 5.00000000


[batch = 3300] train_perplexity =     3.80, train_loss =  1.34, learning_rate = 5.00000000


[batch = 3400] train_perplexity =     3.71, train_loss =  1.31, learning_rate = 5.00000000


[batch = 3500] train_perplexity =     3.79, train_loss =  1.33, learning_rate = 5.00000000


[batch = 3600] train_perplexity =     3.81, train_loss =  1.34, learning_rate = 5.00000000


[batch = 3700] train_perplexity =     3.77, train_loss =  1.33, learning_rate = 5.00000000


[batch = 3800] train_perplexity =     3.76, train_loss =  1.32, learning_rate = 5.00000000


[batch = 3900] train_perplexity =     3.72, train_loss =  1.32, learning_rate = 5.00000000


[batch = 4000] train_perplexity =     3.77, train_loss =  1.33, learning_rate = 5.00000000


[batch = 4100] train_perplexity =     3.64, train_loss =  1.29, learning_rate = 5.00000000


[batch = 4200] train_perplexity =     3.73, train_loss =  1.32, learning_rate = 5.00000000


[batch = 4300] train_perplexity =     3.70, train_loss =  1.31, learning_rate = 5.00000000


[batch = 4400] train_perplexity =     3.74, train_loss =  1.32, learning_rate = 5.00000000


[batch = 4500] train_perplexity =     3.74, train_loss =  1.32, learning_rate = 5.00000000


[batch = 4600] train_perplexity =     3.72, train_loss =  1.31, learning_rate = 5.00000000


[batch = 4700] train_perplexity =     3.76, train_loss =  1.32, learning_rate = 5.00000000


[batch = 4800] train_perplexity =     3.79, train_loss =  1.33, learning_rate = 5.00000000


[batch = 4900] train_perplexity =     3.81, train_loss =  1.34, learning_rate = 5.00000000


[batch = 5000] train_perplexity =     3.77, train_loss =  1.33, learning_rate = 5.00000000


[batch = 5100] train_perplexity =     3.80, train_loss =  1.33, learning_rate = 5.00000000


[batch = 5200] train_perplexity =     3.82, train_loss =  1.34, learning_rate = 5.00000000


[batch = 5300] train_perplexity =     3.78, train_loss =  1.33, learning_rate = 5.00000000


[batch = 5400] train_perplexity =     3.70, train_loss =  1.31, learning_rate = 5.00000000


[batch = 5500] train_perplexity =     3.77, train_loss =  1.33, learning_rate = 5.00000000


[batch = 5600] train_perplexity =     3.79, train_loss =  1.33, learning_rate = 5.00000000


[batch = 5700] train_perplexity =     3.57, train_loss =  1.27, learning_rate = 5.00000000


[batch = 5800] train_perplexity =     3.68, train_loss =  1.30, learning_rate = 5.00000000


[batch = 5900] train_perplexity =     3.77, train_loss =  1.33, learning_rate = 5.00000000


[batch = 6000] train_perplexity =     3.75, train_loss =  1.32, learning_rate = 5.00000000


[batch = 6100] train_perplexity =     3.68, train_loss =  1.30, learning_rate = 5.00000000


[batch = 6200] train_perplexity =     3.68, train_loss =  1.30, learning_rate = 5.00000000


[batch = 6300] train_perplexity =     3.70, train_loss =  1.31, learning_rate = 5.00000000


[batch = 6400] train_perplexity =     3.66, train_loss =  1.30, learning_rate = 5.00000000


[batch = 6500] train_perplexity =     3.60, train_loss =  1.28, learning_rate = 5.00000000


[batch = 6600] train_perplexity =     3.71, train_loss =  1.31, learning_rate = 5.00000000


[batch = 6700] train_perplexity =     3.65, train_loss =  1.30, learning_rate = 5.00000000


[batch = 6800] train_perplexity =     3.70, train_loss =  1.31, learning_rate = 5.00000000


[batch = 6900] train_perplexity =     3.80, train_loss =  1.33, learning_rate = 5.00000000


[epoch =   7] validation_perplexity =     3.87, validation_loss =  1.35

Saved model to 'e200/ptb_char_lstm.pt'...
[epoch =   7] min_validation_perplexity =     3.87, min_validation_loss =  1.35


[batch =  100] train_perplexity =     4.01, train_loss =  1.39, learning_rate = 5.00000000


[batch =  200] train_perplexity =     3.78, train_loss =  1.33, learning_rate = 5.00000000


[batch =  300] train_perplexity =     3.72, train_loss =  1.31, learning_rate = 5.00000000


[batch =  400] train_perplexity =     3.59, train_loss =  1.28, learning_rate = 5.00000000


[batch =  500] train_perplexity =     3.59, train_loss =  1.28, learning_rate = 5.00000000


[batch =  600] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 5.00000000


[batch =  700] train_perplexity =     3.58, train_loss =  1.27, learning_rate = 5.00000000


[batch =  800] train_perplexity =     3.74, train_loss =  1.32, learning_rate = 5.00000000


[batch =  900] train_perplexity =     3.68, train_loss =  1.30, learning_rate = 5.00000000


[batch = 1000] train_perplexity =     3.80, train_loss =  1.33, learning_rate = 5.00000000


[batch = 1100] train_perplexity =     3.85, train_loss =  1.35, learning_rate = 5.00000000


[batch = 1200] train_perplexity =     3.73, train_loss =  1.32, learning_rate = 5.00000000


[batch = 1300] train_perplexity =     3.76, train_loss =  1.32, learning_rate = 5.00000000


[batch = 1400] train_perplexity =     3.68, train_loss =  1.30, learning_rate = 5.00000000


[batch = 1500] train_perplexity =     3.70, train_loss =  1.31, learning_rate = 5.00000000


[batch = 1600] train_perplexity =     3.66, train_loss =  1.30, learning_rate = 5.00000000


[batch = 1700] train_perplexity =     3.69, train_loss =  1.31, learning_rate = 5.00000000


[batch = 1800] train_perplexity =     3.70, train_loss =  1.31, learning_rate = 5.00000000


[batch = 1900] train_perplexity =     3.69, train_loss =  1.31, learning_rate = 5.00000000


[batch = 2000] train_perplexity =     3.81, train_loss =  1.34, learning_rate = 5.00000000


[batch = 2100] train_perplexity =     3.69, train_loss =  1.30, learning_rate = 5.00000000


[batch = 2200] train_perplexity =     3.73, train_loss =  1.32, learning_rate = 5.00000000


[batch = 2300] train_perplexity =     3.85, train_loss =  1.35, learning_rate = 5.00000000


[batch = 2400] train_perplexity =     3.71, train_loss =  1.31, learning_rate = 5.00000000


[batch = 2500] train_perplexity =     3.76, train_loss =  1.33, learning_rate = 5.00000000


[batch = 2600] train_perplexity =     3.80, train_loss =  1.34, learning_rate = 5.00000000


[batch = 2700] train_perplexity =     3.82, train_loss =  1.34, learning_rate = 5.00000000


[batch = 2800] train_perplexity =     3.75, train_loss =  1.32, learning_rate = 5.00000000


[batch = 2900] train_perplexity =     3.71, train_loss =  1.31, learning_rate = 5.00000000


[batch = 3000] train_perplexity =     3.65, train_loss =  1.29, learning_rate = 5.00000000


[batch = 3100] train_perplexity =     3.77, train_loss =  1.33, learning_rate = 5.00000000


[batch = 3200] train_perplexity =     3.87, train_loss =  1.35, learning_rate = 5.00000000


[batch = 3300] train_perplexity =     3.78, train_loss =  1.33, learning_rate = 5.00000000


[batch = 3400] train_perplexity =     3.69, train_loss =  1.31, learning_rate = 5.00000000


[batch = 3500] train_perplexity =     3.76, train_loss =  1.32, learning_rate = 5.00000000


[batch = 3600] train_perplexity =     3.78, train_loss =  1.33, learning_rate = 5.00000000


[batch = 3700] train_perplexity =     3.75, train_loss =  1.32, learning_rate = 5.00000000


[batch = 3800] train_perplexity =     3.74, train_loss =  1.32, learning_rate = 5.00000000


[batch = 3900] train_perplexity =     3.71, train_loss =  1.31, learning_rate = 5.00000000


[batch = 4000] train_perplexity =     3.74, train_loss =  1.32, learning_rate = 5.00000000


[batch = 4100] train_perplexity =     3.61, train_loss =  1.28, learning_rate = 5.00000000


[batch = 4200] train_perplexity =     3.71, train_loss =  1.31, learning_rate = 5.00000000


[batch = 4300] train_perplexity =     3.68, train_loss =  1.30, learning_rate = 5.00000000


[batch = 4400] train_perplexity =     3.71, train_loss =  1.31, learning_rate = 5.00000000


[batch = 4500] train_perplexity =     3.72, train_loss =  1.31, learning_rate = 5.00000000


[batch = 4600] train_perplexity =     3.70, train_loss =  1.31, learning_rate = 5.00000000


[batch = 4700] train_perplexity =     3.73, train_loss =  1.32, learning_rate = 5.00000000


[batch = 4800] train_perplexity =     3.75, train_loss =  1.32, learning_rate = 5.00000000


[batch = 4900] train_perplexity =     3.79, train_loss =  1.33, learning_rate = 5.00000000


[batch = 5000] train_perplexity =     3.75, train_loss =  1.32, learning_rate = 5.00000000


[batch = 5100] train_perplexity =     3.79, train_loss =  1.33, learning_rate = 5.00000000


[batch = 5200] train_perplexity =     3.78, train_loss =  1.33, learning_rate = 5.00000000


[batch = 5300] train_perplexity =     3.76, train_loss =  1.32, learning_rate = 5.00000000


[batch = 5400] train_perplexity =     3.67, train_loss =  1.30, learning_rate = 5.00000000


[batch = 5500] train_perplexity =     3.74, train_loss =  1.32, learning_rate = 5.00000000


[batch = 5600] train_perplexity =     3.77, train_loss =  1.33, learning_rate = 5.00000000


[batch = 5700] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 5.00000000


[batch = 5800] train_perplexity =     3.65, train_loss =  1.30, learning_rate = 5.00000000


[batch = 5900] train_perplexity =     3.75, train_loss =  1.32, learning_rate = 5.00000000


[batch = 6000] train_perplexity =     3.72, train_loss =  1.31, learning_rate = 5.00000000


[batch = 6100] train_perplexity =     3.66, train_loss =  1.30, learning_rate = 5.00000000


[batch = 6200] train_perplexity =     3.65, train_loss =  1.30, learning_rate = 5.00000000


[batch = 6300] train_perplexity =     3.68, train_loss =  1.30, learning_rate = 5.00000000


[batch = 6400] train_perplexity =     3.65, train_loss =  1.29, learning_rate = 5.00000000


[batch = 6500] train_perplexity =     3.58, train_loss =  1.27, learning_rate = 5.00000000


[batch = 6600] train_perplexity =     3.69, train_loss =  1.31, learning_rate = 5.00000000


[batch = 6700] train_perplexity =     3.65, train_loss =  1.29, learning_rate = 5.00000000


[batch = 6800] train_perplexity =     3.69, train_loss =  1.31, learning_rate = 5.00000000


[batch = 6900] train_perplexity =     3.78, train_loss =  1.33, learning_rate = 5.00000000


[epoch =   8] validation_perplexity =     3.85, validation_loss =  1.35

Saved model to 'e200/ptb_char_lstm.pt'...
[epoch =   8] min_validation_perplexity =     3.85, min_validation_loss =  1.35


[batch =  100] train_perplexity =     3.98, train_loss =  1.38, learning_rate = 5.00000000


[batch =  200] train_perplexity =     3.76, train_loss =  1.32, learning_rate = 5.00000000


[batch =  300] train_perplexity =     3.70, train_loss =  1.31, learning_rate = 5.00000000


[batch =  400] train_perplexity =     3.56, train_loss =  1.27, learning_rate = 5.00000000


[batch =  500] train_perplexity =     3.57, train_loss =  1.27, learning_rate = 5.00000000


[batch =  600] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 5.00000000


[batch =  700] train_perplexity =     3.56, train_loss =  1.27, learning_rate = 5.00000000


[batch =  800] train_perplexity =     3.72, train_loss =  1.31, learning_rate = 5.00000000


[batch =  900] train_perplexity =     3.66, train_loss =  1.30, learning_rate = 5.00000000


[batch = 1000] train_perplexity =     3.78, train_loss =  1.33, learning_rate = 5.00000000


[batch = 1100] train_perplexity =     3.83, train_loss =  1.34, learning_rate = 5.00000000


[batch = 1200] train_perplexity =     3.71, train_loss =  1.31, learning_rate = 5.00000000


[batch = 1300] train_perplexity =     3.75, train_loss =  1.32, learning_rate = 5.00000000


[batch = 1400] train_perplexity =     3.66, train_loss =  1.30, learning_rate = 5.00000000


[batch = 1500] train_perplexity =     3.68, train_loss =  1.30, learning_rate = 5.00000000


[batch = 1600] train_perplexity =     3.64, train_loss =  1.29, learning_rate = 5.00000000


[batch = 1700] train_perplexity =     3.66, train_loss =  1.30, learning_rate = 5.00000000


[batch = 1800] train_perplexity =     3.68, train_loss =  1.30, learning_rate = 5.00000000


[batch = 1900] train_perplexity =     3.70, train_loss =  1.31, learning_rate = 5.00000000


[batch = 2000] train_perplexity =     3.80, train_loss =  1.33, learning_rate = 5.00000000


[batch = 2100] train_perplexity =     3.66, train_loss =  1.30, learning_rate = 5.00000000


[batch = 2200] train_perplexity =     3.72, train_loss =  1.31, learning_rate = 5.00000000


[batch = 2300] train_perplexity =     3.83, train_loss =  1.34, learning_rate = 5.00000000


[batch = 2400] train_perplexity =     3.70, train_loss =  1.31, learning_rate = 5.00000000


[batch = 2500] train_perplexity =     3.74, train_loss =  1.32, learning_rate = 5.00000000


[batch = 2600] train_perplexity =     3.78, train_loss =  1.33, learning_rate = 5.00000000


[batch = 2700] train_perplexity =     3.79, train_loss =  1.33, learning_rate = 5.00000000


[batch = 2800] train_perplexity =     3.72, train_loss =  1.31, learning_rate = 5.00000000


[batch = 2900] train_perplexity =     3.70, train_loss =  1.31, learning_rate = 5.00000000


[batch = 3000] train_perplexity =     3.64, train_loss =  1.29, learning_rate = 5.00000000


[batch = 3100] train_perplexity =     3.75, train_loss =  1.32, learning_rate = 5.00000000


[batch = 3200] train_perplexity =     3.86, train_loss =  1.35, learning_rate = 5.00000000


[batch = 3300] train_perplexity =     3.77, train_loss =  1.33, learning_rate = 5.00000000


[batch = 3400] train_perplexity =     3.69, train_loss =  1.31, learning_rate = 5.00000000


[batch = 3500] train_perplexity =     3.75, train_loss =  1.32, learning_rate = 5.00000000


[batch = 3600] train_perplexity =     3.75, train_loss =  1.32, learning_rate = 5.00000000


[batch = 3700] train_perplexity =     3.73, train_loss =  1.32, learning_rate = 5.00000000


[batch = 3800] train_perplexity =     3.72, train_loss =  1.31, learning_rate = 5.00000000


[batch = 3900] train_perplexity =     3.69, train_loss =  1.30, learning_rate = 5.00000000


[batch = 4000] train_perplexity =     3.73, train_loss =  1.32, learning_rate = 5.00000000


[batch = 4100] train_perplexity =     3.60, train_loss =  1.28, learning_rate = 5.00000000


[batch = 4200] train_perplexity =     3.68, train_loss =  1.30, learning_rate = 5.00000000


[batch = 4300] train_perplexity =     3.65, train_loss =  1.30, learning_rate = 5.00000000


[batch = 4400] train_perplexity =     3.70, train_loss =  1.31, learning_rate = 5.00000000


[batch = 4500] train_perplexity =     3.71, train_loss =  1.31, learning_rate = 5.00000000


[batch = 4600] train_perplexity =     3.68, train_loss =  1.30, learning_rate = 5.00000000


[batch = 4700] train_perplexity =     3.72, train_loss =  1.31, learning_rate = 5.00000000


[batch = 4800] train_perplexity =     3.74, train_loss =  1.32, learning_rate = 5.00000000


[batch = 4900] train_perplexity =     3.76, train_loss =  1.32, learning_rate = 5.00000000


[batch = 5000] train_perplexity =     3.74, train_loss =  1.32, learning_rate = 5.00000000


[batch = 5100] train_perplexity =     3.77, train_loss =  1.33, learning_rate = 5.00000000


[batch = 5200] train_perplexity =     3.77, train_loss =  1.33, learning_rate = 5.00000000


[batch = 5300] train_perplexity =     3.74, train_loss =  1.32, learning_rate = 5.00000000


[batch = 5400] train_perplexity =     3.67, train_loss =  1.30, learning_rate = 5.00000000


[batch = 5500] train_perplexity =     3.74, train_loss =  1.32, learning_rate = 5.00000000


[batch = 5600] train_perplexity =     3.75, train_loss =  1.32, learning_rate = 5.00000000


[batch = 5700] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 5.00000000


[batch = 5800] train_perplexity =     3.64, train_loss =  1.29, learning_rate = 5.00000000


[batch = 5900] train_perplexity =     3.73, train_loss =  1.32, learning_rate = 5.00000000


[batch = 6000] train_perplexity =     3.70, train_loss =  1.31, learning_rate = 5.00000000


[batch = 6100] train_perplexity =     3.65, train_loss =  1.30, learning_rate = 5.00000000


[batch = 6200] train_perplexity =     3.65, train_loss =  1.29, learning_rate = 5.00000000


[batch = 6300] train_perplexity =     3.67, train_loss =  1.30, learning_rate = 5.00000000


[batch = 6400] train_perplexity =     3.64, train_loss =  1.29, learning_rate = 5.00000000


[batch = 6500] train_perplexity =     3.57, train_loss =  1.27, learning_rate = 5.00000000


[batch = 6600] train_perplexity =     3.67, train_loss =  1.30, learning_rate = 5.00000000


[batch = 6700] train_perplexity =     3.63, train_loss =  1.29, learning_rate = 5.00000000


[batch = 6800] train_perplexity =     3.68, train_loss =  1.30, learning_rate = 5.00000000


[batch = 6900] train_perplexity =     3.76, train_loss =  1.32, learning_rate = 5.00000000


[epoch =   9] validation_perplexity =     3.83, validation_loss =  1.34

Saved model to 'e200/ptb_char_lstm.pt'...
[epoch =   9] min_validation_perplexity =     3.83, min_validation_loss =  1.34


[batch =  100] train_perplexity =     3.96, train_loss =  1.38, learning_rate = 5.00000000


[batch =  200] train_perplexity =     3.74, train_loss =  1.32, learning_rate = 5.00000000


[batch =  300] train_perplexity =     3.68, train_loss =  1.30, learning_rate = 5.00000000


[batch =  400] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 5.00000000


[batch =  500] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 5.00000000


[batch =  600] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 5.00000000


[batch =  700] train_perplexity =     3.54, train_loss =  1.27, learning_rate = 5.00000000


[batch =  800] train_perplexity =     3.70, train_loss =  1.31, learning_rate = 5.00000000


[batch =  900] train_perplexity =     3.64, train_loss =  1.29, learning_rate = 5.00000000


[batch = 1000] train_perplexity =     3.78, train_loss =  1.33, learning_rate = 5.00000000


[batch = 1100] train_perplexity =     3.82, train_loss =  1.34, learning_rate = 5.00000000


[batch = 1200] train_perplexity =     3.70, train_loss =  1.31, learning_rate = 5.00000000


[batch = 1300] train_perplexity =     3.73, train_loss =  1.32, learning_rate = 5.00000000


[batch = 1400] train_perplexity =     3.65, train_loss =  1.30, learning_rate = 5.00000000


[batch = 1500] train_perplexity =     3.67, train_loss =  1.30, learning_rate = 5.00000000


[batch = 1600] train_perplexity =     3.63, train_loss =  1.29, learning_rate = 5.00000000


[batch = 1700] train_perplexity =     3.65, train_loss =  1.30, learning_rate = 5.00000000


[batch = 1800] train_perplexity =     3.66, train_loss =  1.30, learning_rate = 5.00000000


[batch = 1900] train_perplexity =     3.68, train_loss =  1.30, learning_rate = 5.00000000


[batch = 2000] train_perplexity =     3.78, train_loss =  1.33, learning_rate = 5.00000000


[batch = 2100] train_perplexity =     3.66, train_loss =  1.30, learning_rate = 5.00000000


[batch = 2200] train_perplexity =     3.71, train_loss =  1.31, learning_rate = 5.00000000


[batch = 2300] train_perplexity =     3.81, train_loss =  1.34, learning_rate = 5.00000000


[batch = 2400] train_perplexity =     3.68, train_loss =  1.30, learning_rate = 5.00000000


[batch = 2500] train_perplexity =     3.72, train_loss =  1.31, learning_rate = 5.00000000


[batch = 2600] train_perplexity =     3.76, train_loss =  1.32, learning_rate = 5.00000000


[batch = 2700] train_perplexity =     3.80, train_loss =  1.33, learning_rate = 5.00000000


[batch = 2800] train_perplexity =     3.71, train_loss =  1.31, learning_rate = 5.00000000


[batch = 2900] train_perplexity =     3.67, train_loss =  1.30, learning_rate = 5.00000000


[batch = 3000] train_perplexity =     3.62, train_loss =  1.29, learning_rate = 5.00000000


[batch = 3100] train_perplexity =     3.73, train_loss =  1.32, learning_rate = 5.00000000


[batch = 3200] train_perplexity =     3.84, train_loss =  1.35, learning_rate = 5.00000000


[batch = 3300] train_perplexity =     3.75, train_loss =  1.32, learning_rate = 5.00000000


[batch = 3400] train_perplexity =     3.67, train_loss =  1.30, learning_rate = 5.00000000


[batch = 3500] train_perplexity =     3.73, train_loss =  1.32, learning_rate = 5.00000000


[batch = 3600] train_perplexity =     3.74, train_loss =  1.32, learning_rate = 5.00000000


[batch = 3700] train_perplexity =     3.72, train_loss =  1.31, learning_rate = 5.00000000


[batch = 3800] train_perplexity =     3.71, train_loss =  1.31, learning_rate = 5.00000000


[batch = 3900] train_perplexity =     3.67, train_loss =  1.30, learning_rate = 5.00000000


[batch = 4000] train_perplexity =     3.70, train_loss =  1.31, learning_rate = 5.00000000


[batch = 4100] train_perplexity =     3.57, train_loss =  1.27, learning_rate = 5.00000000


[batch = 4200] train_perplexity =     3.67, train_loss =  1.30, learning_rate = 5.00000000


[batch = 4300] train_perplexity =     3.65, train_loss =  1.29, learning_rate = 5.00000000


[batch = 4400] train_perplexity =     3.68, train_loss =  1.30, learning_rate = 5.00000000


[batch = 4500] train_perplexity =     3.69, train_loss =  1.31, learning_rate = 5.00000000


[batch = 4600] train_perplexity =     3.67, train_loss =  1.30, learning_rate = 5.00000000


[batch = 4700] train_perplexity =     3.70, train_loss =  1.31, learning_rate = 5.00000000


[batch = 4800] train_perplexity =     3.73, train_loss =  1.32, learning_rate = 5.00000000


[batch = 4900] train_perplexity =     3.76, train_loss =  1.32, learning_rate = 5.00000000


[batch = 5000] train_perplexity =     3.72, train_loss =  1.31, learning_rate = 5.00000000


[batch = 5100] train_perplexity =     3.76, train_loss =  1.32, learning_rate = 5.00000000


[batch = 5200] train_perplexity =     3.75, train_loss =  1.32, learning_rate = 5.00000000


[batch = 5300] train_perplexity =     3.72, train_loss =  1.31, learning_rate = 5.00000000


[batch = 5400] train_perplexity =     3.66, train_loss =  1.30, learning_rate = 5.00000000


[batch = 5500] train_perplexity =     3.73, train_loss =  1.32, learning_rate = 5.00000000


[batch = 5600] train_perplexity =     3.73, train_loss =  1.32, learning_rate = 5.00000000


[batch = 5700] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 5.00000000


[batch = 5800] train_perplexity =     3.63, train_loss =  1.29, learning_rate = 5.00000000


[batch = 5900] train_perplexity =     3.72, train_loss =  1.31, learning_rate = 5.00000000


[batch = 6000] train_perplexity =     3.68, train_loss =  1.30, learning_rate = 5.00000000


[batch = 6100] train_perplexity =     3.64, train_loss =  1.29, learning_rate = 5.00000000


[batch = 6200] train_perplexity =     3.63, train_loss =  1.29, learning_rate = 5.00000000


[batch = 6300] train_perplexity =     3.65, train_loss =  1.29, learning_rate = 5.00000000


[batch = 6400] train_perplexity =     3.63, train_loss =  1.29, learning_rate = 5.00000000


[batch = 6500] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 5.00000000


[batch = 6600] train_perplexity =     3.65, train_loss =  1.30, learning_rate = 5.00000000


[batch = 6700] train_perplexity =     3.62, train_loss =  1.29, learning_rate = 5.00000000


[batch = 6800] train_perplexity =     3.66, train_loss =  1.30, learning_rate = 5.00000000


[batch = 6900] train_perplexity =     3.75, train_loss =  1.32, learning_rate = 5.00000000


[epoch =  10] validation_perplexity =     3.83, validation_loss =  1.34

[epoch =  10] annealing learning_rate = 1.25000000

[batch =  100] train_perplexity =     3.95, train_loss =  1.37, learning_rate = 1.25000000


[batch =  200] train_perplexity =     3.72, train_loss =  1.31, learning_rate = 1.25000000


[batch =  300] train_perplexity =     3.63, train_loss =  1.29, learning_rate = 1.25000000


[batch =  400] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 1.25000000


[batch =  500] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 1.25000000


[batch =  600] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 1.25000000


[batch =  700] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 1.25000000


[batch =  800] train_perplexity =     3.65, train_loss =  1.29, learning_rate = 1.25000000


[batch =  900] train_perplexity =     3.57, train_loss =  1.27, learning_rate = 1.25000000


[batch = 1000] train_perplexity =     3.69, train_loss =  1.31, learning_rate = 1.25000000


[batch = 1100] train_perplexity =     3.75, train_loss =  1.32, learning_rate = 1.25000000


[batch = 1200] train_perplexity =     3.63, train_loss =  1.29, learning_rate = 1.25000000


[batch = 1300] train_perplexity =     3.67, train_loss =  1.30, learning_rate = 1.25000000


[batch = 1400] train_perplexity =     3.57, train_loss =  1.27, learning_rate = 1.25000000


[batch = 1500] train_perplexity =     3.59, train_loss =  1.28, learning_rate = 1.25000000


[batch = 1600] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 1.25000000


[batch = 1700] train_perplexity =     3.57, train_loss =  1.27, learning_rate = 1.25000000


[batch = 1800] train_perplexity =     3.58, train_loss =  1.28, learning_rate = 1.25000000


[batch = 1900] train_perplexity =     3.59, train_loss =  1.28, learning_rate = 1.25000000


[batch = 2000] train_perplexity =     3.70, train_loss =  1.31, learning_rate = 1.25000000


[batch = 2100] train_perplexity =     3.57, train_loss =  1.27, learning_rate = 1.25000000


[batch = 2200] train_perplexity =     3.62, train_loss =  1.29, learning_rate = 1.25000000


[batch = 2300] train_perplexity =     3.72, train_loss =  1.31, learning_rate = 1.25000000


[batch = 2400] train_perplexity =     3.60, train_loss =  1.28, learning_rate = 1.25000000


[batch = 2500] train_perplexity =     3.64, train_loss =  1.29, learning_rate = 1.25000000


[batch = 2600] train_perplexity =     3.68, train_loss =  1.30, learning_rate = 1.25000000


[batch = 2700] train_perplexity =     3.70, train_loss =  1.31, learning_rate = 1.25000000


[batch = 2800] train_perplexity =     3.61, train_loss =  1.28, learning_rate = 1.25000000


[batch = 2900] train_perplexity =     3.58, train_loss =  1.28, learning_rate = 1.25000000


[batch = 3000] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 1.25000000


[batch = 3100] train_perplexity =     3.64, train_loss =  1.29, learning_rate = 1.25000000


[batch = 3200] train_perplexity =     3.75, train_loss =  1.32, learning_rate = 1.25000000


[batch = 3300] train_perplexity =     3.66, train_loss =  1.30, learning_rate = 1.25000000


[batch = 3400] train_perplexity =     3.58, train_loss =  1.28, learning_rate = 1.25000000


[batch = 3500] train_perplexity =     3.64, train_loss =  1.29, learning_rate = 1.25000000


[batch = 3600] train_perplexity =     3.65, train_loss =  1.29, learning_rate = 1.25000000


[batch = 3700] train_perplexity =     3.62, train_loss =  1.29, learning_rate = 1.25000000


[batch = 3800] train_perplexity =     3.61, train_loss =  1.28, learning_rate = 1.25000000


[batch = 3900] train_perplexity =     3.59, train_loss =  1.28, learning_rate = 1.25000000


[batch = 4000] train_perplexity =     3.60, train_loss =  1.28, learning_rate = 1.25000000


[batch = 4100] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 1.25000000


[batch = 4200] train_perplexity =     3.57, train_loss =  1.27, learning_rate = 1.25000000


[batch = 4300] train_perplexity =     3.56, train_loss =  1.27, learning_rate = 1.25000000


[batch = 4400] train_perplexity =     3.58, train_loss =  1.28, learning_rate = 1.25000000


[batch = 4500] train_perplexity =     3.59, train_loss =  1.28, learning_rate = 1.25000000


[batch = 4600] train_perplexity =     3.57, train_loss =  1.27, learning_rate = 1.25000000


[batch = 4700] train_perplexity =     3.60, train_loss =  1.28, learning_rate = 1.25000000


[batch = 4800] train_perplexity =     3.64, train_loss =  1.29, learning_rate = 1.25000000


[batch = 4900] train_perplexity =     3.65, train_loss =  1.30, learning_rate = 1.25000000


[batch = 5000] train_perplexity =     3.61, train_loss =  1.28, learning_rate = 1.25000000


[batch = 5100] train_perplexity =     3.66, train_loss =  1.30, learning_rate = 1.25000000


[batch = 5200] train_perplexity =     3.66, train_loss =  1.30, learning_rate = 1.25000000


[batch = 5300] train_perplexity =     3.63, train_loss =  1.29, learning_rate = 1.25000000


[batch = 5400] train_perplexity =     3.56, train_loss =  1.27, learning_rate = 1.25000000


[batch = 5500] train_perplexity =     3.63, train_loss =  1.29, learning_rate = 1.25000000


[batch = 5600] train_perplexity =     3.63, train_loss =  1.29, learning_rate = 1.25000000


[batch = 5700] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 1.25000000


[batch = 5800] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 1.25000000


[batch = 5900] train_perplexity =     3.62, train_loss =  1.29, learning_rate = 1.25000000


[batch = 6000] train_perplexity =     3.59, train_loss =  1.28, learning_rate = 1.25000000


[batch = 6100] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 1.25000000


[batch = 6200] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 1.25000000


[batch = 6300] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 1.25000000


[batch = 6400] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 1.25000000


[batch = 6500] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 1.25000000


[batch = 6600] train_perplexity =     3.56, train_loss =  1.27, learning_rate = 1.25000000


[batch = 6700] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 1.25000000


[batch = 6800] train_perplexity =     3.56, train_loss =  1.27, learning_rate = 1.25000000


[batch = 6900] train_perplexity =     3.64, train_loss =  1.29, learning_rate = 1.25000000


[epoch =  11] validation_perplexity =     3.69, validation_loss =  1.31

Saved model to 'e200/ptb_char_lstm.pt'...
[epoch =  11] min_validation_perplexity =     3.69, min_validation_loss =  1.31


[batch =  100] train_perplexity =     3.85, train_loss =  1.35, learning_rate = 1.25000000


[batch =  200] train_perplexity =     3.64, train_loss =  1.29, learning_rate = 1.25000000


[batch =  300] train_perplexity =     3.58, train_loss =  1.27, learning_rate = 1.25000000


[batch =  400] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 1.25000000


[batch =  500] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 1.25000000


[batch =  600] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 1.25000000


[batch =  700] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 1.25000000


[batch =  800] train_perplexity =     3.60, train_loss =  1.28, learning_rate = 1.25000000


[batch =  900] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 1.25000000


[batch = 1000] train_perplexity =     3.65, train_loss =  1.30, learning_rate = 1.25000000


[batch = 1100] train_perplexity =     3.71, train_loss =  1.31, learning_rate = 1.25000000


[batch = 1200] train_perplexity =     3.59, train_loss =  1.28, learning_rate = 1.25000000


[batch = 1300] train_perplexity =     3.64, train_loss =  1.29, learning_rate = 1.25000000


[batch = 1400] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 1.25000000


[batch = 1500] train_perplexity =     3.57, train_loss =  1.27, learning_rate = 1.25000000


[batch = 1600] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 1.25000000


[batch = 1700] train_perplexity =     3.54, train_loss =  1.27, learning_rate = 1.25000000


[batch = 1800] train_perplexity =     3.56, train_loss =  1.27, learning_rate = 1.25000000


[batch = 1900] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 1.25000000


[batch = 2000] train_perplexity =     3.67, train_loss =  1.30, learning_rate = 1.25000000


[batch = 2100] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 1.25000000


[batch = 2200] train_perplexity =     3.59, train_loss =  1.28, learning_rate = 1.25000000


[batch = 2300] train_perplexity =     3.69, train_loss =  1.31, learning_rate = 1.25000000


[batch = 2400] train_perplexity =     3.57, train_loss =  1.27, learning_rate = 1.25000000


[batch = 2500] train_perplexity =     3.61, train_loss =  1.28, learning_rate = 1.25000000


[batch = 2600] train_perplexity =     3.66, train_loss =  1.30, learning_rate = 1.25000000


[batch = 2700] train_perplexity =     3.68, train_loss =  1.30, learning_rate = 1.25000000


[batch = 2800] train_perplexity =     3.59, train_loss =  1.28, learning_rate = 1.25000000


[batch = 2900] train_perplexity =     3.56, train_loss =  1.27, learning_rate = 1.25000000


[batch = 3000] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 1.25000000


[batch = 3100] train_perplexity =     3.62, train_loss =  1.29, learning_rate = 1.25000000


[batch = 3200] train_perplexity =     3.73, train_loss =  1.32, learning_rate = 1.25000000


[batch = 3300] train_perplexity =     3.63, train_loss =  1.29, learning_rate = 1.25000000


[batch = 3400] train_perplexity =     3.56, train_loss =  1.27, learning_rate = 1.25000000


[batch = 3500] train_perplexity =     3.62, train_loss =  1.29, learning_rate = 1.25000000


[batch = 3600] train_perplexity =     3.63, train_loss =  1.29, learning_rate = 1.25000000


[batch = 3700] train_perplexity =     3.60, train_loss =  1.28, learning_rate = 1.25000000


[batch = 3800] train_perplexity =     3.59, train_loss =  1.28, learning_rate = 1.25000000


[batch = 3900] train_perplexity =     3.57, train_loss =  1.27, learning_rate = 1.25000000


[batch = 4000] train_perplexity =     3.59, train_loss =  1.28, learning_rate = 1.25000000


[batch = 4100] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 1.25000000


[batch = 4200] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 1.25000000


[batch = 4300] train_perplexity =     3.54, train_loss =  1.27, learning_rate = 1.25000000


[batch = 4400] train_perplexity =     3.57, train_loss =  1.27, learning_rate = 1.25000000


[batch = 4500] train_perplexity =     3.58, train_loss =  1.28, learning_rate = 1.25000000


[batch = 4600] train_perplexity =     3.56, train_loss =  1.27, learning_rate = 1.25000000


[batch = 4700] train_perplexity =     3.58, train_loss =  1.28, learning_rate = 1.25000000


[batch = 4800] train_perplexity =     3.62, train_loss =  1.29, learning_rate = 1.25000000


[batch = 4900] train_perplexity =     3.64, train_loss =  1.29, learning_rate = 1.25000000


[batch = 5000] train_perplexity =     3.59, train_loss =  1.28, learning_rate = 1.25000000


[batch = 5100] train_perplexity =     3.64, train_loss =  1.29, learning_rate = 1.25000000


[batch = 5200] train_perplexity =     3.64, train_loss =  1.29, learning_rate = 1.25000000


[batch = 5300] train_perplexity =     3.62, train_loss =  1.29, learning_rate = 1.25000000


[batch = 5400] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 1.25000000


[batch = 5500] train_perplexity =     3.61, train_loss =  1.28, learning_rate = 1.25000000


[batch = 5600] train_perplexity =     3.62, train_loss =  1.29, learning_rate = 1.25000000


[batch = 5700] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 1.25000000


[batch = 5800] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 1.25000000


[batch = 5900] train_perplexity =     3.61, train_loss =  1.28, learning_rate = 1.25000000


[batch = 6000] train_perplexity =     3.58, train_loss =  1.27, learning_rate = 1.25000000


[batch = 6100] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 1.25000000


[batch = 6200] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 1.25000000


[batch = 6300] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 1.25000000


[batch = 6400] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 1.25000000


[batch = 6500] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 1.25000000


[batch = 6600] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 1.25000000


[batch = 6700] train_perplexity =     3.51, train_loss =  1.25, learning_rate = 1.25000000


[batch = 6800] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 1.25000000


[batch = 6900] train_perplexity =     3.63, train_loss =  1.29, learning_rate = 1.25000000


[epoch =  12] validation_perplexity =     3.68, validation_loss =  1.30

Saved model to 'e200/ptb_char_lstm.pt'...
[epoch =  12] min_validation_perplexity =     3.68, min_validation_loss =  1.30


[batch =  100] train_perplexity =     3.83, train_loss =  1.34, learning_rate = 1.25000000


[batch =  200] train_perplexity =     3.63, train_loss =  1.29, learning_rate = 1.25000000


[batch =  300] train_perplexity =     3.56, train_loss =  1.27, learning_rate = 1.25000000


[batch =  400] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 1.25000000


[batch =  500] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 1.25000000


[batch =  600] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 1.25000000


[batch =  700] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 1.25000000


[batch =  800] train_perplexity =     3.59, train_loss =  1.28, learning_rate = 1.25000000


[batch =  900] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 1.25000000


[batch = 1000] train_perplexity =     3.64, train_loss =  1.29, learning_rate = 1.25000000


[batch = 1100] train_perplexity =     3.70, train_loss =  1.31, learning_rate = 1.25000000


[batch = 1200] train_perplexity =     3.58, train_loss =  1.28, learning_rate = 1.25000000


[batch = 1300] train_perplexity =     3.63, train_loss =  1.29, learning_rate = 1.25000000


[batch = 1400] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 1.25000000


[batch = 1500] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 1.25000000


[batch = 1600] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 1.25000000


[batch = 1700] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 1.25000000


[batch = 1800] train_perplexity =     3.54, train_loss =  1.27, learning_rate = 1.25000000


[batch = 1900] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 1.25000000


[batch = 2000] train_perplexity =     3.66, train_loss =  1.30, learning_rate = 1.25000000


[batch = 2100] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 1.25000000


[batch = 2200] train_perplexity =     3.58, train_loss =  1.28, learning_rate = 1.25000000


[batch = 2300] train_perplexity =     3.68, train_loss =  1.30, learning_rate = 1.25000000


[batch = 2400] train_perplexity =     3.56, train_loss =  1.27, learning_rate = 1.25000000


[batch = 2500] train_perplexity =     3.60, train_loss =  1.28, learning_rate = 1.25000000


[batch = 2600] train_perplexity =     3.65, train_loss =  1.29, learning_rate = 1.25000000


[batch = 2700] train_perplexity =     3.67, train_loss =  1.30, learning_rate = 1.25000000


[batch = 2800] train_perplexity =     3.58, train_loss =  1.28, learning_rate = 1.25000000


[batch = 2900] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 1.25000000


[batch = 3000] train_perplexity =     3.51, train_loss =  1.25, learning_rate = 1.25000000


[batch = 3100] train_perplexity =     3.61, train_loss =  1.28, learning_rate = 1.25000000


[batch = 3200] train_perplexity =     3.71, train_loss =  1.31, learning_rate = 1.25000000


[batch = 3300] train_perplexity =     3.62, train_loss =  1.29, learning_rate = 1.25000000


[batch = 3400] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 1.25000000


[batch = 3500] train_perplexity =     3.61, train_loss =  1.28, learning_rate = 1.25000000


[batch = 3600] train_perplexity =     3.62, train_loss =  1.29, learning_rate = 1.25000000


[batch = 3700] train_perplexity =     3.59, train_loss =  1.28, learning_rate = 1.25000000


[batch = 3800] train_perplexity =     3.58, train_loss =  1.28, learning_rate = 1.25000000


[batch = 3900] train_perplexity =     3.56, train_loss =  1.27, learning_rate = 1.25000000


[batch = 4000] train_perplexity =     3.58, train_loss =  1.28, learning_rate = 1.25000000


[batch = 4100] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 1.25000000


[batch = 4200] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 1.25000000


[batch = 4300] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 1.25000000


[batch = 4400] train_perplexity =     3.56, train_loss =  1.27, learning_rate = 1.25000000


[batch = 4500] train_perplexity =     3.57, train_loss =  1.27, learning_rate = 1.25000000


[batch = 4600] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 1.25000000


[batch = 4700] train_perplexity =     3.58, train_loss =  1.27, learning_rate = 1.25000000


[batch = 4800] train_perplexity =     3.61, train_loss =  1.28, learning_rate = 1.25000000


[batch = 4900] train_perplexity =     3.63, train_loss =  1.29, learning_rate = 1.25000000


[batch = 5000] train_perplexity =     3.58, train_loss =  1.28, learning_rate = 1.25000000


[batch = 5100] train_perplexity =     3.63, train_loss =  1.29, learning_rate = 1.25000000


[batch = 5200] train_perplexity =     3.63, train_loss =  1.29, learning_rate = 1.25000000


[batch = 5300] train_perplexity =     3.61, train_loss =  1.28, learning_rate = 1.25000000


[batch = 5400] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 1.25000000


[batch = 5500] train_perplexity =     3.60, train_loss =  1.28, learning_rate = 1.25000000


[batch = 5600] train_perplexity =     3.61, train_loss =  1.28, learning_rate = 1.25000000


[batch = 5700] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 1.25000000


[batch = 5800] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 1.25000000


[batch = 5900] train_perplexity =     3.60, train_loss =  1.28, learning_rate = 1.25000000


[batch = 6000] train_perplexity =     3.57, train_loss =  1.27, learning_rate = 1.25000000


[batch = 6100] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 1.25000000


[batch = 6200] train_perplexity =     3.51, train_loss =  1.25, learning_rate = 1.25000000


[batch = 6300] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 1.25000000


[batch = 6400] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 1.25000000


[batch = 6500] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 1.25000000


[batch = 6600] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 1.25000000


[batch = 6700] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 1.25000000


[batch = 6800] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 1.25000000


[batch = 6900] train_perplexity =     3.62, train_loss =  1.29, learning_rate = 1.25000000


[epoch =  13] validation_perplexity =     3.67, validation_loss =  1.30

Saved model to 'e200/ptb_char_lstm.pt'...
[epoch =  13] min_validation_perplexity =     3.67, min_validation_loss =  1.30


[batch =  100] train_perplexity =     3.82, train_loss =  1.34, learning_rate = 1.25000000


[batch =  200] train_perplexity =     3.62, train_loss =  1.29, learning_rate = 1.25000000


[batch =  300] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 1.25000000


[batch =  400] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 1.25000000


[batch =  500] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 1.25000000


[batch =  600] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 1.25000000


[batch =  700] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 1.25000000


[batch =  800] train_perplexity =     3.58, train_loss =  1.27, learning_rate = 1.25000000


[batch =  900] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 1.25000000


[batch = 1000] train_perplexity =     3.63, train_loss =  1.29, learning_rate = 1.25000000


[batch = 1100] train_perplexity =     3.69, train_loss =  1.31, learning_rate = 1.25000000


[batch = 1200] train_perplexity =     3.57, train_loss =  1.27, learning_rate = 1.25000000


[batch = 1300] train_perplexity =     3.62, train_loss =  1.29, learning_rate = 1.25000000


[batch = 1400] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 1.25000000


[batch = 1500] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 1.25000000


[batch = 1600] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 1.25000000


[batch = 1700] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 1.25000000


[batch = 1800] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 1.25000000


[batch = 1900] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 1.25000000


[batch = 2000] train_perplexity =     3.65, train_loss =  1.30, learning_rate = 1.25000000


[batch = 2100] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 1.25000000


[batch = 2200] train_perplexity =     3.58, train_loss =  1.27, learning_rate = 1.25000000


[batch = 2300] train_perplexity =     3.67, train_loss =  1.30, learning_rate = 1.25000000


[batch = 2400] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 1.25000000


[batch = 2500] train_perplexity =     3.59, train_loss =  1.28, learning_rate = 1.25000000


[batch = 2600] train_perplexity =     3.64, train_loss =  1.29, learning_rate = 1.25000000


[batch = 2700] train_perplexity =     3.66, train_loss =  1.30, learning_rate = 1.25000000


[batch = 2800] train_perplexity =     3.58, train_loss =  1.27, learning_rate = 1.25000000


[batch = 2900] train_perplexity =     3.54, train_loss =  1.27, learning_rate = 1.25000000


[batch = 3000] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 1.25000000


[batch = 3100] train_perplexity =     3.60, train_loss =  1.28, learning_rate = 1.25000000


[batch = 3200] train_perplexity =     3.70, train_loss =  1.31, learning_rate = 1.25000000


[batch = 3300] train_perplexity =     3.61, train_loss =  1.28, learning_rate = 1.25000000


[batch = 3400] train_perplexity =     3.54, train_loss =  1.27, learning_rate = 1.25000000


[batch = 3500] train_perplexity =     3.60, train_loss =  1.28, learning_rate = 1.25000000


[batch = 3600] train_perplexity =     3.61, train_loss =  1.29, learning_rate = 1.25000000


[batch = 3700] train_perplexity =     3.59, train_loss =  1.28, learning_rate = 1.25000000


[batch = 3800] train_perplexity =     3.57, train_loss =  1.27, learning_rate = 1.25000000


[batch = 3900] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 1.25000000


[batch = 4000] train_perplexity =     3.57, train_loss =  1.27, learning_rate = 1.25000000


[batch = 4100] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 1.25000000


[batch = 4200] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 1.25000000


[batch = 4300] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 1.25000000


[batch = 4400] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 1.25000000


[batch = 4500] train_perplexity =     3.56, train_loss =  1.27, learning_rate = 1.25000000


[batch = 4600] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 1.25000000


[batch = 4700] train_perplexity =     3.57, train_loss =  1.27, learning_rate = 1.25000000


[batch = 4800] train_perplexity =     3.60, train_loss =  1.28, learning_rate = 1.25000000


[batch = 4900] train_perplexity =     3.62, train_loss =  1.29, learning_rate = 1.25000000


[batch = 5000] train_perplexity =     3.58, train_loss =  1.27, learning_rate = 1.25000000


[batch = 5100] train_perplexity =     3.63, train_loss =  1.29, learning_rate = 1.25000000


[batch = 5200] train_perplexity =     3.62, train_loss =  1.29, learning_rate = 1.25000000


[batch = 5300] train_perplexity =     3.60, train_loss =  1.28, learning_rate = 1.25000000


[batch = 5400] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 1.25000000


[batch = 5500] train_perplexity =     3.59, train_loss =  1.28, learning_rate = 1.25000000


[batch = 5600] train_perplexity =     3.60, train_loss =  1.28, learning_rate = 1.25000000


[batch = 5700] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 1.25000000


[batch = 5800] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 1.25000000


[batch = 5900] train_perplexity =     3.60, train_loss =  1.28, learning_rate = 1.25000000


[batch = 6000] train_perplexity =     3.56, train_loss =  1.27, learning_rate = 1.25000000


[batch = 6100] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 1.25000000


[batch = 6200] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 1.25000000


[batch = 6300] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 1.25000000


[batch = 6400] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 1.25000000


[batch = 6500] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 1.25000000


[batch = 6600] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 1.25000000


[batch = 6700] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 1.25000000


[batch = 6800] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 1.25000000


[batch = 6900] train_perplexity =     3.62, train_loss =  1.29, learning_rate = 1.25000000


[epoch =  14] validation_perplexity =     3.67, validation_loss =  1.30

Saved model to 'e200/ptb_char_lstm.pt'...
[epoch =  14] min_validation_perplexity =     3.67, min_validation_loss =  1.30


[batch =  100] train_perplexity =     3.82, train_loss =  1.34, learning_rate = 1.25000000


[batch =  200] train_perplexity =     3.61, train_loss =  1.28, learning_rate = 1.25000000


[batch =  300] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 1.25000000


[batch =  400] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 1.25000000


[batch =  500] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 1.25000000


[batch =  600] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 1.25000000


[batch =  700] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 1.25000000


[batch =  800] train_perplexity =     3.57, train_loss =  1.27, learning_rate = 1.25000000


[batch =  900] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 1.25000000


[batch = 1000] train_perplexity =     3.62, train_loss =  1.29, learning_rate = 1.25000000


[batch = 1100] train_perplexity =     3.68, train_loss =  1.30, learning_rate = 1.25000000


[batch = 1200] train_perplexity =     3.57, train_loss =  1.27, learning_rate = 1.25000000


[batch = 1300] train_perplexity =     3.61, train_loss =  1.28, learning_rate = 1.25000000


[batch = 1400] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 1.25000000


[batch = 1500] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 1.25000000


[batch = 1600] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 1.25000000


[batch = 1700] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 1.25000000


[batch = 1800] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 1.25000000


[batch = 1900] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 1.25000000


[batch = 2000] train_perplexity =     3.65, train_loss =  1.29, learning_rate = 1.25000000


[batch = 2100] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 1.25000000


[batch = 2200] train_perplexity =     3.57, train_loss =  1.27, learning_rate = 1.25000000


[batch = 2300] train_perplexity =     3.66, train_loss =  1.30, learning_rate = 1.25000000


[batch = 2400] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 1.25000000


[batch = 2500] train_perplexity =     3.58, train_loss =  1.28, learning_rate = 1.25000000


[batch = 2600] train_perplexity =     3.63, train_loss =  1.29, learning_rate = 1.25000000


[batch = 2700] train_perplexity =     3.65, train_loss =  1.29, learning_rate = 1.25000000


[batch = 2800] train_perplexity =     3.57, train_loss =  1.27, learning_rate = 1.25000000


[batch = 2900] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 1.25000000


[batch = 3000] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 1.25000000


[batch = 3100] train_perplexity =     3.59, train_loss =  1.28, learning_rate = 1.25000000


[batch = 3200] train_perplexity =     3.70, train_loss =  1.31, learning_rate = 1.25000000


[batch = 3300] train_perplexity =     3.60, train_loss =  1.28, learning_rate = 1.25000000


[batch = 3400] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 1.25000000


[batch = 3500] train_perplexity =     3.60, train_loss =  1.28, learning_rate = 1.25000000


[batch = 3600] train_perplexity =     3.61, train_loss =  1.28, learning_rate = 1.25000000


[batch = 3700] train_perplexity =     3.58, train_loss =  1.27, learning_rate = 1.25000000


[batch = 3800] train_perplexity =     3.57, train_loss =  1.27, learning_rate = 1.25000000


[batch = 3900] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 1.25000000


[batch = 4000] train_perplexity =     3.56, train_loss =  1.27, learning_rate = 1.25000000


[batch = 4100] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 1.25000000


[batch = 4200] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 1.25000000


[batch = 4300] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 1.25000000


[batch = 4400] train_perplexity =     3.54, train_loss =  1.27, learning_rate = 1.25000000


[batch = 4500] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 1.25000000


[batch = 4600] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 1.25000000


[batch = 4700] train_perplexity =     3.57, train_loss =  1.27, learning_rate = 1.25000000


[batch = 4800] train_perplexity =     3.59, train_loss =  1.28, learning_rate = 1.25000000


[batch = 4900] train_perplexity =     3.61, train_loss =  1.29, learning_rate = 1.25000000


[batch = 5000] train_perplexity =     3.57, train_loss =  1.27, learning_rate = 1.25000000


[batch = 5100] train_perplexity =     3.62, train_loss =  1.29, learning_rate = 1.25000000


[batch = 5200] train_perplexity =     3.62, train_loss =  1.29, learning_rate = 1.25000000


[batch = 5300] train_perplexity =     3.60, train_loss =  1.28, learning_rate = 1.25000000


[batch = 5400] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 1.25000000


[batch = 5500] train_perplexity =     3.59, train_loss =  1.28, learning_rate = 1.25000000


[batch = 5600] train_perplexity =     3.60, train_loss =  1.28, learning_rate = 1.25000000


[batch = 5700] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 1.25000000


[batch = 5800] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 1.25000000


[batch = 5900] train_perplexity =     3.59, train_loss =  1.28, learning_rate = 1.25000000


[batch = 6000] train_perplexity =     3.56, train_loss =  1.27, learning_rate = 1.25000000


[batch = 6100] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 1.25000000


[batch = 6200] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 1.25000000


[batch = 6300] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 1.25000000


[batch = 6400] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 1.25000000


[batch = 6500] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 1.25000000


[batch = 6600] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 1.25000000


[batch = 6700] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 1.25000000


[batch = 6800] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 1.25000000


[batch = 6900] train_perplexity =     3.61, train_loss =  1.28, learning_rate = 1.25000000


[epoch =  15] validation_perplexity =     3.66, validation_loss =  1.30

Saved model to 'e200/ptb_char_lstm.pt'...
[epoch =  15] min_validation_perplexity =     3.66, min_validation_loss =  1.30


[batch =  100] train_perplexity =     3.81, train_loss =  1.34, learning_rate = 1.25000000


[batch =  200] train_perplexity =     3.61, train_loss =  1.28, learning_rate = 1.25000000


[batch =  300] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 1.25000000


[batch =  400] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 1.25000000


[batch =  500] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 1.25000000


[batch =  600] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 1.25000000


[batch =  700] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 1.25000000


[batch =  800] train_perplexity =     3.56, train_loss =  1.27, learning_rate = 1.25000000


[batch =  900] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 1.25000000


[batch = 1000] train_perplexity =     3.62, train_loss =  1.29, learning_rate = 1.25000000


[batch = 1100] train_perplexity =     3.68, train_loss =  1.30, learning_rate = 1.25000000


[batch = 1200] train_perplexity =     3.56, train_loss =  1.27, learning_rate = 1.25000000


[batch = 1300] train_perplexity =     3.60, train_loss =  1.28, learning_rate = 1.25000000


[batch = 1400] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 1.25000000


[batch = 1500] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 1.25000000


[batch = 1600] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 1.25000000


[batch = 1700] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 1.25000000


[batch = 1800] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 1.25000000


[batch = 1900] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 1.25000000


[batch = 2000] train_perplexity =     3.64, train_loss =  1.29, learning_rate = 1.25000000


[batch = 2100] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 1.25000000


[batch = 2200] train_perplexity =     3.56, train_loss =  1.27, learning_rate = 1.25000000


[batch = 2300] train_perplexity =     3.65, train_loss =  1.29, learning_rate = 1.25000000


[batch = 2400] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 1.25000000


[batch = 2500] train_perplexity =     3.57, train_loss =  1.27, learning_rate = 1.25000000


[batch = 2600] train_perplexity =     3.63, train_loss =  1.29, learning_rate = 1.25000000


[batch = 2700] train_perplexity =     3.64, train_loss =  1.29, learning_rate = 1.25000000


[batch = 2800] train_perplexity =     3.56, train_loss =  1.27, learning_rate = 1.25000000


[batch = 2900] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 1.25000000


[batch = 3000] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 1.25000000


[batch = 3100] train_perplexity =     3.59, train_loss =  1.28, learning_rate = 1.25000000


[batch = 3200] train_perplexity =     3.69, train_loss =  1.30, learning_rate = 1.25000000


[batch = 3300] train_perplexity =     3.59, train_loss =  1.28, learning_rate = 1.25000000


[batch = 3400] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 1.25000000


[batch = 3500] train_perplexity =     3.59, train_loss =  1.28, learning_rate = 1.25000000


[batch = 3600] train_perplexity =     3.60, train_loss =  1.28, learning_rate = 1.25000000


[batch = 3700] train_perplexity =     3.57, train_loss =  1.27, learning_rate = 1.25000000


[batch = 3800] train_perplexity =     3.56, train_loss =  1.27, learning_rate = 1.25000000


[batch = 3900] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 1.25000000


[batch = 4000] train_perplexity =     3.56, train_loss =  1.27, learning_rate = 1.25000000


[batch = 4100] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 1.25000000


[batch = 4200] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 1.25000000


[batch = 4300] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 1.25000000


[batch = 4400] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 1.25000000


[batch = 4500] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 1.25000000


[batch = 4600] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 1.25000000


[batch = 4700] train_perplexity =     3.56, train_loss =  1.27, learning_rate = 1.25000000


[batch = 4800] train_perplexity =     3.59, train_loss =  1.28, learning_rate = 1.25000000


[batch = 4900] train_perplexity =     3.61, train_loss =  1.28, learning_rate = 1.25000000


[batch = 5000] train_perplexity =     3.56, train_loss =  1.27, learning_rate = 1.25000000


[batch = 5100] train_perplexity =     3.62, train_loss =  1.29, learning_rate = 1.25000000


[batch = 5200] train_perplexity =     3.61, train_loss =  1.28, learning_rate = 1.25000000


[batch = 5300] train_perplexity =     3.59, train_loss =  1.28, learning_rate = 1.25000000


[batch = 5400] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 1.25000000


[batch = 5500] train_perplexity =     3.58, train_loss =  1.28, learning_rate = 1.25000000


[batch = 5600] train_perplexity =     3.60, train_loss =  1.28, learning_rate = 1.25000000


[batch = 5700] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 1.25000000


[batch = 5800] train_perplexity =     3.51, train_loss =  1.25, learning_rate = 1.25000000


[batch = 5900] train_perplexity =     3.58, train_loss =  1.28, learning_rate = 1.25000000


[batch = 6000] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 1.25000000


[batch = 6100] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 1.25000000


[batch = 6200] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 1.25000000


[batch = 6300] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 1.25000000


[batch = 6400] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 1.25000000


[batch = 6500] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 1.25000000


[batch = 6600] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 1.25000000


[batch = 6700] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 1.25000000


[batch = 6800] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 1.25000000


[batch = 6900] train_perplexity =     3.61, train_loss =  1.28, learning_rate = 1.25000000


[epoch =  16] validation_perplexity =     3.66, validation_loss =  1.30

Saved model to 'e200/ptb_char_lstm.pt'...
[epoch =  16] min_validation_perplexity =     3.66, min_validation_loss =  1.30


[batch =  100] train_perplexity =     3.80, train_loss =  1.34, learning_rate = 1.25000000


[batch =  200] train_perplexity =     3.60, train_loss =  1.28, learning_rate = 1.25000000


[batch =  300] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 1.25000000


[batch =  400] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 1.25000000


[batch =  500] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 1.25000000


[batch =  600] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 1.25000000


[batch =  700] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 1.25000000


[batch =  800] train_perplexity =     3.56, train_loss =  1.27, learning_rate = 1.25000000


[batch =  900] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 1.25000000


[batch = 1000] train_perplexity =     3.61, train_loss =  1.28, learning_rate = 1.25000000


[batch = 1100] train_perplexity =     3.67, train_loss =  1.30, learning_rate = 1.25000000


[batch = 1200] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 1.25000000


[batch = 1300] train_perplexity =     3.60, train_loss =  1.28, learning_rate = 1.25000000


[batch = 1400] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 1.25000000


[batch = 1500] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 1.25000000


[batch = 1600] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 1.25000000


[batch = 1700] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 1.25000000


[batch = 1800] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 1.25000000


[batch = 1900] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 1.25000000


[batch = 2000] train_perplexity =     3.64, train_loss =  1.29, learning_rate = 1.25000000


[batch = 2100] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 1.25000000


[batch = 2200] train_perplexity =     3.56, train_loss =  1.27, learning_rate = 1.25000000


[batch = 2300] train_perplexity =     3.65, train_loss =  1.29, learning_rate = 1.25000000


[batch = 2400] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 1.25000000


[batch = 2500] train_perplexity =     3.57, train_loss =  1.27, learning_rate = 1.25000000


[batch = 2600] train_perplexity =     3.62, train_loss =  1.29, learning_rate = 1.25000000


[batch = 2700] train_perplexity =     3.64, train_loss =  1.29, learning_rate = 1.25000000


[batch = 2800] train_perplexity =     3.56, train_loss =  1.27, learning_rate = 1.25000000


[batch = 2900] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 1.25000000


[batch = 3000] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 1.25000000


[batch = 3100] train_perplexity =     3.58, train_loss =  1.28, learning_rate = 1.25000000


[batch = 3200] train_perplexity =     3.68, train_loss =  1.30, learning_rate = 1.25000000


[batch = 3300] train_perplexity =     3.59, train_loss =  1.28, learning_rate = 1.25000000


[batch = 3400] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 1.25000000


[batch = 3500] train_perplexity =     3.59, train_loss =  1.28, learning_rate = 1.25000000


[batch = 3600] train_perplexity =     3.60, train_loss =  1.28, learning_rate = 1.25000000


[batch = 3700] train_perplexity =     3.57, train_loss =  1.27, learning_rate = 1.25000000


[batch = 3800] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 1.25000000


[batch = 3900] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 1.25000000


[batch = 4000] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 1.25000000


[batch = 4100] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 1.25000000


[batch = 4200] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 1.25000000


[batch = 4300] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 1.25000000


[batch = 4400] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 1.25000000


[batch = 4500] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 1.25000000


[batch = 4600] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 1.25000000


[batch = 4700] train_perplexity =     3.56, train_loss =  1.27, learning_rate = 1.25000000


[batch = 4800] train_perplexity =     3.58, train_loss =  1.28, learning_rate = 1.25000000


[batch = 4900] train_perplexity =     3.61, train_loss =  1.28, learning_rate = 1.25000000


[batch = 5000] train_perplexity =     3.56, train_loss =  1.27, learning_rate = 1.25000000


[batch = 5100] train_perplexity =     3.61, train_loss =  1.28, learning_rate = 1.25000000


[batch = 5200] train_perplexity =     3.61, train_loss =  1.28, learning_rate = 1.25000000


[batch = 5300] train_perplexity =     3.59, train_loss =  1.28, learning_rate = 1.25000000


[batch = 5400] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 1.25000000


[batch = 5500] train_perplexity =     3.58, train_loss =  1.27, learning_rate = 1.25000000


[batch = 5600] train_perplexity =     3.59, train_loss =  1.28, learning_rate = 1.25000000


[batch = 5700] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 1.25000000


[batch = 5800] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 1.25000000


[batch = 5900] train_perplexity =     3.58, train_loss =  1.27, learning_rate = 1.25000000


[batch = 6000] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 1.25000000


[batch = 6100] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 1.25000000


[batch = 6200] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 1.25000000


[batch = 6300] train_perplexity =     3.51, train_loss =  1.25, learning_rate = 1.25000000


[batch = 6400] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 1.25000000


[batch = 6500] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 1.25000000


[batch = 6600] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 1.25000000


[batch = 6700] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 1.25000000


[batch = 6800] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 1.25000000


[batch = 6900] train_perplexity =     3.60, train_loss =  1.28, learning_rate = 1.25000000


[epoch =  17] validation_perplexity =     3.66, validation_loss =  1.30

Saved model to 'e200/ptb_char_lstm.pt'...
[epoch =  17] min_validation_perplexity =     3.66, min_validation_loss =  1.30


[batch =  100] train_perplexity =     3.80, train_loss =  1.33, learning_rate = 1.25000000


[batch =  200] train_perplexity =     3.60, train_loss =  1.28, learning_rate = 1.25000000


[batch =  300] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 1.25000000


[batch =  400] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 1.25000000


[batch =  500] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 1.25000000


[batch =  600] train_perplexity =     3.38, train_loss =  1.22, learning_rate = 1.25000000


[batch =  700] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 1.25000000


[batch =  800] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 1.25000000


[batch =  900] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 1.25000000


[batch = 1000] train_perplexity =     3.61, train_loss =  1.28, learning_rate = 1.25000000


[batch = 1100] train_perplexity =     3.67, train_loss =  1.30, learning_rate = 1.25000000


[batch = 1200] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 1.25000000


[batch = 1300] train_perplexity =     3.59, train_loss =  1.28, learning_rate = 1.25000000


[batch = 1400] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 1.25000000


[batch = 1500] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 1.25000000


[batch = 1600] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 1.25000000


[batch = 1700] train_perplexity =     3.51, train_loss =  1.25, learning_rate = 1.25000000


[batch = 1800] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 1.25000000


[batch = 1900] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 1.25000000


[batch = 2000] train_perplexity =     3.63, train_loss =  1.29, learning_rate = 1.25000000


[batch = 2100] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 1.25000000


[batch = 2200] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 1.25000000


[batch = 2300] train_perplexity =     3.64, train_loss =  1.29, learning_rate = 1.25000000


[batch = 2400] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 1.25000000


[batch = 2500] train_perplexity =     3.56, train_loss =  1.27, learning_rate = 1.25000000


[batch = 2600] train_perplexity =     3.62, train_loss =  1.29, learning_rate = 1.25000000


[batch = 2700] train_perplexity =     3.63, train_loss =  1.29, learning_rate = 1.25000000


[batch = 2800] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 1.25000000


[batch = 2900] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 1.25000000


[batch = 3000] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 1.25000000


[batch = 3100] train_perplexity =     3.58, train_loss =  1.28, learning_rate = 1.25000000


[batch = 3200] train_perplexity =     3.68, train_loss =  1.30, learning_rate = 1.25000000


[batch = 3300] train_perplexity =     3.59, train_loss =  1.28, learning_rate = 1.25000000


[batch = 3400] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 1.25000000


[batch = 3500] train_perplexity =     3.59, train_loss =  1.28, learning_rate = 1.25000000


[batch = 3600] train_perplexity =     3.59, train_loss =  1.28, learning_rate = 1.25000000


[batch = 3700] train_perplexity =     3.56, train_loss =  1.27, learning_rate = 1.25000000


[batch = 3800] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 1.25000000


[batch = 3900] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 1.25000000


[batch = 4000] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 1.25000000


[batch = 4100] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 1.25000000


[batch = 4200] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 1.25000000


[batch = 4300] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 1.25000000


[batch = 4400] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 1.25000000


[batch = 4500] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 1.25000000


[batch = 4600] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 1.25000000


[batch = 4700] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 1.25000000


[batch = 4800] train_perplexity =     3.58, train_loss =  1.28, learning_rate = 1.25000000


[batch = 4900] train_perplexity =     3.60, train_loss =  1.28, learning_rate = 1.25000000


[batch = 5000] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 1.25000000


[batch = 5100] train_perplexity =     3.61, train_loss =  1.28, learning_rate = 1.25000000


[batch = 5200] train_perplexity =     3.60, train_loss =  1.28, learning_rate = 1.25000000


[batch = 5300] train_perplexity =     3.58, train_loss =  1.28, learning_rate = 1.25000000


[batch = 5400] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 1.25000000


[batch = 5500] train_perplexity =     3.58, train_loss =  1.27, learning_rate = 1.25000000


[batch = 5600] train_perplexity =     3.59, train_loss =  1.28, learning_rate = 1.25000000


[batch = 5700] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 1.25000000


[batch = 5800] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 1.25000000


[batch = 5900] train_perplexity =     3.58, train_loss =  1.27, learning_rate = 1.25000000


[batch = 6000] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 1.25000000


[batch = 6100] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 1.25000000


[batch = 6200] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 1.25000000


[batch = 6300] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 1.25000000


[batch = 6400] train_perplexity =     3.47, train_loss =  1.25, learning_rate = 1.25000000


[batch = 6500] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 1.25000000


[batch = 6600] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 1.25000000


[batch = 6700] train_perplexity =     3.47, train_loss =  1.25, learning_rate = 1.25000000


[batch = 6800] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 1.25000000


[batch = 6900] train_perplexity =     3.60, train_loss =  1.28, learning_rate = 1.25000000


[epoch =  18] validation_perplexity =     3.65, validation_loss =  1.30

Saved model to 'e200/ptb_char_lstm.pt'...
[epoch =  18] min_validation_perplexity =     3.65, min_validation_loss =  1.30


[batch =  100] train_perplexity =     3.79, train_loss =  1.33, learning_rate = 1.25000000


[batch =  200] train_perplexity =     3.59, train_loss =  1.28, learning_rate = 1.25000000


[batch =  300] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 1.25000000


[batch =  400] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 1.25000000


[batch =  500] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 1.25000000


[batch =  600] train_perplexity =     3.38, train_loss =  1.22, learning_rate = 1.25000000


[batch =  700] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 1.25000000


[batch =  800] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 1.25000000


[batch =  900] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 1.25000000


[batch = 1000] train_perplexity =     3.60, train_loss =  1.28, learning_rate = 1.25000000


[batch = 1100] train_perplexity =     3.66, train_loss =  1.30, learning_rate = 1.25000000


[batch = 1200] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 1.25000000


[batch = 1300] train_perplexity =     3.59, train_loss =  1.28, learning_rate = 1.25000000


[batch = 1400] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 1.25000000


[batch = 1500] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 1.25000000


[batch = 1600] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 1.25000000


[batch = 1700] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 1.25000000


[batch = 1800] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 1.25000000


[batch = 1900] train_perplexity =     3.51, train_loss =  1.25, learning_rate = 1.25000000


[batch = 2000] train_perplexity =     3.63, train_loss =  1.29, learning_rate = 1.25000000


[batch = 2100] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 1.25000000


[batch = 2200] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 1.25000000


[batch = 2300] train_perplexity =     3.63, train_loss =  1.29, learning_rate = 1.25000000


[batch = 2400] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 1.25000000


[batch = 2500] train_perplexity =     3.56, train_loss =  1.27, learning_rate = 1.25000000


[batch = 2600] train_perplexity =     3.62, train_loss =  1.29, learning_rate = 1.25000000


[batch = 2700] train_perplexity =     3.63, train_loss =  1.29, learning_rate = 1.25000000


[batch = 2800] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 1.25000000


[batch = 2900] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 1.25000000


[batch = 3000] train_perplexity =     3.47, train_loss =  1.25, learning_rate = 1.25000000


[batch = 3100] train_perplexity =     3.57, train_loss =  1.27, learning_rate = 1.25000000


[batch = 3200] train_perplexity =     3.67, train_loss =  1.30, learning_rate = 1.25000000


[batch = 3300] train_perplexity =     3.58, train_loss =  1.28, learning_rate = 1.25000000


[batch = 3400] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 1.25000000


[batch = 3500] train_perplexity =     3.58, train_loss =  1.28, learning_rate = 1.25000000


[batch = 3600] train_perplexity =     3.59, train_loss =  1.28, learning_rate = 1.25000000


[batch = 3700] train_perplexity =     3.56, train_loss =  1.27, learning_rate = 1.25000000


[batch = 3800] train_perplexity =     3.54, train_loss =  1.27, learning_rate = 1.25000000


[batch = 3900] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 1.25000000


[batch = 4000] train_perplexity =     3.54, train_loss =  1.27, learning_rate = 1.25000000


[batch = 4100] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 1.25000000


[batch = 4200] train_perplexity =     3.51, train_loss =  1.25, learning_rate = 1.25000000


[batch = 4300] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 1.25000000


[batch = 4400] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 1.25000000


[batch = 4500] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 1.25000000


[batch = 4600] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 1.25000000


[batch = 4700] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 1.25000000


[batch = 4800] train_perplexity =     3.57, train_loss =  1.27, learning_rate = 1.25000000


[batch = 4900] train_perplexity =     3.59, train_loss =  1.28, learning_rate = 1.25000000


[batch = 5000] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 1.25000000


[batch = 5100] train_perplexity =     3.60, train_loss =  1.28, learning_rate = 1.25000000


[batch = 5200] train_perplexity =     3.60, train_loss =  1.28, learning_rate = 1.25000000


[batch = 5300] train_perplexity =     3.58, train_loss =  1.28, learning_rate = 1.25000000


[batch = 5400] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 1.25000000


[batch = 5500] train_perplexity =     3.57, train_loss =  1.27, learning_rate = 1.25000000


[batch = 5600] train_perplexity =     3.59, train_loss =  1.28, learning_rate = 1.25000000


[batch = 5700] train_perplexity =     3.38, train_loss =  1.22, learning_rate = 1.25000000


[batch = 5800] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 1.25000000


[batch = 5900] train_perplexity =     3.57, train_loss =  1.27, learning_rate = 1.25000000


[batch = 6000] train_perplexity =     3.54, train_loss =  1.27, learning_rate = 1.25000000


[batch = 6100] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 1.25000000


[batch = 6200] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 1.25000000


[batch = 6300] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 1.25000000


[batch = 6400] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 1.25000000


[batch = 6500] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 1.25000000


[batch = 6600] train_perplexity =     3.51, train_loss =  1.25, learning_rate = 1.25000000


[batch = 6700] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 1.25000000


[batch = 6800] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 1.25000000


[batch = 6900] train_perplexity =     3.60, train_loss =  1.28, learning_rate = 1.25000000


[epoch =  19] validation_perplexity =     3.65, validation_loss =  1.30

Saved model to 'e200/ptb_char_lstm.pt'...
[epoch =  19] min_validation_perplexity =     3.65, min_validation_loss =  1.30


[batch =  100] train_perplexity =     3.79, train_loss =  1.33, learning_rate = 1.25000000


[batch =  200] train_perplexity =     3.59, train_loss =  1.28, learning_rate = 1.25000000


[batch =  300] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 1.25000000


[batch =  400] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 1.25000000


[batch =  500] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 1.25000000


[batch =  600] train_perplexity =     3.37, train_loss =  1.22, learning_rate = 1.25000000


[batch =  700] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 1.25000000


[batch =  800] train_perplexity =     3.54, train_loss =  1.27, learning_rate = 1.25000000


[batch =  900] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 1.25000000


[batch = 1000] train_perplexity =     3.60, train_loss =  1.28, learning_rate = 1.25000000


[batch = 1100] train_perplexity =     3.66, train_loss =  1.30, learning_rate = 1.25000000


[batch = 1200] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 1.25000000


[batch = 1300] train_perplexity =     3.58, train_loss =  1.28, learning_rate = 1.25000000


[batch = 1400] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 1.25000000


[batch = 1500] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 1.25000000


[batch = 1600] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 1.25000000


[batch = 1700] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 1.25000000


[batch = 1800] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 1.25000000


[batch = 1900] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 1.25000000


[batch = 2000] train_perplexity =     3.63, train_loss =  1.29, learning_rate = 1.25000000


[batch = 2100] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 1.25000000


[batch = 2200] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 1.25000000


[batch = 2300] train_perplexity =     3.63, train_loss =  1.29, learning_rate = 1.25000000


[batch = 2400] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 1.25000000


[batch = 2500] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 1.25000000


[batch = 2600] train_perplexity =     3.61, train_loss =  1.28, learning_rate = 1.25000000


[batch = 2700] train_perplexity =     3.63, train_loss =  1.29, learning_rate = 1.25000000


[batch = 2800] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 1.25000000


[batch = 2900] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 1.25000000


[batch = 3000] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 1.25000000


[batch = 3100] train_perplexity =     3.57, train_loss =  1.27, learning_rate = 1.25000000


[batch = 3200] train_perplexity =     3.67, train_loss =  1.30, learning_rate = 1.25000000


[batch = 3300] train_perplexity =     3.58, train_loss =  1.28, learning_rate = 1.25000000


[batch = 3400] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 1.25000000


[batch = 3500] train_perplexity =     3.58, train_loss =  1.28, learning_rate = 1.25000000


[batch = 3600] train_perplexity =     3.59, train_loss =  1.28, learning_rate = 1.25000000


[batch = 3700] train_perplexity =     3.56, train_loss =  1.27, learning_rate = 1.25000000


[batch = 3800] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 1.25000000


[batch = 3900] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 1.25000000


[batch = 4000] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 1.25000000


[batch = 4100] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 1.25000000


[batch = 4200] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 1.25000000


[batch = 4300] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 1.25000000


[batch = 4400] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 1.25000000


[batch = 4500] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 1.25000000


[batch = 4600] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 1.25000000


[batch = 4700] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 1.25000000


[batch = 4800] train_perplexity =     3.57, train_loss =  1.27, learning_rate = 1.25000000


[batch = 4900] train_perplexity =     3.59, train_loss =  1.28, learning_rate = 1.25000000


[batch = 5000] train_perplexity =     3.54, train_loss =  1.27, learning_rate = 1.25000000


[batch = 5100] train_perplexity =     3.60, train_loss =  1.28, learning_rate = 1.25000000


[batch = 5200] train_perplexity =     3.60, train_loss =  1.28, learning_rate = 1.25000000


[batch = 5300] train_perplexity =     3.58, train_loss =  1.27, learning_rate = 1.25000000


[batch = 5400] train_perplexity =     3.51, train_loss =  1.25, learning_rate = 1.25000000


[batch = 5500] train_perplexity =     3.57, train_loss =  1.27, learning_rate = 1.25000000


[batch = 5600] train_perplexity =     3.58, train_loss =  1.28, learning_rate = 1.25000000


[batch = 5700] train_perplexity =     3.38, train_loss =  1.22, learning_rate = 1.25000000


[batch = 5800] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 1.25000000


[batch = 5900] train_perplexity =     3.57, train_loss =  1.27, learning_rate = 1.25000000


[batch = 6000] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 1.25000000


[batch = 6100] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 1.25000000


[batch = 6200] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 1.25000000


[batch = 6300] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 1.25000000


[batch = 6400] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 1.25000000


[batch = 6500] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 1.25000000


[batch = 6600] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 1.25000000


[batch = 6700] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 1.25000000


[batch = 6800] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 1.25000000


[batch = 6900] train_perplexity =     3.59, train_loss =  1.28, learning_rate = 1.25000000


[epoch =  20] validation_perplexity =     3.65, validation_loss =  1.29

Saved model to 'e200/ptb_char_lstm.pt'...
[epoch =  20] min_validation_perplexity =     3.65, min_validation_loss =  1.29


[batch =  100] train_perplexity =     3.78, train_loss =  1.33, learning_rate = 1.25000000


[batch =  200] train_perplexity =     3.59, train_loss =  1.28, learning_rate = 1.25000000


[batch =  300] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 1.25000000


[batch =  400] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 1.25000000


[batch =  500] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 1.25000000


[batch =  600] train_perplexity =     3.37, train_loss =  1.21, learning_rate = 1.25000000


[batch =  700] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 1.25000000


[batch =  800] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 1.25000000


[batch =  900] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 1.25000000


[batch = 1000] train_perplexity =     3.59, train_loss =  1.28, learning_rate = 1.25000000


[batch = 1100] train_perplexity =     3.65, train_loss =  1.30, learning_rate = 1.25000000


[batch = 1200] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 1.25000000


[batch = 1300] train_perplexity =     3.58, train_loss =  1.28, learning_rate = 1.25000000


[batch = 1400] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 1.25000000


[batch = 1500] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 1.25000000


[batch = 1600] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 1.25000000


[batch = 1700] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 1.25000000


[batch = 1800] train_perplexity =     3.51, train_loss =  1.25, learning_rate = 1.25000000


[batch = 1900] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 1.25000000


[batch = 2000] train_perplexity =     3.62, train_loss =  1.29, learning_rate = 1.25000000


[batch = 2100] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 1.25000000


[batch = 2200] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 1.25000000


[batch = 2300] train_perplexity =     3.63, train_loss =  1.29, learning_rate = 1.25000000


[batch = 2400] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 1.25000000


[batch = 2500] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 1.25000000


[batch = 2600] train_perplexity =     3.61, train_loss =  1.28, learning_rate = 1.25000000


[batch = 2700] train_perplexity =     3.62, train_loss =  1.29, learning_rate = 1.25000000


[batch = 2800] train_perplexity =     3.54, train_loss =  1.27, learning_rate = 1.25000000


[batch = 2900] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 1.25000000


[batch = 3000] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 1.25000000


[batch = 3100] train_perplexity =     3.57, train_loss =  1.27, learning_rate = 1.25000000


[batch = 3200] train_perplexity =     3.67, train_loss =  1.30, learning_rate = 1.25000000


[batch = 3300] train_perplexity =     3.58, train_loss =  1.27, learning_rate = 1.25000000


[batch = 3400] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 1.25000000


[batch = 3500] train_perplexity =     3.58, train_loss =  1.28, learning_rate = 1.25000000


[batch = 3600] train_perplexity =     3.58, train_loss =  1.28, learning_rate = 1.25000000


[batch = 3700] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 1.25000000


[batch = 3800] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 1.25000000


[batch = 3900] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 1.25000000


[batch = 4000] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 1.25000000


[batch = 4100] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 1.25000000


[batch = 4200] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 1.25000000


[batch = 4300] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 1.25000000


[batch = 4400] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 1.25000000


[batch = 4500] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 1.25000000


[batch = 4600] train_perplexity =     3.51, train_loss =  1.25, learning_rate = 1.25000000


[batch = 4700] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 1.25000000


[batch = 4800] train_perplexity =     3.57, train_loss =  1.27, learning_rate = 1.25000000


[batch = 4900] train_perplexity =     3.59, train_loss =  1.28, learning_rate = 1.25000000


[batch = 5000] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 1.25000000


[batch = 5100] train_perplexity =     3.59, train_loss =  1.28, learning_rate = 1.25000000


[batch = 5200] train_perplexity =     3.59, train_loss =  1.28, learning_rate = 1.25000000


[batch = 5300] train_perplexity =     3.58, train_loss =  1.27, learning_rate = 1.25000000


[batch = 5400] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 1.25000000


[batch = 5500] train_perplexity =     3.57, train_loss =  1.27, learning_rate = 1.25000000


[batch = 5600] train_perplexity =     3.58, train_loss =  1.28, learning_rate = 1.25000000


[batch = 5700] train_perplexity =     3.38, train_loss =  1.22, learning_rate = 1.25000000


[batch = 5800] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 1.25000000


[batch = 5900] train_perplexity =     3.57, train_loss =  1.27, learning_rate = 1.25000000


[batch = 6000] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 1.25000000


[batch = 6100] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 1.25000000


[batch = 6200] train_perplexity =     3.47, train_loss =  1.25, learning_rate = 1.25000000


[batch = 6300] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 1.25000000


[batch = 6400] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 1.25000000


[batch = 6500] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 1.25000000


[batch = 6600] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 1.25000000


[batch = 6700] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 1.25000000


[batch = 6800] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 1.25000000


[batch = 6900] train_perplexity =     3.59, train_loss =  1.28, learning_rate = 1.25000000


[epoch =  21] validation_perplexity =     3.65, validation_loss =  1.29

Saved model to 'e200/ptb_char_lstm.pt'...
[epoch =  21] min_validation_perplexity =     3.65, min_validation_loss =  1.29


[batch =  100] train_perplexity =     3.78, train_loss =  1.33, learning_rate = 1.25000000


[batch =  200] train_perplexity =     3.58, train_loss =  1.28, learning_rate = 1.25000000


[batch =  300] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 1.25000000


[batch =  400] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 1.25000000


[batch =  500] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 1.25000000


[batch =  600] train_perplexity =     3.37, train_loss =  1.21, learning_rate = 1.25000000


[batch =  700] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 1.25000000


[batch =  800] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 1.25000000


[batch =  900] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 1.25000000


[batch = 1000] train_perplexity =     3.59, train_loss =  1.28, learning_rate = 1.25000000


[batch = 1100] train_perplexity =     3.65, train_loss =  1.29, learning_rate = 1.25000000


[batch = 1200] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 1.25000000


[batch = 1300] train_perplexity =     3.58, train_loss =  1.27, learning_rate = 1.25000000


[batch = 1400] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 1.25000000


[batch = 1500] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 1.25000000


[batch = 1600] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 1.25000000


[batch = 1700] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 1.25000000


[batch = 1800] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 1.25000000


[batch = 1900] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 1.25000000


[batch = 2000] train_perplexity =     3.62, train_loss =  1.29, learning_rate = 1.25000000


[batch = 2100] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 1.25000000


[batch = 2200] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 1.25000000


[batch = 2300] train_perplexity =     3.62, train_loss =  1.29, learning_rate = 1.25000000


[batch = 2400] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 1.25000000


[batch = 2500] train_perplexity =     3.54, train_loss =  1.27, learning_rate = 1.25000000


[batch = 2600] train_perplexity =     3.61, train_loss =  1.28, learning_rate = 1.25000000


[batch = 2700] train_perplexity =     3.62, train_loss =  1.29, learning_rate = 1.25000000


[batch = 2800] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 1.25000000


[batch = 2900] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 1.25000000


[batch = 3000] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 1.25000000


[batch = 3100] train_perplexity =     3.57, train_loss =  1.27, learning_rate = 1.25000000


[batch = 3200] train_perplexity =     3.66, train_loss =  1.30, learning_rate = 1.25000000


[batch = 3300] train_perplexity =     3.57, train_loss =  1.27, learning_rate = 1.25000000


[batch = 3400] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 1.25000000


[batch = 3500] train_perplexity =     3.57, train_loss =  1.27, learning_rate = 1.25000000


[batch = 3600] train_perplexity =     3.58, train_loss =  1.28, learning_rate = 1.25000000


[batch = 3700] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 1.25000000


[batch = 3800] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 1.25000000


[batch = 3900] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 1.25000000


[batch = 4000] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 1.25000000


[batch = 4100] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 1.25000000


[batch = 4200] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 1.25000000


[batch = 4300] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 1.25000000


[batch = 4400] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 1.25000000


[batch = 4500] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 1.25000000


[batch = 4600] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 1.25000000


[batch = 4700] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 1.25000000


[batch = 4800] train_perplexity =     3.57, train_loss =  1.27, learning_rate = 1.25000000


[batch = 4900] train_perplexity =     3.58, train_loss =  1.28, learning_rate = 1.25000000


[batch = 5000] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 1.25000000


[batch = 5100] train_perplexity =     3.59, train_loss =  1.28, learning_rate = 1.25000000


[batch = 5200] train_perplexity =     3.59, train_loss =  1.28, learning_rate = 1.25000000


[batch = 5300] train_perplexity =     3.57, train_loss =  1.27, learning_rate = 1.25000000


[batch = 5400] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 1.25000000


[batch = 5500] train_perplexity =     3.56, train_loss =  1.27, learning_rate = 1.25000000


[batch = 5600] train_perplexity =     3.58, train_loss =  1.27, learning_rate = 1.25000000


[batch = 5700] train_perplexity =     3.38, train_loss =  1.22, learning_rate = 1.25000000


[batch = 5800] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 1.25000000


[batch = 5900] train_perplexity =     3.56, train_loss =  1.27, learning_rate = 1.25000000


[batch = 6000] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 1.25000000


[batch = 6100] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 1.25000000


[batch = 6200] train_perplexity =     3.47, train_loss =  1.25, learning_rate = 1.25000000


[batch = 6300] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 1.25000000


[batch = 6400] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 1.25000000


[batch = 6500] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 1.25000000


[batch = 6600] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 1.25000000


[batch = 6700] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 1.25000000


[batch = 6800] train_perplexity =     3.51, train_loss =  1.25, learning_rate = 1.25000000


[batch = 6900] train_perplexity =     3.59, train_loss =  1.28, learning_rate = 1.25000000


[epoch =  22] validation_perplexity =     3.65, validation_loss =  1.29

[epoch =  22] annealing learning_rate = 0.31250000

[batch =  100] train_perplexity =     3.79, train_loss =  1.33, learning_rate = 0.31250000


[batch =  200] train_perplexity =     3.59, train_loss =  1.28, learning_rate = 0.31250000


[batch =  300] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.31250000


[batch =  400] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.31250000


[batch =  500] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.31250000


[batch =  600] train_perplexity =     3.36, train_loss =  1.21, learning_rate = 0.31250000


[batch =  700] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.31250000


[batch =  800] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.31250000


[batch =  900] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.31250000


[batch = 1000] train_perplexity =     3.58, train_loss =  1.28, learning_rate = 0.31250000


[batch = 1100] train_perplexity =     3.64, train_loss =  1.29, learning_rate = 0.31250000


[batch = 1200] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.31250000


[batch = 1300] train_perplexity =     3.57, train_loss =  1.27, learning_rate = 0.31250000


[batch = 1400] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.31250000


[batch = 1500] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.31250000


[batch = 1600] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.31250000


[batch = 1700] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.31250000


[batch = 1800] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.31250000


[batch = 1900] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.31250000


[batch = 2000] train_perplexity =     3.61, train_loss =  1.28, learning_rate = 0.31250000


[batch = 2100] train_perplexity =     3.47, train_loss =  1.25, learning_rate = 0.31250000


[batch = 2200] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.31250000


[batch = 2300] train_perplexity =     3.61, train_loss =  1.28, learning_rate = 0.31250000


[batch = 2400] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.31250000


[batch = 2500] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.31250000


[batch = 2600] train_perplexity =     3.59, train_loss =  1.28, learning_rate = 0.31250000


[batch = 2700] train_perplexity =     3.61, train_loss =  1.28, learning_rate = 0.31250000


[batch = 2800] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.31250000


[batch = 2900] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.31250000


[batch = 3000] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.31250000


[batch = 3100] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 0.31250000


[batch = 3200] train_perplexity =     3.65, train_loss =  1.30, learning_rate = 0.31250000


[batch = 3300] train_perplexity =     3.56, train_loss =  1.27, learning_rate = 0.31250000


[batch = 3400] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.31250000


[batch = 3500] train_perplexity =     3.56, train_loss =  1.27, learning_rate = 0.31250000


[batch = 3600] train_perplexity =     3.56, train_loss =  1.27, learning_rate = 0.31250000


[batch = 3700] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.31250000


[batch = 3800] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.31250000


[batch = 3900] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.31250000


[batch = 4000] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.31250000


[batch = 4100] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.31250000


[batch = 4200] train_perplexity =     3.47, train_loss =  1.25, learning_rate = 0.31250000


[batch = 4300] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.31250000


[batch = 4400] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.31250000


[batch = 4500] train_perplexity =     3.51, train_loss =  1.25, learning_rate = 0.31250000


[batch = 4600] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.31250000


[batch = 4700] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.31250000


[batch = 4800] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 0.31250000


[batch = 4900] train_perplexity =     3.56, train_loss =  1.27, learning_rate = 0.31250000


[batch = 5000] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.31250000


[batch = 5100] train_perplexity =     3.57, train_loss =  1.27, learning_rate = 0.31250000


[batch = 5200] train_perplexity =     3.57, train_loss =  1.27, learning_rate = 0.31250000


[batch = 5300] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 0.31250000


[batch = 5400] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.31250000


[batch = 5500] train_perplexity =     3.54, train_loss =  1.27, learning_rate = 0.31250000


[batch = 5600] train_perplexity =     3.56, train_loss =  1.27, learning_rate = 0.31250000


[batch = 5700] train_perplexity =     3.35, train_loss =  1.21, learning_rate = 0.31250000


[batch = 5800] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.31250000


[batch = 5900] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.31250000


[batch = 6000] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.31250000


[batch = 6100] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.31250000


[batch = 6200] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.31250000


[batch = 6300] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.31250000


[batch = 6400] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.31250000


[batch = 6500] train_perplexity =     3.38, train_loss =  1.22, learning_rate = 0.31250000


[batch = 6600] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.31250000


[batch = 6700] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.31250000


[batch = 6800] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.31250000


[batch = 6900] train_perplexity =     3.56, train_loss =  1.27, learning_rate = 0.31250000


[epoch =  23] validation_perplexity =     3.61, validation_loss =  1.28

Saved model to 'e200/ptb_char_lstm.pt'...
[epoch =  23] min_validation_perplexity =     3.61, min_validation_loss =  1.28


[batch =  100] train_perplexity =     3.76, train_loss =  1.32, learning_rate = 0.31250000


[batch =  200] train_perplexity =     3.57, train_loss =  1.27, learning_rate = 0.31250000


[batch =  300] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.31250000


[batch =  400] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.31250000


[batch =  500] train_perplexity =     3.38, train_loss =  1.22, learning_rate = 0.31250000


[batch =  600] train_perplexity =     3.35, train_loss =  1.21, learning_rate = 0.31250000


[batch =  700] train_perplexity =     3.38, train_loss =  1.22, learning_rate = 0.31250000


[batch =  800] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.31250000


[batch =  900] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.31250000


[batch = 1000] train_perplexity =     3.57, train_loss =  1.27, learning_rate = 0.31250000


[batch = 1100] train_perplexity =     3.63, train_loss =  1.29, learning_rate = 0.31250000


[batch = 1200] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.31250000


[batch = 1300] train_perplexity =     3.56, train_loss =  1.27, learning_rate = 0.31250000


[batch = 1400] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.31250000


[batch = 1500] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.31250000


[batch = 1600] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.31250000


[batch = 1700] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.31250000


[batch = 1800] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.31250000


[batch = 1900] train_perplexity =     3.47, train_loss =  1.25, learning_rate = 0.31250000


[batch = 2000] train_perplexity =     3.60, train_loss =  1.28, learning_rate = 0.31250000


[batch = 2100] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.31250000


[batch = 2200] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.31250000


[batch = 2300] train_perplexity =     3.60, train_loss =  1.28, learning_rate = 0.31250000


[batch = 2400] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.31250000


[batch = 2500] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.31250000


[batch = 2600] train_perplexity =     3.59, train_loss =  1.28, learning_rate = 0.31250000


[batch = 2700] train_perplexity =     3.60, train_loss =  1.28, learning_rate = 0.31250000


[batch = 2800] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.31250000


[batch = 2900] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.31250000


[batch = 3000] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.31250000


[batch = 3100] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 0.31250000


[batch = 3200] train_perplexity =     3.65, train_loss =  1.29, learning_rate = 0.31250000


[batch = 3300] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 0.31250000


[batch = 3400] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.31250000


[batch = 3500] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 0.31250000


[batch = 3600] train_perplexity =     3.56, train_loss =  1.27, learning_rate = 0.31250000


[batch = 3700] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.31250000


[batch = 3800] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.31250000


[batch = 3900] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.31250000


[batch = 4000] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.31250000


[batch = 4100] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.31250000


[batch = 4200] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.31250000


[batch = 4300] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.31250000


[batch = 4400] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.31250000


[batch = 4500] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.31250000


[batch = 4600] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.31250000


[batch = 4700] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.31250000


[batch = 4800] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 0.31250000


[batch = 4900] train_perplexity =     3.56, train_loss =  1.27, learning_rate = 0.31250000


[batch = 5000] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.31250000


[batch = 5100] train_perplexity =     3.57, train_loss =  1.27, learning_rate = 0.31250000


[batch = 5200] train_perplexity =     3.57, train_loss =  1.27, learning_rate = 0.31250000


[batch = 5300] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 0.31250000


[batch = 5400] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.31250000


[batch = 5500] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.31250000


[batch = 5600] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 0.31250000


[batch = 5700] train_perplexity =     3.35, train_loss =  1.21, learning_rate = 0.31250000


[batch = 5800] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.31250000


[batch = 5900] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.31250000


[batch = 6000] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.31250000


[batch = 6100] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.31250000


[batch = 6200] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.31250000


[batch = 6300] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.31250000


[batch = 6400] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.31250000


[batch = 6500] train_perplexity =     3.38, train_loss =  1.22, learning_rate = 0.31250000


[batch = 6600] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.31250000


[batch = 6700] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.31250000


[batch = 6800] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.31250000


[batch = 6900] train_perplexity =     3.56, train_loss =  1.27, learning_rate = 0.31250000


[epoch =  24] validation_perplexity =     3.60, validation_loss =  1.28

Saved model to 'e200/ptb_char_lstm.pt'...
[epoch =  24] min_validation_perplexity =     3.60, min_validation_loss =  1.28


[batch =  100] train_perplexity =     3.76, train_loss =  1.32, learning_rate = 0.31250000


[batch =  200] train_perplexity =     3.57, train_loss =  1.27, learning_rate = 0.31250000


[batch =  300] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.31250000


[batch =  400] train_perplexity =     3.38, train_loss =  1.22, learning_rate = 0.31250000


[batch =  500] train_perplexity =     3.38, train_loss =  1.22, learning_rate = 0.31250000


[batch =  600] train_perplexity =     3.35, train_loss =  1.21, learning_rate = 0.31250000


[batch =  700] train_perplexity =     3.37, train_loss =  1.22, learning_rate = 0.31250000


[batch =  800] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.31250000


[batch =  900] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.31250000


[batch = 1000] train_perplexity =     3.57, train_loss =  1.27, learning_rate = 0.31250000


[batch = 1100] train_perplexity =     3.63, train_loss =  1.29, learning_rate = 0.31250000


[batch = 1200] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.31250000


[batch = 1300] train_perplexity =     3.56, train_loss =  1.27, learning_rate = 0.31250000


[batch = 1400] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.31250000


[batch = 1500] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.31250000


[batch = 1600] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.31250000


[batch = 1700] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.31250000


[batch = 1800] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.31250000


[batch = 1900] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.31250000


[batch = 2000] train_perplexity =     3.60, train_loss =  1.28, learning_rate = 0.31250000


[batch = 2100] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.31250000


[batch = 2200] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.31250000


[batch = 2300] train_perplexity =     3.60, train_loss =  1.28, learning_rate = 0.31250000


[batch = 2400] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.31250000


[batch = 2500] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.31250000


[batch = 2600] train_perplexity =     3.58, train_loss =  1.28, learning_rate = 0.31250000


[batch = 2700] train_perplexity =     3.60, train_loss =  1.28, learning_rate = 0.31250000


[batch = 2800] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.31250000


[batch = 2900] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.31250000


[batch = 3000] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.31250000


[batch = 3100] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.31250000


[batch = 3200] train_perplexity =     3.64, train_loss =  1.29, learning_rate = 0.31250000


[batch = 3300] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 0.31250000


[batch = 3400] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.31250000


[batch = 3500] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 0.31250000


[batch = 3600] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 0.31250000


[batch = 3700] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.31250000


[batch = 3800] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.31250000


[batch = 3900] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.31250000


[batch = 4000] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.31250000


[batch = 4100] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.31250000


[batch = 4200] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.31250000


[batch = 4300] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.31250000


[batch = 4400] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.31250000


[batch = 4500] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.31250000


[batch = 4600] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.31250000


[batch = 4700] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.31250000


[batch = 4800] train_perplexity =     3.54, train_loss =  1.27, learning_rate = 0.31250000


[batch = 4900] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 0.31250000


[batch = 5000] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.31250000


[batch = 5100] train_perplexity =     3.57, train_loss =  1.27, learning_rate = 0.31250000


[batch = 5200] train_perplexity =     3.56, train_loss =  1.27, learning_rate = 0.31250000


[batch = 5300] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 0.31250000


[batch = 5400] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.31250000


[batch = 5500] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.31250000


[batch = 5600] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 0.31250000


[batch = 5700] train_perplexity =     3.35, train_loss =  1.21, learning_rate = 0.31250000


[batch = 5800] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.31250000


[batch = 5900] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.31250000


[batch = 6000] train_perplexity =     3.51, train_loss =  1.25, learning_rate = 0.31250000


[batch = 6100] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.31250000


[batch = 6200] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.31250000


[batch = 6300] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.31250000


[batch = 6400] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.31250000


[batch = 6500] train_perplexity =     3.37, train_loss =  1.22, learning_rate = 0.31250000


[batch = 6600] train_perplexity =     3.47, train_loss =  1.25, learning_rate = 0.31250000


[batch = 6700] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.31250000


[batch = 6800] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.31250000


[batch = 6900] train_perplexity =     3.56, train_loss =  1.27, learning_rate = 0.31250000


[epoch =  25] validation_perplexity =     3.60, validation_loss =  1.28

Saved model to 'e200/ptb_char_lstm.pt'...
[epoch =  25] min_validation_perplexity =     3.60, min_validation_loss =  1.28


[batch =  100] train_perplexity =     3.75, train_loss =  1.32, learning_rate = 0.31250000


[batch =  200] train_perplexity =     3.56, train_loss =  1.27, learning_rate = 0.31250000


[batch =  300] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.31250000


[batch =  400] train_perplexity =     3.38, train_loss =  1.22, learning_rate = 0.31250000


[batch =  500] train_perplexity =     3.38, train_loss =  1.22, learning_rate = 0.31250000


[batch =  600] train_perplexity =     3.34, train_loss =  1.21, learning_rate = 0.31250000


[batch =  700] train_perplexity =     3.37, train_loss =  1.22, learning_rate = 0.31250000


[batch =  800] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.31250000


[batch =  900] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.31250000


[batch = 1000] train_perplexity =     3.56, train_loss =  1.27, learning_rate = 0.31250000


[batch = 1100] train_perplexity =     3.62, train_loss =  1.29, learning_rate = 0.31250000


[batch = 1200] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.31250000


[batch = 1300] train_perplexity =     3.56, train_loss =  1.27, learning_rate = 0.31250000


[batch = 1400] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.31250000


[batch = 1500] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.31250000


[batch = 1600] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.31250000


[batch = 1700] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.31250000


[batch = 1800] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.31250000


[batch = 1900] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.31250000


[batch = 2000] train_perplexity =     3.60, train_loss =  1.28, learning_rate = 0.31250000


[batch = 2100] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.31250000


[batch = 2200] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.31250000


[batch = 2300] train_perplexity =     3.60, train_loss =  1.28, learning_rate = 0.31250000


[batch = 2400] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.31250000


[batch = 2500] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.31250000


[batch = 2600] train_perplexity =     3.58, train_loss =  1.28, learning_rate = 0.31250000


[batch = 2700] train_perplexity =     3.59, train_loss =  1.28, learning_rate = 0.31250000


[batch = 2800] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.31250000


[batch = 2900] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.31250000


[batch = 3000] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.31250000


[batch = 3100] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.31250000


[batch = 3200] train_perplexity =     3.64, train_loss =  1.29, learning_rate = 0.31250000


[batch = 3300] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 0.31250000


[batch = 3400] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.31250000


[batch = 3500] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 0.31250000


[batch = 3600] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 0.31250000


[batch = 3700] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.31250000


[batch = 3800] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.31250000


[batch = 3900] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.31250000


[batch = 4000] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.31250000


[batch = 4100] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.31250000


[batch = 4200] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.31250000


[batch = 4300] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.31250000


[batch = 4400] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.31250000


[batch = 4500] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.31250000


[batch = 4600] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.31250000


[batch = 4700] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.31250000


[batch = 4800] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.31250000


[batch = 4900] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 0.31250000


[batch = 5000] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.31250000


[batch = 5100] train_perplexity =     3.57, train_loss =  1.27, learning_rate = 0.31250000


[batch = 5200] train_perplexity =     3.56, train_loss =  1.27, learning_rate = 0.31250000


[batch = 5300] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 0.31250000


[batch = 5400] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.31250000


[batch = 5500] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.31250000


[batch = 5600] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 0.31250000


[batch = 5700] train_perplexity =     3.35, train_loss =  1.21, learning_rate = 0.31250000


[batch = 5800] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.31250000


[batch = 5900] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.31250000


[batch = 6000] train_perplexity =     3.51, train_loss =  1.25, learning_rate = 0.31250000


[batch = 6100] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.31250000


[batch = 6200] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.31250000


[batch = 6300] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.31250000


[batch = 6400] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.31250000


[batch = 6500] train_perplexity =     3.37, train_loss =  1.22, learning_rate = 0.31250000


[batch = 6600] train_perplexity =     3.47, train_loss =  1.25, learning_rate = 0.31250000


[batch = 6700] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.31250000


[batch = 6800] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.31250000


[batch = 6900] train_perplexity =     3.56, train_loss =  1.27, learning_rate = 0.31250000


[epoch =  26] validation_perplexity =     3.60, validation_loss =  1.28

Saved model to 'e200/ptb_char_lstm.pt'...
[epoch =  26] min_validation_perplexity =     3.60, min_validation_loss =  1.28


[batch =  100] train_perplexity =     3.75, train_loss =  1.32, learning_rate = 0.31250000


[batch =  200] train_perplexity =     3.56, train_loss =  1.27, learning_rate = 0.31250000


[batch =  300] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.31250000


[batch =  400] train_perplexity =     3.38, train_loss =  1.22, learning_rate = 0.31250000


[batch =  500] train_perplexity =     3.37, train_loss =  1.22, learning_rate = 0.31250000


[batch =  600] train_perplexity =     3.34, train_loss =  1.21, learning_rate = 0.31250000


[batch =  700] train_perplexity =     3.37, train_loss =  1.21, learning_rate = 0.31250000


[batch =  800] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.31250000


[batch =  900] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.31250000


[batch = 1000] train_perplexity =     3.56, train_loss =  1.27, learning_rate = 0.31250000


[batch = 1100] train_perplexity =     3.62, train_loss =  1.29, learning_rate = 0.31250000


[batch = 1200] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.31250000


[batch = 1300] train_perplexity =     3.56, train_loss =  1.27, learning_rate = 0.31250000


[batch = 1400] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.31250000


[batch = 1500] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.31250000


[batch = 1600] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.31250000


[batch = 1700] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.31250000


[batch = 1800] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.31250000


[batch = 1900] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.31250000


[batch = 2000] train_perplexity =     3.59, train_loss =  1.28, learning_rate = 0.31250000


[batch = 2100] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.31250000


[batch = 2200] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.31250000


[batch = 2300] train_perplexity =     3.59, train_loss =  1.28, learning_rate = 0.31250000


[batch = 2400] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.31250000


[batch = 2500] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.31250000


[batch = 2600] train_perplexity =     3.58, train_loss =  1.28, learning_rate = 0.31250000


[batch = 2700] train_perplexity =     3.59, train_loss =  1.28, learning_rate = 0.31250000


[batch = 2800] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.31250000


[batch = 2900] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.31250000


[batch = 3000] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.31250000


[batch = 3100] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.31250000


[batch = 3200] train_perplexity =     3.64, train_loss =  1.29, learning_rate = 0.31250000


[batch = 3300] train_perplexity =     3.54, train_loss =  1.27, learning_rate = 0.31250000


[batch = 3400] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.31250000


[batch = 3500] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 0.31250000


[batch = 3600] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 0.31250000


[batch = 3700] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.31250000


[batch = 3800] train_perplexity =     3.51, train_loss =  1.25, learning_rate = 0.31250000


[batch = 3900] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.31250000


[batch = 4000] train_perplexity =     3.51, train_loss =  1.25, learning_rate = 0.31250000


[batch = 4100] train_perplexity =     3.38, train_loss =  1.22, learning_rate = 0.31250000


[batch = 4200] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.31250000


[batch = 4300] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.31250000


[batch = 4400] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.31250000


[batch = 4500] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.31250000


[batch = 4600] train_perplexity =     3.47, train_loss =  1.25, learning_rate = 0.31250000


[batch = 4700] train_perplexity =     3.51, train_loss =  1.25, learning_rate = 0.31250000


[batch = 4800] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.31250000


[batch = 4900] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 0.31250000


[batch = 5000] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.31250000


[batch = 5100] train_perplexity =     3.56, train_loss =  1.27, learning_rate = 0.31250000


[batch = 5200] train_perplexity =     3.56, train_loss =  1.27, learning_rate = 0.31250000


[batch = 5300] train_perplexity =     3.54, train_loss =  1.27, learning_rate = 0.31250000


[batch = 5400] train_perplexity =     3.47, train_loss =  1.25, learning_rate = 0.31250000


[batch = 5500] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.31250000


[batch = 5600] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 0.31250000


[batch = 5700] train_perplexity =     3.35, train_loss =  1.21, learning_rate = 0.31250000


[batch = 5800] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.31250000


[batch = 5900] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.31250000


[batch = 6000] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.31250000


[batch = 6100] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.31250000


[batch = 6200] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.31250000


[batch = 6300] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.31250000


[batch = 6400] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.31250000


[batch = 6500] train_perplexity =     3.37, train_loss =  1.22, learning_rate = 0.31250000


[batch = 6600] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.31250000


[batch = 6700] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.31250000


[batch = 6800] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.31250000


[batch = 6900] train_perplexity =     3.56, train_loss =  1.27, learning_rate = 0.31250000


[epoch =  27] validation_perplexity =     3.60, validation_loss =  1.28

Saved model to 'e200/ptb_char_lstm.pt'...
[epoch =  27] min_validation_perplexity =     3.60, min_validation_loss =  1.28


[batch =  100] train_perplexity =     3.75, train_loss =  1.32, learning_rate = 0.31250000


[batch =  200] train_perplexity =     3.56, train_loss =  1.27, learning_rate = 0.31250000


[batch =  300] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.31250000


[batch =  400] train_perplexity =     3.38, train_loss =  1.22, learning_rate = 0.31250000


[batch =  500] train_perplexity =     3.37, train_loss =  1.22, learning_rate = 0.31250000


[batch =  600] train_perplexity =     3.34, train_loss =  1.21, learning_rate = 0.31250000


[batch =  700] train_perplexity =     3.37, train_loss =  1.21, learning_rate = 0.31250000


[batch =  800] train_perplexity =     3.51, train_loss =  1.25, learning_rate = 0.31250000


[batch =  900] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.31250000


[batch = 1000] train_perplexity =     3.56, train_loss =  1.27, learning_rate = 0.31250000


[batch = 1100] train_perplexity =     3.62, train_loss =  1.29, learning_rate = 0.31250000


[batch = 1200] train_perplexity =     3.51, train_loss =  1.25, learning_rate = 0.31250000


[batch = 1300] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 0.31250000


[batch = 1400] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.31250000


[batch = 1500] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.31250000


[batch = 1600] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.31250000


[batch = 1700] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.31250000


[batch = 1800] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.31250000


[batch = 1900] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.31250000


[batch = 2000] train_perplexity =     3.59, train_loss =  1.28, learning_rate = 0.31250000


[batch = 2100] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.31250000


[batch = 2200] train_perplexity =     3.51, train_loss =  1.25, learning_rate = 0.31250000


[batch = 2300] train_perplexity =     3.59, train_loss =  1.28, learning_rate = 0.31250000


[batch = 2400] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.31250000


[batch = 2500] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.31250000


[batch = 2600] train_perplexity =     3.58, train_loss =  1.27, learning_rate = 0.31250000


[batch = 2700] train_perplexity =     3.59, train_loss =  1.28, learning_rate = 0.31250000


[batch = 2800] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.31250000


[batch = 2900] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.31250000


[batch = 3000] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.31250000


[batch = 3100] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.31250000


[batch = 3200] train_perplexity =     3.64, train_loss =  1.29, learning_rate = 0.31250000


[batch = 3300] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.31250000


[batch = 3400] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.31250000


[batch = 3500] train_perplexity =     3.54, train_loss =  1.27, learning_rate = 0.31250000


[batch = 3600] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 0.31250000


[batch = 3700] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.31250000


[batch = 3800] train_perplexity =     3.51, train_loss =  1.25, learning_rate = 0.31250000


[batch = 3900] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.31250000


[batch = 4000] train_perplexity =     3.51, train_loss =  1.25, learning_rate = 0.31250000


[batch = 4100] train_perplexity =     3.38, train_loss =  1.22, learning_rate = 0.31250000


[batch = 4200] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.31250000


[batch = 4300] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.31250000


[batch = 4400] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.31250000


[batch = 4500] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.31250000


[batch = 4600] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.31250000


[batch = 4700] train_perplexity =     3.51, train_loss =  1.25, learning_rate = 0.31250000


[batch = 4800] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.31250000


[batch = 4900] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 0.31250000


[batch = 5000] train_perplexity =     3.51, train_loss =  1.25, learning_rate = 0.31250000


[batch = 5100] train_perplexity =     3.56, train_loss =  1.27, learning_rate = 0.31250000


[batch = 5200] train_perplexity =     3.56, train_loss =  1.27, learning_rate = 0.31250000


[batch = 5300] train_perplexity =     3.54, train_loss =  1.27, learning_rate = 0.31250000


[batch = 5400] train_perplexity =     3.47, train_loss =  1.25, learning_rate = 0.31250000


[batch = 5500] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.31250000


[batch = 5600] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 0.31250000


[batch = 5700] train_perplexity =     3.35, train_loss =  1.21, learning_rate = 0.31250000


[batch = 5800] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.31250000


[batch = 5900] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.31250000


[batch = 6000] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.31250000


[batch = 6100] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.31250000


[batch = 6200] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.31250000


[batch = 6300] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.31250000


[batch = 6400] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.31250000


[batch = 6500] train_perplexity =     3.37, train_loss =  1.22, learning_rate = 0.31250000


[batch = 6600] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.31250000


[batch = 6700] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.31250000


[batch = 6800] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.31250000


[batch = 6900] train_perplexity =     3.56, train_loss =  1.27, learning_rate = 0.31250000


[epoch =  28] validation_perplexity =     3.60, validation_loss =  1.28

Saved model to 'e200/ptb_char_lstm.pt'...
[epoch =  28] min_validation_perplexity =     3.60, min_validation_loss =  1.28


[batch =  100] train_perplexity =     3.75, train_loss =  1.32, learning_rate = 0.31250000


[batch =  200] train_perplexity =     3.56, train_loss =  1.27, learning_rate = 0.31250000


[batch =  300] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.31250000


[batch =  400] train_perplexity =     3.38, train_loss =  1.22, learning_rate = 0.31250000


[batch =  500] train_perplexity =     3.37, train_loss =  1.21, learning_rate = 0.31250000


[batch =  600] train_perplexity =     3.34, train_loss =  1.21, learning_rate = 0.31250000


[batch =  700] train_perplexity =     3.37, train_loss =  1.21, learning_rate = 0.31250000


[batch =  800] train_perplexity =     3.51, train_loss =  1.25, learning_rate = 0.31250000


[batch =  900] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.31250000


[batch = 1000] train_perplexity =     3.56, train_loss =  1.27, learning_rate = 0.31250000


[batch = 1100] train_perplexity =     3.62, train_loss =  1.29, learning_rate = 0.31250000


[batch = 1200] train_perplexity =     3.51, train_loss =  1.25, learning_rate = 0.31250000


[batch = 1300] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 0.31250000


[batch = 1400] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.31250000


[batch = 1500] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.31250000


[batch = 1600] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.31250000


[batch = 1700] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.31250000


[batch = 1800] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.31250000


[batch = 1900] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.31250000


[batch = 2000] train_perplexity =     3.59, train_loss =  1.28, learning_rate = 0.31250000


[batch = 2100] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.31250000


[batch = 2200] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.31250000


[batch = 2300] train_perplexity =     3.59, train_loss =  1.28, learning_rate = 0.31250000


[batch = 2400] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.31250000


[batch = 2500] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.31250000


[batch = 2600] train_perplexity =     3.58, train_loss =  1.27, learning_rate = 0.31250000


[batch = 2700] train_perplexity =     3.59, train_loss =  1.28, learning_rate = 0.31250000


[batch = 2800] train_perplexity =     3.51, train_loss =  1.25, learning_rate = 0.31250000


[batch = 2900] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.31250000


[batch = 3000] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.31250000


[batch = 3100] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.31250000


[batch = 3200] train_perplexity =     3.64, train_loss =  1.29, learning_rate = 0.31250000


[batch = 3300] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.31250000


[batch = 3400] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.31250000


[batch = 3500] train_perplexity =     3.54, train_loss =  1.27, learning_rate = 0.31250000


[batch = 3600] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 0.31250000


[batch = 3700] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.31250000


[batch = 3800] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.31250000


[batch = 3900] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.31250000


[batch = 4000] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.31250000


[batch = 4100] train_perplexity =     3.38, train_loss =  1.22, learning_rate = 0.31250000


[batch = 4200] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.31250000


[batch = 4300] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.31250000


[batch = 4400] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.31250000


[batch = 4500] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.31250000


[batch = 4600] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.31250000


[batch = 4700] train_perplexity =     3.51, train_loss =  1.25, learning_rate = 0.31250000


[batch = 4800] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.31250000


[batch = 4900] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 0.31250000


[batch = 5000] train_perplexity =     3.51, train_loss =  1.25, learning_rate = 0.31250000


[batch = 5100] train_perplexity =     3.56, train_loss =  1.27, learning_rate = 0.31250000


[batch = 5200] train_perplexity =     3.56, train_loss =  1.27, learning_rate = 0.31250000


[batch = 5300] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.31250000


[batch = 5400] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.31250000


[batch = 5500] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.31250000


[batch = 5600] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 0.31250000


[batch = 5700] train_perplexity =     3.35, train_loss =  1.21, learning_rate = 0.31250000


[batch = 5800] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.31250000


[batch = 5900] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.31250000


[batch = 6000] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.31250000


[batch = 6100] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.31250000


[batch = 6200] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.31250000


[batch = 6300] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.31250000


[batch = 6400] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.31250000


[batch = 6500] train_perplexity =     3.37, train_loss =  1.21, learning_rate = 0.31250000


[batch = 6600] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.31250000


[batch = 6700] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.31250000


[batch = 6800] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.31250000


[batch = 6900] train_perplexity =     3.56, train_loss =  1.27, learning_rate = 0.31250000


[epoch =  29] validation_perplexity =     3.60, validation_loss =  1.28

Saved model to 'e200/ptb_char_lstm.pt'...
[epoch =  29] min_validation_perplexity =     3.60, min_validation_loss =  1.28


[batch =  100] train_perplexity =     3.74, train_loss =  1.32, learning_rate = 0.31250000


[batch =  200] train_perplexity =     3.56, train_loss =  1.27, learning_rate = 0.31250000


[batch =  300] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.31250000


[batch =  400] train_perplexity =     3.38, train_loss =  1.22, learning_rate = 0.31250000


[batch =  500] train_perplexity =     3.37, train_loss =  1.21, learning_rate = 0.31250000


[batch =  600] train_perplexity =     3.34, train_loss =  1.20, learning_rate = 0.31250000


[batch =  700] train_perplexity =     3.37, train_loss =  1.21, learning_rate = 0.31250000


[batch =  800] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.31250000


[batch =  900] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.31250000


[batch = 1000] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 0.31250000


[batch = 1100] train_perplexity =     3.62, train_loss =  1.29, learning_rate = 0.31250000


[batch = 1200] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.31250000


[batch = 1300] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 0.31250000


[batch = 1400] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.31250000


[batch = 1500] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.31250000


[batch = 1600] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.31250000


[batch = 1700] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.31250000


[batch = 1800] train_perplexity =     3.47, train_loss =  1.25, learning_rate = 0.31250000


[batch = 1900] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.31250000


[batch = 2000] train_perplexity =     3.59, train_loss =  1.28, learning_rate = 0.31250000


[batch = 2100] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.31250000


[batch = 2200] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.31250000


[batch = 2300] train_perplexity =     3.59, train_loss =  1.28, learning_rate = 0.31250000


[batch = 2400] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.31250000


[batch = 2500] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.31250000


[batch = 2600] train_perplexity =     3.58, train_loss =  1.27, learning_rate = 0.31250000


[batch = 2700] train_perplexity =     3.59, train_loss =  1.28, learning_rate = 0.31250000


[batch = 2800] train_perplexity =     3.51, train_loss =  1.25, learning_rate = 0.31250000


[batch = 2900] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.31250000


[batch = 3000] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.31250000


[batch = 3100] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.31250000


[batch = 3200] train_perplexity =     3.63, train_loss =  1.29, learning_rate = 0.31250000


[batch = 3300] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.31250000


[batch = 3400] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.31250000


[batch = 3500] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.31250000


[batch = 3600] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 0.31250000


[batch = 3700] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.31250000


[batch = 3800] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.31250000


[batch = 3900] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.31250000


[batch = 4000] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.31250000


[batch = 4100] train_perplexity =     3.38, train_loss =  1.22, learning_rate = 0.31250000


[batch = 4200] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.31250000


[batch = 4300] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.31250000


[batch = 4400] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.31250000


[batch = 4500] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.31250000


[batch = 4600] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.31250000


[batch = 4700] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.31250000


[batch = 4800] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.31250000


[batch = 4900] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 0.31250000


[batch = 5000] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.31250000


[batch = 5100] train_perplexity =     3.56, train_loss =  1.27, learning_rate = 0.31250000


[batch = 5200] train_perplexity =     3.56, train_loss =  1.27, learning_rate = 0.31250000


[batch = 5300] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.31250000


[batch = 5400] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.31250000


[batch = 5500] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.31250000


[batch = 5600] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 0.31250000


[batch = 5700] train_perplexity =     3.35, train_loss =  1.21, learning_rate = 0.31250000


[batch = 5800] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.31250000


[batch = 5900] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.31250000


[batch = 6000] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.31250000


[batch = 6100] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.31250000


[batch = 6200] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.31250000


[batch = 6300] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.31250000


[batch = 6400] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.31250000


[batch = 6500] train_perplexity =     3.37, train_loss =  1.21, learning_rate = 0.31250000


[batch = 6600] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.31250000


[batch = 6700] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.31250000


[batch = 6800] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.31250000


[batch = 6900] train_perplexity =     3.56, train_loss =  1.27, learning_rate = 0.31250000


[epoch =  30] validation_perplexity =     3.60, validation_loss =  1.28

Saved model to 'e200/ptb_char_lstm.pt'...
[epoch =  30] min_validation_perplexity =     3.60, min_validation_loss =  1.28


[batch =  100] train_perplexity =     3.74, train_loss =  1.32, learning_rate = 0.31250000


[batch =  200] train_perplexity =     3.56, train_loss =  1.27, learning_rate = 0.31250000


[batch =  300] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.31250000


[batch =  400] train_perplexity =     3.37, train_loss =  1.22, learning_rate = 0.31250000


[batch =  500] train_perplexity =     3.37, train_loss =  1.21, learning_rate = 0.31250000


[batch =  600] train_perplexity =     3.34, train_loss =  1.20, learning_rate = 0.31250000


[batch =  700] train_perplexity =     3.36, train_loss =  1.21, learning_rate = 0.31250000


[batch =  800] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.31250000


[batch =  900] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.31250000


[batch = 1000] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 0.31250000


[batch = 1100] train_perplexity =     3.62, train_loss =  1.29, learning_rate = 0.31250000


[batch = 1200] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.31250000


[batch = 1300] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 0.31250000


[batch = 1400] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.31250000


[batch = 1500] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.31250000


[batch = 1600] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.31250000


[batch = 1700] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.31250000


[batch = 1800] train_perplexity =     3.47, train_loss =  1.25, learning_rate = 0.31250000


[batch = 1900] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.31250000


[batch = 2000] train_perplexity =     3.59, train_loss =  1.28, learning_rate = 0.31250000


[batch = 2100] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.31250000


[batch = 2200] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.31250000


[batch = 2300] train_perplexity =     3.59, train_loss =  1.28, learning_rate = 0.31250000


[batch = 2400] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.31250000


[batch = 2500] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.31250000


[batch = 2600] train_perplexity =     3.57, train_loss =  1.27, learning_rate = 0.31250000


[batch = 2700] train_perplexity =     3.59, train_loss =  1.28, learning_rate = 0.31250000


[batch = 2800] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.31250000


[batch = 2900] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.31250000


[batch = 3000] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.31250000


[batch = 3100] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.31250000


[batch = 3200] train_perplexity =     3.63, train_loss =  1.29, learning_rate = 0.31250000


[batch = 3300] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.31250000


[batch = 3400] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.31250000


[batch = 3500] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.31250000


[batch = 3600] train_perplexity =     3.54, train_loss =  1.27, learning_rate = 0.31250000


[batch = 3700] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.31250000


[batch = 3800] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.31250000


[batch = 3900] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.31250000


[batch = 4000] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.31250000


[batch = 4100] train_perplexity =     3.38, train_loss =  1.22, learning_rate = 0.31250000


[batch = 4200] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.31250000


[batch = 4300] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.31250000


[batch = 4400] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.31250000


[batch = 4500] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.31250000


[batch = 4600] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.31250000


[batch = 4700] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.31250000


[batch = 4800] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.31250000


[batch = 4900] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 0.31250000


[batch = 5000] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.31250000


[batch = 5100] train_perplexity =     3.56, train_loss =  1.27, learning_rate = 0.31250000


[batch = 5200] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 0.31250000


[batch = 5300] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.31250000


[batch = 5400] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.31250000


[batch = 5500] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.31250000


[batch = 5600] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 0.31250000


[batch = 5700] train_perplexity =     3.34, train_loss =  1.21, learning_rate = 0.31250000


[batch = 5800] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.31250000


[batch = 5900] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.31250000


[batch = 6000] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.31250000


[batch = 6100] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.31250000


[batch = 6200] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.31250000


[batch = 6300] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.31250000


[batch = 6400] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.31250000


[batch = 6500] train_perplexity =     3.37, train_loss =  1.21, learning_rate = 0.31250000


[batch = 6600] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.31250000


[batch = 6700] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.31250000


[batch = 6800] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.31250000


[batch = 6900] train_perplexity =     3.56, train_loss =  1.27, learning_rate = 0.31250000


[epoch =  31] validation_perplexity =     3.60, validation_loss =  1.28

Saved model to 'e200/ptb_char_lstm.pt'...
[epoch =  31] min_validation_perplexity =     3.60, min_validation_loss =  1.28


[batch =  100] train_perplexity =     3.74, train_loss =  1.32, learning_rate = 0.31250000


[batch =  200] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 0.31250000


[batch =  300] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.31250000


[batch =  400] train_perplexity =     3.37, train_loss =  1.22, learning_rate = 0.31250000


[batch =  500] train_perplexity =     3.37, train_loss =  1.21, learning_rate = 0.31250000


[batch =  600] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.31250000


[batch =  700] train_perplexity =     3.36, train_loss =  1.21, learning_rate = 0.31250000


[batch =  800] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.31250000


[batch =  900] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.31250000


[batch = 1000] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 0.31250000


[batch = 1100] train_perplexity =     3.62, train_loss =  1.29, learning_rate = 0.31250000


[batch = 1200] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.31250000


[batch = 1300] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 0.31250000


[batch = 1400] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.31250000


[batch = 1500] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.31250000


[batch = 1600] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.31250000


[batch = 1700] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.31250000


[batch = 1800] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.31250000


[batch = 1900] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.31250000


[batch = 2000] train_perplexity =     3.59, train_loss =  1.28, learning_rate = 0.31250000


[batch = 2100] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.31250000


[batch = 2200] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.31250000


[batch = 2300] train_perplexity =     3.59, train_loss =  1.28, learning_rate = 0.31250000


[batch = 2400] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.31250000


[batch = 2500] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.31250000


[batch = 2600] train_perplexity =     3.57, train_loss =  1.27, learning_rate = 0.31250000


[batch = 2700] train_perplexity =     3.59, train_loss =  1.28, learning_rate = 0.31250000


[batch = 2800] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.31250000


[batch = 2900] train_perplexity =     3.47, train_loss =  1.25, learning_rate = 0.31250000


[batch = 3000] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.31250000


[batch = 3100] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.31250000


[batch = 3200] train_perplexity =     3.63, train_loss =  1.29, learning_rate = 0.31250000


[batch = 3300] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.31250000


[batch = 3400] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.31250000


[batch = 3500] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.31250000


[batch = 3600] train_perplexity =     3.54, train_loss =  1.27, learning_rate = 0.31250000


[batch = 3700] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.31250000


[batch = 3800] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.31250000


[batch = 3900] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.31250000


[batch = 4000] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.31250000


[batch = 4100] train_perplexity =     3.38, train_loss =  1.22, learning_rate = 0.31250000


[batch = 4200] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.31250000


[batch = 4300] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.31250000


[batch = 4400] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.31250000


[batch = 4500] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.31250000


[batch = 4600] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.31250000


[batch = 4700] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.31250000


[batch = 4800] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.31250000


[batch = 4900] train_perplexity =     3.54, train_loss =  1.27, learning_rate = 0.31250000


[batch = 5000] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.31250000


[batch = 5100] train_perplexity =     3.56, train_loss =  1.27, learning_rate = 0.31250000


[batch = 5200] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 0.31250000


[batch = 5300] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.31250000


[batch = 5400] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.31250000


[batch = 5500] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.31250000


[batch = 5600] train_perplexity =     3.54, train_loss =  1.27, learning_rate = 0.31250000


[batch = 5700] train_perplexity =     3.34, train_loss =  1.21, learning_rate = 0.31250000


[batch = 5800] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.31250000


[batch = 5900] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.31250000


[batch = 6000] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.31250000


[batch = 6100] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.31250000


[batch = 6200] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.31250000


[batch = 6300] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.31250000


[batch = 6400] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.31250000


[batch = 6500] train_perplexity =     3.37, train_loss =  1.21, learning_rate = 0.31250000


[batch = 6600] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.31250000


[batch = 6700] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.31250000


[batch = 6800] train_perplexity =     3.47, train_loss =  1.25, learning_rate = 0.31250000


[batch = 6900] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 0.31250000


[epoch =  32] validation_perplexity =     3.60, validation_loss =  1.28

Saved model to 'e200/ptb_char_lstm.pt'...
[epoch =  32] min_validation_perplexity =     3.60, min_validation_loss =  1.28


[batch =  100] train_perplexity =     3.74, train_loss =  1.32, learning_rate = 0.31250000


[batch =  200] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 0.31250000


[batch =  300] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.31250000


[batch =  400] train_perplexity =     3.37, train_loss =  1.22, learning_rate = 0.31250000


[batch =  500] train_perplexity =     3.37, train_loss =  1.21, learning_rate = 0.31250000


[batch =  600] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.31250000


[batch =  700] train_perplexity =     3.36, train_loss =  1.21, learning_rate = 0.31250000


[batch =  800] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.31250000


[batch =  900] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.31250000


[batch = 1000] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 0.31250000


[batch = 1100] train_perplexity =     3.61, train_loss =  1.28, learning_rate = 0.31250000


[batch = 1200] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.31250000


[batch = 1300] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 0.31250000


[batch = 1400] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.31250000


[batch = 1500] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.31250000


[batch = 1600] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.31250000


[batch = 1700] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.31250000


[batch = 1800] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.31250000


[batch = 1900] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.31250000


[batch = 2000] train_perplexity =     3.59, train_loss =  1.28, learning_rate = 0.31250000


[batch = 2100] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.31250000


[batch = 2200] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.31250000


[batch = 2300] train_perplexity =     3.59, train_loss =  1.28, learning_rate = 0.31250000


[batch = 2400] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.31250000


[batch = 2500] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.31250000


[batch = 2600] train_perplexity =     3.57, train_loss =  1.27, learning_rate = 0.31250000


[batch = 2700] train_perplexity =     3.58, train_loss =  1.28, learning_rate = 0.31250000


[batch = 2800] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.31250000


[batch = 2900] train_perplexity =     3.47, train_loss =  1.25, learning_rate = 0.31250000


[batch = 3000] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.31250000


[batch = 3100] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.31250000


[batch = 3200] train_perplexity =     3.63, train_loss =  1.29, learning_rate = 0.31250000


[batch = 3300] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.31250000


[batch = 3400] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.31250000


[batch = 3500] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.31250000


[batch = 3600] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.31250000


[batch = 3700] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.31250000


[batch = 3800] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.31250000


[batch = 3900] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.31250000


[batch = 4000] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.31250000


[batch = 4100] train_perplexity =     3.38, train_loss =  1.22, learning_rate = 0.31250000


[batch = 4200] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.31250000


[batch = 4300] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.31250000


[batch = 4400] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.31250000


[batch = 4500] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.31250000


[batch = 4600] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.31250000


[batch = 4700] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.31250000


[batch = 4800] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.31250000


[batch = 4900] train_perplexity =     3.54, train_loss =  1.27, learning_rate = 0.31250000


[batch = 5000] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.31250000


[batch = 5100] train_perplexity =     3.56, train_loss =  1.27, learning_rate = 0.31250000


[batch = 5200] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 0.31250000


[batch = 5300] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.31250000


[batch = 5400] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.31250000


[batch = 5500] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.31250000


[batch = 5600] train_perplexity =     3.54, train_loss =  1.27, learning_rate = 0.31250000


[batch = 5700] train_perplexity =     3.34, train_loss =  1.21, learning_rate = 0.31250000


[batch = 5800] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.31250000


[batch = 5900] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.31250000


[batch = 6000] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.31250000


[batch = 6100] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.31250000


[batch = 6200] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.31250000


[batch = 6300] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.31250000


[batch = 6400] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.31250000


[batch = 6500] train_perplexity =     3.37, train_loss =  1.21, learning_rate = 0.31250000


[batch = 6600] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.31250000


[batch = 6700] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.31250000


[batch = 6800] train_perplexity =     3.47, train_loss =  1.25, learning_rate = 0.31250000


[batch = 6900] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 0.31250000


[epoch =  33] validation_perplexity =     3.60, validation_loss =  1.28

Saved model to 'e200/ptb_char_lstm.pt'...
[epoch =  33] min_validation_perplexity =     3.60, min_validation_loss =  1.28


[batch =  100] train_perplexity =     3.74, train_loss =  1.32, learning_rate = 0.31250000


[batch =  200] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 0.31250000


[batch =  300] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.31250000


[batch =  400] train_perplexity =     3.37, train_loss =  1.22, learning_rate = 0.31250000


[batch =  500] train_perplexity =     3.36, train_loss =  1.21, learning_rate = 0.31250000


[batch =  600] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.31250000


[batch =  700] train_perplexity =     3.36, train_loss =  1.21, learning_rate = 0.31250000


[batch =  800] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.31250000


[batch =  900] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.31250000


[batch = 1000] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 0.31250000


[batch = 1100] train_perplexity =     3.61, train_loss =  1.28, learning_rate = 0.31250000


[batch = 1200] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.31250000


[batch = 1300] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 0.31250000


[batch = 1400] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.31250000


[batch = 1500] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.31250000


[batch = 1600] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.31250000


[batch = 1700] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.31250000


[batch = 1800] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.31250000


[batch = 1900] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.31250000


[batch = 2000] train_perplexity =     3.58, train_loss =  1.28, learning_rate = 0.31250000


[batch = 2100] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.31250000


[batch = 2200] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.31250000


[batch = 2300] train_perplexity =     3.58, train_loss =  1.28, learning_rate = 0.31250000


[batch = 2400] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.31250000


[batch = 2500] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.31250000


[batch = 2600] train_perplexity =     3.57, train_loss =  1.27, learning_rate = 0.31250000


[batch = 2700] train_perplexity =     3.58, train_loss =  1.28, learning_rate = 0.31250000


[batch = 2800] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.31250000


[batch = 2900] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.31250000


[batch = 3000] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.31250000


[batch = 3100] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.31250000


[batch = 3200] train_perplexity =     3.63, train_loss =  1.29, learning_rate = 0.31250000


[batch = 3300] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.31250000


[batch = 3400] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.31250000


[batch = 3500] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.31250000


[batch = 3600] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.31250000


[batch = 3700] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.31250000


[batch = 3800] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.31250000


[batch = 3900] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.31250000


[batch = 4000] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.31250000


[batch = 4100] train_perplexity =     3.38, train_loss =  1.22, learning_rate = 0.31250000


[batch = 4200] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.31250000


[batch = 4300] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.31250000


[batch = 4400] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.31250000


[batch = 4500] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.31250000


[batch = 4600] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.31250000


[batch = 4700] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.31250000


[batch = 4800] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.31250000


[batch = 4900] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.31250000


[batch = 5000] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.31250000


[batch = 5100] train_perplexity =     3.56, train_loss =  1.27, learning_rate = 0.31250000


[batch = 5200] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 0.31250000


[batch = 5300] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.31250000


[batch = 5400] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.31250000


[batch = 5500] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.31250000


[batch = 5600] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.31250000


[batch = 5700] train_perplexity =     3.34, train_loss =  1.21, learning_rate = 0.31250000


[batch = 5800] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.31250000


[batch = 5900] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.31250000


[batch = 6000] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.31250000


[batch = 6100] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.31250000


[batch = 6200] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.31250000


[batch = 6300] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.31250000


[batch = 6400] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.31250000


[batch = 6500] train_perplexity =     3.36, train_loss =  1.21, learning_rate = 0.31250000


[batch = 6600] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.31250000


[batch = 6700] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.31250000


[batch = 6800] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.31250000


[batch = 6900] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 0.31250000


[epoch =  34] validation_perplexity =     3.60, validation_loss =  1.28

Saved model to 'e200/ptb_char_lstm.pt'...
[epoch =  34] min_validation_perplexity =     3.60, min_validation_loss =  1.28


[batch =  100] train_perplexity =     3.74, train_loss =  1.32, learning_rate = 0.31250000


[batch =  200] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 0.31250000


[batch =  300] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.31250000


[batch =  400] train_perplexity =     3.37, train_loss =  1.21, learning_rate = 0.31250000


[batch =  500] train_perplexity =     3.36, train_loss =  1.21, learning_rate = 0.31250000


[batch =  600] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.31250000


[batch =  700] train_perplexity =     3.36, train_loss =  1.21, learning_rate = 0.31250000


[batch =  800] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.31250000


[batch =  900] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.31250000


[batch = 1000] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 0.31250000


[batch = 1100] train_perplexity =     3.61, train_loss =  1.28, learning_rate = 0.31250000


[batch = 1200] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.31250000


[batch = 1300] train_perplexity =     3.54, train_loss =  1.27, learning_rate = 0.31250000


[batch = 1400] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.31250000


[batch = 1500] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.31250000


[batch = 1600] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.31250000


[batch = 1700] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.31250000


[batch = 1800] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.31250000


[batch = 1900] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.31250000


[batch = 2000] train_perplexity =     3.58, train_loss =  1.28, learning_rate = 0.31250000


[batch = 2100] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.31250000


[batch = 2200] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.31250000


[batch = 2300] train_perplexity =     3.58, train_loss =  1.28, learning_rate = 0.31250000


[batch = 2400] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.31250000


[batch = 2500] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.31250000


[batch = 2600] train_perplexity =     3.57, train_loss =  1.27, learning_rate = 0.31250000


[batch = 2700] train_perplexity =     3.58, train_loss =  1.28, learning_rate = 0.31250000


[batch = 2800] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.31250000


[batch = 2900] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.31250000


[batch = 3000] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.31250000


[batch = 3100] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.31250000


[batch = 3200] train_perplexity =     3.63, train_loss =  1.29, learning_rate = 0.31250000


[batch = 3300] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.31250000


[batch = 3400] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.31250000


[batch = 3500] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.31250000


[batch = 3600] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.31250000


[batch = 3700] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.31250000


[batch = 3800] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.31250000


[batch = 3900] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.31250000


[batch = 4000] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.31250000


[batch = 4100] train_perplexity =     3.38, train_loss =  1.22, learning_rate = 0.31250000


[batch = 4200] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.31250000


[batch = 4300] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.31250000


[batch = 4400] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.31250000


[batch = 4500] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.31250000


[batch = 4600] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.31250000


[batch = 4700] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.31250000


[batch = 4800] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.31250000


[batch = 4900] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.31250000


[batch = 5000] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.31250000


[batch = 5100] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 0.31250000


[batch = 5200] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 0.31250000


[batch = 5300] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.31250000


[batch = 5400] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.31250000


[batch = 5500] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.31250000


[batch = 5600] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.31250000


[batch = 5700] train_perplexity =     3.34, train_loss =  1.21, learning_rate = 0.31250000


[batch = 5800] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.31250000


[batch = 5900] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.31250000


[batch = 6000] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.31250000


[batch = 6100] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.31250000


[batch = 6200] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.31250000


[batch = 6300] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.31250000


[batch = 6400] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.31250000


[batch = 6500] train_perplexity =     3.36, train_loss =  1.21, learning_rate = 0.31250000


[batch = 6600] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.31250000


[batch = 6700] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.31250000


[batch = 6800] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.31250000


[batch = 6900] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 0.31250000


[epoch =  35] validation_perplexity =     3.60, validation_loss =  1.28

Saved model to 'e200/ptb_char_lstm.pt'...
[epoch =  35] min_validation_perplexity =     3.60, min_validation_loss =  1.28


[batch =  100] train_perplexity =     3.74, train_loss =  1.32, learning_rate = 0.31250000


[batch =  200] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 0.31250000


[batch =  300] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.31250000


[batch =  400] train_perplexity =     3.37, train_loss =  1.21, learning_rate = 0.31250000


[batch =  500] train_perplexity =     3.36, train_loss =  1.21, learning_rate = 0.31250000


[batch =  600] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.31250000


[batch =  700] train_perplexity =     3.36, train_loss =  1.21, learning_rate = 0.31250000


[batch =  800] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.31250000


[batch =  900] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.31250000


[batch = 1000] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 0.31250000


[batch = 1100] train_perplexity =     3.61, train_loss =  1.28, learning_rate = 0.31250000


[batch = 1200] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.31250000


[batch = 1300] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.31250000


[batch = 1400] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.31250000


[batch = 1500] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.31250000


[batch = 1600] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.31250000


[batch = 1700] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.31250000


[batch = 1800] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.31250000


[batch = 1900] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.31250000


[batch = 2000] train_perplexity =     3.58, train_loss =  1.28, learning_rate = 0.31250000


[batch = 2100] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.31250000


[batch = 2200] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.31250000


[batch = 2300] train_perplexity =     3.58, train_loss =  1.28, learning_rate = 0.31250000


[batch = 2400] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.31250000


[batch = 2500] train_perplexity =     3.51, train_loss =  1.25, learning_rate = 0.31250000


[batch = 2600] train_perplexity =     3.57, train_loss =  1.27, learning_rate = 0.31250000


[batch = 2700] train_perplexity =     3.58, train_loss =  1.28, learning_rate = 0.31250000


[batch = 2800] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.31250000


[batch = 2900] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.31250000


[batch = 3000] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.31250000


[batch = 3100] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.31250000


[batch = 3200] train_perplexity =     3.63, train_loss =  1.29, learning_rate = 0.31250000


[batch = 3300] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.31250000


[batch = 3400] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.31250000


[batch = 3500] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.31250000


[batch = 3600] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.31250000


[batch = 3700] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.31250000


[batch = 3800] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.31250000


[batch = 3900] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.31250000


[batch = 4000] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.31250000


[batch = 4100] train_perplexity =     3.37, train_loss =  1.22, learning_rate = 0.31250000


[batch = 4200] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.31250000


[batch = 4300] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.31250000


[batch = 4400] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.31250000


[batch = 4500] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.31250000


[batch = 4600] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.31250000


[batch = 4700] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.31250000


[batch = 4800] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.31250000


[batch = 4900] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.31250000


[batch = 5000] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.31250000


[batch = 5100] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 0.31250000


[batch = 5200] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 0.31250000


[batch = 5300] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.31250000


[batch = 5400] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.31250000


[batch = 5500] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.31250000


[batch = 5600] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.31250000


[batch = 5700] train_perplexity =     3.34, train_loss =  1.21, learning_rate = 0.31250000


[batch = 5800] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.31250000


[batch = 5900] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.31250000


[batch = 6000] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.31250000


[batch = 6100] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.31250000


[batch = 6200] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.31250000


[batch = 6300] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.31250000


[batch = 6400] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.31250000


[batch = 6500] train_perplexity =     3.36, train_loss =  1.21, learning_rate = 0.31250000


[batch = 6600] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.31250000


[batch = 6700] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.31250000


[batch = 6800] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.31250000


[batch = 6900] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 0.31250000


[epoch =  36] validation_perplexity =     3.60, validation_loss =  1.28

Saved model to 'e200/ptb_char_lstm.pt'...
[epoch =  36] min_validation_perplexity =     3.60, min_validation_loss =  1.28


[batch =  100] train_perplexity =     3.74, train_loss =  1.32, learning_rate = 0.31250000


[batch =  200] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 0.31250000


[batch =  300] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.31250000


[batch =  400] train_perplexity =     3.37, train_loss =  1.21, learning_rate = 0.31250000


[batch =  500] train_perplexity =     3.36, train_loss =  1.21, learning_rate = 0.31250000


[batch =  600] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.31250000


[batch =  700] train_perplexity =     3.36, train_loss =  1.21, learning_rate = 0.31250000


[batch =  800] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.31250000


[batch =  900] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.31250000


[batch = 1000] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 0.31250000


[batch = 1100] train_perplexity =     3.61, train_loss =  1.28, learning_rate = 0.31250000


[batch = 1200] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.31250000


[batch = 1300] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.31250000


[batch = 1400] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.31250000


[batch = 1500] train_perplexity =     3.47, train_loss =  1.25, learning_rate = 0.31250000


[batch = 1600] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.31250000


[batch = 1700] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.31250000


[batch = 1800] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.31250000


[batch = 1900] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.31250000


[batch = 2000] train_perplexity =     3.58, train_loss =  1.28, learning_rate = 0.31250000


[batch = 2100] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.31250000


[batch = 2200] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.31250000


[batch = 2300] train_perplexity =     3.58, train_loss =  1.28, learning_rate = 0.31250000


[batch = 2400] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.31250000


[batch = 2500] train_perplexity =     3.51, train_loss =  1.25, learning_rate = 0.31250000


[batch = 2600] train_perplexity =     3.57, train_loss =  1.27, learning_rate = 0.31250000


[batch = 2700] train_perplexity =     3.58, train_loss =  1.28, learning_rate = 0.31250000


[batch = 2800] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.31250000


[batch = 2900] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.31250000


[batch = 3000] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.31250000


[batch = 3100] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.31250000


[batch = 3200] train_perplexity =     3.63, train_loss =  1.29, learning_rate = 0.31250000


[batch = 3300] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.31250000


[batch = 3400] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.31250000


[batch = 3500] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.31250000


[batch = 3600] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.31250000


[batch = 3700] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.31250000


[batch = 3800] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.31250000


[batch = 3900] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.31250000


[batch = 4000] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.31250000


[batch = 4100] train_perplexity =     3.37, train_loss =  1.22, learning_rate = 0.31250000


[batch = 4200] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.31250000


[batch = 4300] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.31250000


[batch = 4400] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.31250000


[batch = 4500] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.31250000


[batch = 4600] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.31250000


[batch = 4700] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.31250000


[batch = 4800] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.31250000


[batch = 4900] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.31250000


[batch = 5000] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.31250000


[batch = 5100] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 0.31250000


[batch = 5200] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 0.31250000


[batch = 5300] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.31250000


[batch = 5400] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.31250000


[batch = 5500] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.31250000


[batch = 5600] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.31250000


[batch = 5700] train_perplexity =     3.34, train_loss =  1.21, learning_rate = 0.31250000


[batch = 5800] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.31250000


[batch = 5900] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.31250000


[batch = 6000] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.31250000


[batch = 6100] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.31250000


[batch = 6200] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.31250000


[batch = 6300] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.31250000


[batch = 6400] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.31250000


[batch = 6500] train_perplexity =     3.36, train_loss =  1.21, learning_rate = 0.31250000


[batch = 6600] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.31250000


[batch = 6700] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.31250000


[batch = 6800] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.31250000


[batch = 6900] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 0.31250000


[epoch =  37] validation_perplexity =     3.59, validation_loss =  1.28

Saved model to 'e200/ptb_char_lstm.pt'...
[epoch =  37] min_validation_perplexity =     3.59, min_validation_loss =  1.28


[batch =  100] train_perplexity =     3.73, train_loss =  1.32, learning_rate = 0.31250000


[batch =  200] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 0.31250000


[batch =  300] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.31250000


[batch =  400] train_perplexity =     3.37, train_loss =  1.21, learning_rate = 0.31250000


[batch =  500] train_perplexity =     3.36, train_loss =  1.21, learning_rate = 0.31250000


[batch =  600] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.31250000


[batch =  700] train_perplexity =     3.36, train_loss =  1.21, learning_rate = 0.31250000


[batch =  800] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.31250000


[batch =  900] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.31250000


[batch = 1000] train_perplexity =     3.54, train_loss =  1.27, learning_rate = 0.31250000


[batch = 1100] train_perplexity =     3.61, train_loss =  1.28, learning_rate = 0.31250000


[batch = 1200] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.31250000


[batch = 1300] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.31250000


[batch = 1400] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.31250000


[batch = 1500] train_perplexity =     3.47, train_loss =  1.25, learning_rate = 0.31250000


[batch = 1600] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.31250000


[batch = 1700] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.31250000


[batch = 1800] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.31250000


[batch = 1900] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.31250000


[batch = 2000] train_perplexity =     3.58, train_loss =  1.28, learning_rate = 0.31250000


[batch = 2100] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.31250000


[batch = 2200] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.31250000


[batch = 2300] train_perplexity =     3.58, train_loss =  1.28, learning_rate = 0.31250000


[batch = 2400] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.31250000


[batch = 2500] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.31250000


[batch = 2600] train_perplexity =     3.57, train_loss =  1.27, learning_rate = 0.31250000


[batch = 2700] train_perplexity =     3.58, train_loss =  1.27, learning_rate = 0.31250000


[batch = 2800] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.31250000


[batch = 2900] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.31250000


[batch = 3000] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.31250000


[batch = 3100] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.31250000


[batch = 3200] train_perplexity =     3.63, train_loss =  1.29, learning_rate = 0.31250000


[batch = 3300] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.31250000


[batch = 3400] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.31250000


[batch = 3500] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.31250000


[batch = 3600] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.31250000


[batch = 3700] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.31250000


[batch = 3800] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.31250000


[batch = 3900] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.31250000


[batch = 4000] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.31250000


[batch = 4100] train_perplexity =     3.37, train_loss =  1.22, learning_rate = 0.31250000


[batch = 4200] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.31250000


[batch = 4300] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.31250000


[batch = 4400] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.31250000


[batch = 4500] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.31250000


[batch = 4600] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.31250000


[batch = 4700] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.31250000


[batch = 4800] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.31250000


[batch = 4900] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.31250000


[batch = 5000] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.31250000


[batch = 5100] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 0.31250000


[batch = 5200] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 0.31250000


[batch = 5300] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.31250000


[batch = 5400] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.31250000


[batch = 5500] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.31250000


[batch = 5600] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.31250000


[batch = 5700] train_perplexity =     3.34, train_loss =  1.21, learning_rate = 0.31250000


[batch = 5800] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.31250000


[batch = 5900] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.31250000


[batch = 6000] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.31250000


[batch = 6100] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.31250000


[batch = 6200] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.31250000


[batch = 6300] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.31250000


[batch = 6400] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.31250000


[batch = 6500] train_perplexity =     3.36, train_loss =  1.21, learning_rate = 0.31250000


[batch = 6600] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.31250000


[batch = 6700] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.31250000


[batch = 6800] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.31250000


[batch = 6900] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 0.31250000


[epoch =  38] validation_perplexity =     3.59, validation_loss =  1.28

Saved model to 'e200/ptb_char_lstm.pt'...
[epoch =  38] min_validation_perplexity =     3.59, min_validation_loss =  1.28


[batch =  100] train_perplexity =     3.73, train_loss =  1.32, learning_rate = 0.31250000


[batch =  200] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 0.31250000


[batch =  300] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.31250000


[batch =  400] train_perplexity =     3.36, train_loss =  1.21, learning_rate = 0.31250000


[batch =  500] train_perplexity =     3.36, train_loss =  1.21, learning_rate = 0.31250000


[batch =  600] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.31250000


[batch =  700] train_perplexity =     3.36, train_loss =  1.21, learning_rate = 0.31250000


[batch =  800] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.31250000


[batch =  900] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.31250000


[batch = 1000] train_perplexity =     3.54, train_loss =  1.27, learning_rate = 0.31250000


[batch = 1100] train_perplexity =     3.61, train_loss =  1.28, learning_rate = 0.31250000


[batch = 1200] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.31250000


[batch = 1300] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.31250000


[batch = 1400] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.31250000


[batch = 1500] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.31250000


[batch = 1600] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.31250000


[batch = 1700] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.31250000


[batch = 1800] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.31250000


[batch = 1900] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.31250000


[batch = 2000] train_perplexity =     3.58, train_loss =  1.28, learning_rate = 0.31250000


[batch = 2100] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.31250000


[batch = 2200] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.31250000


[batch = 2300] train_perplexity =     3.58, train_loss =  1.27, learning_rate = 0.31250000


[batch = 2400] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.31250000


[batch = 2500] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.31250000


[batch = 2600] train_perplexity =     3.57, train_loss =  1.27, learning_rate = 0.31250000


[batch = 2700] train_perplexity =     3.58, train_loss =  1.27, learning_rate = 0.31250000


[batch = 2800] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.31250000


[batch = 2900] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.31250000


[batch = 3000] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.31250000


[batch = 3100] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.31250000


[batch = 3200] train_perplexity =     3.62, train_loss =  1.29, learning_rate = 0.31250000


[batch = 3300] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.31250000


[batch = 3400] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.31250000


[batch = 3500] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.31250000


[batch = 3600] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.31250000


[batch = 3700] train_perplexity =     3.51, train_loss =  1.25, learning_rate = 0.31250000


[batch = 3800] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.31250000


[batch = 3900] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.31250000


[batch = 4000] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.31250000


[batch = 4100] train_perplexity =     3.37, train_loss =  1.22, learning_rate = 0.31250000


[batch = 4200] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.31250000


[batch = 4300] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.31250000


[batch = 4400] train_perplexity =     3.47, train_loss =  1.25, learning_rate = 0.31250000


[batch = 4500] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.31250000


[batch = 4600] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.31250000


[batch = 4700] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.31250000


[batch = 4800] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.31250000


[batch = 4900] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.31250000


[batch = 5000] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.31250000


[batch = 5100] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 0.31250000


[batch = 5200] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 0.31250000


[batch = 5300] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.31250000


[batch = 5400] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.31250000


[batch = 5500] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.31250000


[batch = 5600] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.31250000


[batch = 5700] train_perplexity =     3.34, train_loss =  1.21, learning_rate = 0.31250000


[batch = 5800] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.31250000


[batch = 5900] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.31250000


[batch = 6000] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.31250000


[batch = 6100] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.31250000


[batch = 6200] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.31250000


[batch = 6300] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.31250000


[batch = 6400] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.31250000


[batch = 6500] train_perplexity =     3.36, train_loss =  1.21, learning_rate = 0.31250000


[batch = 6600] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.31250000


[batch = 6700] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.31250000


[batch = 6800] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.31250000


[batch = 6900] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 0.31250000


[epoch =  39] validation_perplexity =     3.59, validation_loss =  1.28

Saved model to 'e200/ptb_char_lstm.pt'...
[epoch =  39] min_validation_perplexity =     3.59, min_validation_loss =  1.28


[batch =  100] train_perplexity =     3.73, train_loss =  1.32, learning_rate = 0.31250000


[batch =  200] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 0.31250000


[batch =  300] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.31250000


[batch =  400] train_perplexity =     3.36, train_loss =  1.21, learning_rate = 0.31250000


[batch =  500] train_perplexity =     3.36, train_loss =  1.21, learning_rate = 0.31250000


[batch =  600] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.31250000


[batch =  700] train_perplexity =     3.35, train_loss =  1.21, learning_rate = 0.31250000


[batch =  800] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.31250000


[batch =  900] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.31250000


[batch = 1000] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.31250000


[batch = 1100] train_perplexity =     3.61, train_loss =  1.28, learning_rate = 0.31250000


[batch = 1200] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.31250000


[batch = 1300] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.31250000


[batch = 1400] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.31250000


[batch = 1500] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.31250000


[batch = 1600] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.31250000


[batch = 1700] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.31250000


[batch = 1800] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.31250000


[batch = 1900] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.31250000


[batch = 2000] train_perplexity =     3.58, train_loss =  1.27, learning_rate = 0.31250000


[batch = 2100] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.31250000


[batch = 2200] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.31250000


[batch = 2300] train_perplexity =     3.58, train_loss =  1.27, learning_rate = 0.31250000


[batch = 2400] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.31250000


[batch = 2500] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.31250000


[batch = 2600] train_perplexity =     3.56, train_loss =  1.27, learning_rate = 0.31250000


[batch = 2700] train_perplexity =     3.58, train_loss =  1.27, learning_rate = 0.31250000


[batch = 2800] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.31250000


[batch = 2900] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.31250000


[batch = 3000] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.31250000


[batch = 3100] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.31250000


[batch = 3200] train_perplexity =     3.62, train_loss =  1.29, learning_rate = 0.31250000


[batch = 3300] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.31250000


[batch = 3400] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.31250000


[batch = 3500] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.31250000


[batch = 3600] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.31250000


[batch = 3700] train_perplexity =     3.51, train_loss =  1.25, learning_rate = 0.31250000


[batch = 3800] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.31250000


[batch = 3900] train_perplexity =     3.47, train_loss =  1.25, learning_rate = 0.31250000


[batch = 4000] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.31250000


[batch = 4100] train_perplexity =     3.37, train_loss =  1.22, learning_rate = 0.31250000


[batch = 4200] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.31250000


[batch = 4300] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.31250000


[batch = 4400] train_perplexity =     3.47, train_loss =  1.25, learning_rate = 0.31250000


[batch = 4500] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.31250000


[batch = 4600] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.31250000


[batch = 4700] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.31250000


[batch = 4800] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.31250000


[batch = 4900] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.31250000


[batch = 5000] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.31250000


[batch = 5100] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 0.31250000


[batch = 5200] train_perplexity =     3.54, train_loss =  1.27, learning_rate = 0.31250000


[batch = 5300] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.31250000


[batch = 5400] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.31250000


[batch = 5500] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.31250000


[batch = 5600] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.31250000


[batch = 5700] train_perplexity =     3.34, train_loss =  1.21, learning_rate = 0.31250000


[batch = 5800] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.31250000


[batch = 5900] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.31250000


[batch = 6000] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.31250000


[batch = 6100] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.31250000


[batch = 6200] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.31250000


[batch = 6300] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.31250000


[batch = 6400] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.31250000


[batch = 6500] train_perplexity =     3.36, train_loss =  1.21, learning_rate = 0.31250000


[batch = 6600] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.31250000


[batch = 6700] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.31250000


[batch = 6800] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.31250000


[batch = 6900] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 0.31250000


[epoch =  40] validation_perplexity =     3.59, validation_loss =  1.28

Saved model to 'e200/ptb_char_lstm.pt'...
[epoch =  40] min_validation_perplexity =     3.59, min_validation_loss =  1.28


[batch =  100] train_perplexity =     3.73, train_loss =  1.32, learning_rate = 0.31250000


[batch =  200] train_perplexity =     3.54, train_loss =  1.27, learning_rate = 0.31250000


[batch =  300] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.31250000


[batch =  400] train_perplexity =     3.36, train_loss =  1.21, learning_rate = 0.31250000


[batch =  500] train_perplexity =     3.36, train_loss =  1.21, learning_rate = 0.31250000


[batch =  600] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.31250000


[batch =  700] train_perplexity =     3.35, train_loss =  1.21, learning_rate = 0.31250000


[batch =  800] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.31250000


[batch =  900] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.31250000


[batch = 1000] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.31250000


[batch = 1100] train_perplexity =     3.61, train_loss =  1.28, learning_rate = 0.31250000


[batch = 1200] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.31250000


[batch = 1300] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.31250000


[batch = 1400] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.31250000


[batch = 1500] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.31250000


[batch = 1600] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.31250000


[batch = 1700] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.31250000


[batch = 1800] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.31250000


[batch = 1900] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.31250000


[batch = 2000] train_perplexity =     3.58, train_loss =  1.27, learning_rate = 0.31250000


[batch = 2100] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.31250000


[batch = 2200] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.31250000


[batch = 2300] train_perplexity =     3.58, train_loss =  1.27, learning_rate = 0.31250000


[batch = 2400] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.31250000


[batch = 2500] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.31250000


[batch = 2600] train_perplexity =     3.56, train_loss =  1.27, learning_rate = 0.31250000


[batch = 2700] train_perplexity =     3.58, train_loss =  1.27, learning_rate = 0.31250000


[batch = 2800] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.31250000


[batch = 2900] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.31250000


[batch = 3000] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.31250000


[batch = 3100] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.31250000


[batch = 3200] train_perplexity =     3.62, train_loss =  1.29, learning_rate = 0.31250000


[batch = 3300] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.31250000


[batch = 3400] train_perplexity =     3.47, train_loss =  1.25, learning_rate = 0.31250000


[batch = 3500] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.31250000


[batch = 3600] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.31250000


[batch = 3700] train_perplexity =     3.51, train_loss =  1.25, learning_rate = 0.31250000


[batch = 3800] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.31250000


[batch = 3900] train_perplexity =     3.47, train_loss =  1.25, learning_rate = 0.31250000


[batch = 4000] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.31250000


[batch = 4100] train_perplexity =     3.37, train_loss =  1.21, learning_rate = 0.31250000


[batch = 4200] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.31250000


[batch = 4300] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.31250000


[batch = 4400] train_perplexity =     3.47, train_loss =  1.25, learning_rate = 0.31250000


[batch = 4500] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.31250000


[batch = 4600] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.31250000


[batch = 4700] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.31250000


[batch = 4800] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.31250000


[batch = 4900] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.31250000


[batch = 5000] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.31250000


[batch = 5100] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 0.31250000


[batch = 5200] train_perplexity =     3.54, train_loss =  1.27, learning_rate = 0.31250000


[batch = 5300] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.31250000


[batch = 5400] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.31250000


[batch = 5500] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.31250000


[batch = 5600] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.31250000


[batch = 5700] train_perplexity =     3.34, train_loss =  1.20, learning_rate = 0.31250000


[batch = 5800] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.31250000


[batch = 5900] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.31250000


[batch = 6000] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.31250000


[batch = 6100] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.31250000


[batch = 6200] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.31250000


[batch = 6300] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.31250000


[batch = 6400] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.31250000


[batch = 6500] train_perplexity =     3.36, train_loss =  1.21, learning_rate = 0.31250000


[batch = 6600] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.31250000


[batch = 6700] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.31250000


[batch = 6800] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.31250000


[batch = 6900] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 0.31250000


[epoch =  41] validation_perplexity =     3.59, validation_loss =  1.28

Saved model to 'e200/ptb_char_lstm.pt'...
[epoch =  41] min_validation_perplexity =     3.59, min_validation_loss =  1.28


[batch =  100] train_perplexity =     3.73, train_loss =  1.32, learning_rate = 0.31250000


[batch =  200] train_perplexity =     3.54, train_loss =  1.27, learning_rate = 0.31250000


[batch =  300] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.31250000


[batch =  400] train_perplexity =     3.36, train_loss =  1.21, learning_rate = 0.31250000


[batch =  500] train_perplexity =     3.36, train_loss =  1.21, learning_rate = 0.31250000


[batch =  600] train_perplexity =     3.32, train_loss =  1.20, learning_rate = 0.31250000


[batch =  700] train_perplexity =     3.35, train_loss =  1.21, learning_rate = 0.31250000


[batch =  800] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.31250000


[batch =  900] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.31250000


[batch = 1000] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.31250000


[batch = 1100] train_perplexity =     3.60, train_loss =  1.28, learning_rate = 0.31250000


[batch = 1200] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.31250000


[batch = 1300] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.31250000


[batch = 1400] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.31250000


[batch = 1500] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.31250000


[batch = 1600] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.31250000


[batch = 1700] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.31250000


[batch = 1800] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.31250000


[batch = 1900] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.31250000


[batch = 2000] train_perplexity =     3.58, train_loss =  1.27, learning_rate = 0.31250000


[batch = 2100] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.31250000


[batch = 2200] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.31250000


[batch = 2300] train_perplexity =     3.57, train_loss =  1.27, learning_rate = 0.31250000


[batch = 2400] train_perplexity =     3.47, train_loss =  1.25, learning_rate = 0.31250000


[batch = 2500] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.31250000


[batch = 2600] train_perplexity =     3.56, train_loss =  1.27, learning_rate = 0.31250000


[batch = 2700] train_perplexity =     3.57, train_loss =  1.27, learning_rate = 0.31250000


[batch = 2800] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.31250000


[batch = 2900] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.31250000


[batch = 3000] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.31250000


[batch = 3100] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.31250000


[batch = 3200] train_perplexity =     3.62, train_loss =  1.29, learning_rate = 0.31250000


[batch = 3300] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.31250000


[batch = 3400] train_perplexity =     3.47, train_loss =  1.25, learning_rate = 0.31250000


[batch = 3500] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.31250000


[batch = 3600] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.31250000


[batch = 3700] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.31250000


[batch = 3800] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.31250000


[batch = 3900] train_perplexity =     3.47, train_loss =  1.25, learning_rate = 0.31250000


[batch = 4000] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.31250000


[batch = 4100] train_perplexity =     3.37, train_loss =  1.21, learning_rate = 0.31250000


[batch = 4200] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.31250000


[batch = 4300] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.31250000


[batch = 4400] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.31250000


[batch = 4500] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.31250000


[batch = 4600] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.31250000


[batch = 4700] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.31250000


[batch = 4800] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.31250000


[batch = 4900] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.31250000


[batch = 5000] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.31250000


[batch = 5100] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 0.31250000


[batch = 5200] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.31250000


[batch = 5300] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.31250000


[batch = 5400] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.31250000


[batch = 5500] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.31250000


[batch = 5600] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.31250000


[batch = 5700] train_perplexity =     3.34, train_loss =  1.20, learning_rate = 0.31250000


[batch = 5800] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.31250000


[batch = 5900] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.31250000


[batch = 6000] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.31250000


[batch = 6100] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.31250000


[batch = 6200] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.31250000


[batch = 6300] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.31250000


[batch = 6400] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.31250000


[batch = 6500] train_perplexity =     3.36, train_loss =  1.21, learning_rate = 0.31250000


[batch = 6600] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.31250000


[batch = 6700] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.31250000


[batch = 6800] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.31250000


[batch = 6900] train_perplexity =     3.54, train_loss =  1.27, learning_rate = 0.31250000


[epoch =  42] validation_perplexity =     3.59, validation_loss =  1.28

Saved model to 'e200/ptb_char_lstm.pt'...
[epoch =  42] min_validation_perplexity =     3.59, min_validation_loss =  1.28


[batch =  100] train_perplexity =     3.73, train_loss =  1.32, learning_rate = 0.31250000


[batch =  200] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.31250000


[batch =  300] train_perplexity =     3.47, train_loss =  1.25, learning_rate = 0.31250000


[batch =  400] train_perplexity =     3.36, train_loss =  1.21, learning_rate = 0.31250000


[batch =  500] train_perplexity =     3.36, train_loss =  1.21, learning_rate = 0.31250000


[batch =  600] train_perplexity =     3.32, train_loss =  1.20, learning_rate = 0.31250000


[batch =  700] train_perplexity =     3.35, train_loss =  1.21, learning_rate = 0.31250000


[batch =  800] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.31250000


[batch =  900] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.31250000


[batch = 1000] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.31250000


[batch = 1100] train_perplexity =     3.60, train_loss =  1.28, learning_rate = 0.31250000


[batch = 1200] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.31250000


[batch = 1300] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.31250000


[batch = 1400] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.31250000


[batch = 1500] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.31250000


[batch = 1600] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.31250000


[batch = 1700] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.31250000


[batch = 1800] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.31250000


[batch = 1900] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.31250000


[batch = 2000] train_perplexity =     3.58, train_loss =  1.27, learning_rate = 0.31250000


[batch = 2100] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.31250000


[batch = 2200] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.31250000


[batch = 2300] train_perplexity =     3.57, train_loss =  1.27, learning_rate = 0.31250000


[batch = 2400] train_perplexity =     3.47, train_loss =  1.25, learning_rate = 0.31250000


[batch = 2500] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.31250000


[batch = 2600] train_perplexity =     3.56, train_loss =  1.27, learning_rate = 0.31250000


[batch = 2700] train_perplexity =     3.57, train_loss =  1.27, learning_rate = 0.31250000


[batch = 2800] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.31250000


[batch = 2900] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.31250000


[batch = 3000] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.31250000


[batch = 3100] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.31250000


[batch = 3200] train_perplexity =     3.62, train_loss =  1.29, learning_rate = 0.31250000


[batch = 3300] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.31250000


[batch = 3400] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.31250000


[batch = 3500] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.31250000


[batch = 3600] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.31250000


[batch = 3700] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.31250000


[batch = 3800] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.31250000


[batch = 3900] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.31250000


[batch = 4000] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.31250000


[batch = 4100] train_perplexity =     3.37, train_loss =  1.21, learning_rate = 0.31250000


[batch = 4200] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.31250000


[batch = 4300] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.31250000


[batch = 4400] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.31250000


[batch = 4500] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.31250000


[batch = 4600] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.31250000


[batch = 4700] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.31250000


[batch = 4800] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.31250000


[batch = 4900] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.31250000


[batch = 5000] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.31250000


[batch = 5100] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 0.31250000


[batch = 5200] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.31250000


[batch = 5300] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.31250000


[batch = 5400] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.31250000


[batch = 5500] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.31250000


[batch = 5600] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.31250000


[batch = 5700] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.31250000


[batch = 5800] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.31250000


[batch = 5900] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.31250000


[batch = 6000] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.31250000


[batch = 6100] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.31250000


[batch = 6200] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.31250000


[batch = 6300] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.31250000


[batch = 6400] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.31250000


[batch = 6500] train_perplexity =     3.36, train_loss =  1.21, learning_rate = 0.31250000


[batch = 6600] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.31250000


[batch = 6700] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.31250000


[batch = 6800] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.31250000


[batch = 6900] train_perplexity =     3.54, train_loss =  1.27, learning_rate = 0.31250000


[epoch =  43] validation_perplexity =     3.59, validation_loss =  1.28

Saved model to 'e200/ptb_char_lstm.pt'...
[epoch =  43] min_validation_perplexity =     3.59, min_validation_loss =  1.28


[batch =  100] train_perplexity =     3.73, train_loss =  1.32, learning_rate = 0.31250000


[batch =  200] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.31250000


[batch =  300] train_perplexity =     3.47, train_loss =  1.25, learning_rate = 0.31250000


[batch =  400] train_perplexity =     3.36, train_loss =  1.21, learning_rate = 0.31250000


[batch =  500] train_perplexity =     3.36, train_loss =  1.21, learning_rate = 0.31250000


[batch =  600] train_perplexity =     3.32, train_loss =  1.20, learning_rate = 0.31250000


[batch =  700] train_perplexity =     3.35, train_loss =  1.21, learning_rate = 0.31250000


[batch =  800] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.31250000


[batch =  900] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.31250000


[batch = 1000] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.31250000


[batch = 1100] train_perplexity =     3.60, train_loss =  1.28, learning_rate = 0.31250000


[batch = 1200] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.31250000


[batch = 1300] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.31250000


[batch = 1400] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.31250000


[batch = 1500] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.31250000


[batch = 1600] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.31250000


[batch = 1700] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.31250000


[batch = 1800] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.31250000


[batch = 1900] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.31250000


[batch = 2000] train_perplexity =     3.57, train_loss =  1.27, learning_rate = 0.31250000


[batch = 2100] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.31250000


[batch = 2200] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.31250000


[batch = 2300] train_perplexity =     3.57, train_loss =  1.27, learning_rate = 0.31250000


[batch = 2400] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.31250000


[batch = 2500] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.31250000


[batch = 2600] train_perplexity =     3.56, train_loss =  1.27, learning_rate = 0.31250000


[batch = 2700] train_perplexity =     3.57, train_loss =  1.27, learning_rate = 0.31250000


[batch = 2800] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.31250000


[batch = 2900] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.31250000


[batch = 3000] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.31250000


[batch = 3100] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.31250000


[batch = 3200] train_perplexity =     3.62, train_loss =  1.29, learning_rate = 0.31250000


[batch = 3300] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.31250000


[batch = 3400] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.31250000


[batch = 3500] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.31250000


[batch = 3600] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.31250000


[batch = 3700] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.31250000


[batch = 3800] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.31250000


[batch = 3900] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.31250000


[batch = 4000] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.31250000


[batch = 4100] train_perplexity =     3.37, train_loss =  1.21, learning_rate = 0.31250000


[batch = 4200] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.31250000


[batch = 4300] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.31250000


[batch = 4400] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.31250000


[batch = 4500] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.31250000


[batch = 4600] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.31250000


[batch = 4700] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.31250000


[batch = 4800] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.31250000


[batch = 4900] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.31250000


[batch = 5000] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.31250000


[batch = 5100] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 0.31250000


[batch = 5200] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.31250000


[batch = 5300] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.31250000


[batch = 5400] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.31250000


[batch = 5500] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.31250000


[batch = 5600] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.31250000


[batch = 5700] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.31250000


[batch = 5800] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.31250000


[batch = 5900] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.31250000


[batch = 6000] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.31250000


[batch = 6100] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.31250000


[batch = 6200] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.31250000


[batch = 6300] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.31250000


[batch = 6400] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.31250000


[batch = 6500] train_perplexity =     3.36, train_loss =  1.21, learning_rate = 0.31250000


[batch = 6600] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.31250000


[batch = 6700] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.31250000


[batch = 6800] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.31250000


[batch = 6900] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.31250000


[epoch =  44] validation_perplexity =     3.59, validation_loss =  1.28

Saved model to 'e200/ptb_char_lstm.pt'...
[epoch =  44] min_validation_perplexity =     3.59, min_validation_loss =  1.28


[batch =  100] train_perplexity =     3.73, train_loss =  1.32, learning_rate = 0.31250000


[batch =  200] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.31250000


[batch =  300] train_perplexity =     3.47, train_loss =  1.25, learning_rate = 0.31250000


[batch =  400] train_perplexity =     3.36, train_loss =  1.21, learning_rate = 0.31250000


[batch =  500] train_perplexity =     3.35, train_loss =  1.21, learning_rate = 0.31250000


[batch =  600] train_perplexity =     3.32, train_loss =  1.20, learning_rate = 0.31250000


[batch =  700] train_perplexity =     3.35, train_loss =  1.21, learning_rate = 0.31250000


[batch =  800] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.31250000


[batch =  900] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.31250000


[batch = 1000] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.31250000


[batch = 1100] train_perplexity =     3.60, train_loss =  1.28, learning_rate = 0.31250000


[batch = 1200] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.31250000


[batch = 1300] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.31250000


[batch = 1400] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.31250000


[batch = 1500] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.31250000


[batch = 1600] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.31250000


[batch = 1700] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.31250000


[batch = 1800] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.31250000


[batch = 1900] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.31250000


[batch = 2000] train_perplexity =     3.57, train_loss =  1.27, learning_rate = 0.31250000


[batch = 2100] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.31250000


[batch = 2200] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.31250000


[batch = 2300] train_perplexity =     3.57, train_loss =  1.27, learning_rate = 0.31250000


[batch = 2400] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.31250000


[batch = 2500] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.31250000


[batch = 2600] train_perplexity =     3.56, train_loss =  1.27, learning_rate = 0.31250000


[batch = 2700] train_perplexity =     3.57, train_loss =  1.27, learning_rate = 0.31250000


[batch = 2800] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.31250000


[batch = 2900] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.31250000


[batch = 3000] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.31250000


[batch = 3100] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.31250000


[batch = 3200] train_perplexity =     3.62, train_loss =  1.29, learning_rate = 0.31250000


[batch = 3300] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.31250000


[batch = 3400] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.31250000


[batch = 3500] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.31250000


[batch = 3600] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.31250000


[batch = 3700] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.31250000


[batch = 3800] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.31250000


[batch = 3900] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.31250000


[batch = 4000] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.31250000


[batch = 4100] train_perplexity =     3.37, train_loss =  1.21, learning_rate = 0.31250000


[batch = 4200] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.31250000


[batch = 4300] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.31250000


[batch = 4400] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.31250000


[batch = 4500] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.31250000


[batch = 4600] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.31250000


[batch = 4700] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.31250000


[batch = 4800] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.31250000


[batch = 4900] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.31250000


[batch = 5000] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.31250000


[batch = 5100] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 0.31250000


[batch = 5200] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.31250000


[batch = 5300] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.31250000


[batch = 5400] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.31250000


[batch = 5500] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.31250000


[batch = 5600] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.31250000


[batch = 5700] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.31250000


[batch = 5800] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.31250000


[batch = 5900] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.31250000


[batch = 6000] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.31250000


[batch = 6100] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.31250000


[batch = 6200] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.31250000


[batch = 6300] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.31250000


[batch = 6400] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.31250000


[batch = 6500] train_perplexity =     3.36, train_loss =  1.21, learning_rate = 0.31250000


[batch = 6600] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.31250000


[batch = 6700] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.31250000


[batch = 6800] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.31250000


[batch = 6900] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.31250000


[epoch =  45] validation_perplexity =     3.59, validation_loss =  1.28

Saved model to 'e200/ptb_char_lstm.pt'...
[epoch =  45] min_validation_perplexity =     3.59, min_validation_loss =  1.28


[batch =  100] train_perplexity =     3.73, train_loss =  1.32, learning_rate = 0.31250000


[batch =  200] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.31250000


[batch =  300] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.31250000


[batch =  400] train_perplexity =     3.36, train_loss =  1.21, learning_rate = 0.31250000


[batch =  500] train_perplexity =     3.35, train_loss =  1.21, learning_rate = 0.31250000


[batch =  600] train_perplexity =     3.32, train_loss =  1.20, learning_rate = 0.31250000


[batch =  700] train_perplexity =     3.35, train_loss =  1.21, learning_rate = 0.31250000


[batch =  800] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.31250000


[batch =  900] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.31250000


[batch = 1000] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.31250000


[batch = 1100] train_perplexity =     3.60, train_loss =  1.28, learning_rate = 0.31250000


[batch = 1200] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.31250000


[batch = 1300] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.31250000


[batch = 1400] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.31250000


[batch = 1500] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.31250000


[batch = 1600] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.31250000


[batch = 1700] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.31250000


[batch = 1800] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.31250000


[batch = 1900] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.31250000


[batch = 2000] train_perplexity =     3.57, train_loss =  1.27, learning_rate = 0.31250000


[batch = 2100] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.31250000


[batch = 2200] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.31250000


[batch = 2300] train_perplexity =     3.57, train_loss =  1.27, learning_rate = 0.31250000


[batch = 2400] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.31250000


[batch = 2500] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.31250000


[batch = 2600] train_perplexity =     3.56, train_loss =  1.27, learning_rate = 0.31250000


[batch = 2700] train_perplexity =     3.57, train_loss =  1.27, learning_rate = 0.31250000


[batch = 2800] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.31250000


[batch = 2900] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.31250000


[batch = 3000] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.31250000


[batch = 3100] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.31250000


[batch = 3200] train_perplexity =     3.62, train_loss =  1.29, learning_rate = 0.31250000


[batch = 3300] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.31250000


[batch = 3400] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.31250000


[batch = 3500] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.31250000


[batch = 3600] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.31250000


[batch = 3700] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.31250000


[batch = 3800] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.31250000


[batch = 3900] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.31250000


[batch = 4000] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.31250000


[batch = 4100] train_perplexity =     3.37, train_loss =  1.21, learning_rate = 0.31250000


[batch = 4200] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.31250000


[batch = 4300] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.31250000


[batch = 4400] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.31250000


[batch = 4500] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.31250000


[batch = 4600] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.31250000


[batch = 4700] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.31250000


[batch = 4800] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.31250000


[batch = 4900] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.31250000


[batch = 5000] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.31250000


[batch = 5100] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 0.31250000


[batch = 5200] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.31250000


[batch = 5300] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.31250000


[batch = 5400] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.31250000


[batch = 5500] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.31250000


[batch = 5600] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.31250000


[batch = 5700] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.31250000


[batch = 5800] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.31250000


[batch = 5900] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.31250000


[batch = 6000] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.31250000


[batch = 6100] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.31250000


[batch = 6200] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.31250000


[batch = 6300] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.31250000


[batch = 6400] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.31250000


[batch = 6500] train_perplexity =     3.36, train_loss =  1.21, learning_rate = 0.31250000


[batch = 6600] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.31250000


[batch = 6700] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.31250000


[batch = 6800] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.31250000


[batch = 6900] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.31250000


[epoch =  46] validation_perplexity =     3.59, validation_loss =  1.28

Saved model to 'e200/ptb_char_lstm.pt'...
[epoch =  46] min_validation_perplexity =     3.59, min_validation_loss =  1.28


[batch =  100] train_perplexity =     3.73, train_loss =  1.32, learning_rate = 0.31250000


[batch =  200] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.31250000


[batch =  300] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.31250000


[batch =  400] train_perplexity =     3.36, train_loss =  1.21, learning_rate = 0.31250000


[batch =  500] train_perplexity =     3.35, train_loss =  1.21, learning_rate = 0.31250000


[batch =  600] train_perplexity =     3.32, train_loss =  1.20, learning_rate = 0.31250000


[batch =  700] train_perplexity =     3.35, train_loss =  1.21, learning_rate = 0.31250000


[batch =  800] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.31250000


[batch =  900] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.31250000


[batch = 1000] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.31250000


[batch = 1100] train_perplexity =     3.60, train_loss =  1.28, learning_rate = 0.31250000


[batch = 1200] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.31250000


[batch = 1300] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.31250000


[batch = 1400] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.31250000


[batch = 1500] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.31250000


[batch = 1600] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.31250000


[batch = 1700] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.31250000


[batch = 1800] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.31250000


[batch = 1900] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.31250000


[batch = 2000] train_perplexity =     3.57, train_loss =  1.27, learning_rate = 0.31250000


[batch = 2100] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.31250000


[batch = 2200] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.31250000


[batch = 2300] train_perplexity =     3.57, train_loss =  1.27, learning_rate = 0.31250000


[batch = 2400] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.31250000


[batch = 2500] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.31250000


[batch = 2600] train_perplexity =     3.56, train_loss =  1.27, learning_rate = 0.31250000


[batch = 2700] train_perplexity =     3.57, train_loss =  1.27, learning_rate = 0.31250000


[batch = 2800] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.31250000


[batch = 2900] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.31250000


[batch = 3000] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.31250000


[batch = 3100] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.31250000


[batch = 3200] train_perplexity =     3.62, train_loss =  1.29, learning_rate = 0.31250000


[batch = 3300] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.31250000


[batch = 3400] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.31250000


[batch = 3500] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.31250000


[batch = 3600] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.31250000


[batch = 3700] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.31250000


[batch = 3800] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.31250000


[batch = 3900] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.31250000


[batch = 4000] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.31250000


[batch = 4100] train_perplexity =     3.36, train_loss =  1.21, learning_rate = 0.31250000


[batch = 4200] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.31250000


[batch = 4300] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.31250000


[batch = 4400] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.31250000


[batch = 4500] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.31250000


[batch = 4600] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.31250000


[batch = 4700] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.31250000


[batch = 4800] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.31250000


[batch = 4900] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.31250000


[batch = 5000] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.31250000


[batch = 5100] train_perplexity =     3.54, train_loss =  1.27, learning_rate = 0.31250000


[batch = 5200] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.31250000


[batch = 5300] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.31250000


[batch = 5400] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.31250000


[batch = 5500] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.31250000


[batch = 5600] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.31250000


[batch = 5700] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.31250000


[batch = 5800] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.31250000


[batch = 5900] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.31250000


[batch = 6000] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.31250000


[batch = 6100] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.31250000


[batch = 6200] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.31250000


[batch = 6300] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.31250000


[batch = 6400] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.31250000


[batch = 6500] train_perplexity =     3.36, train_loss =  1.21, learning_rate = 0.31250000


[batch = 6600] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.31250000


[batch = 6700] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.31250000


[batch = 6800] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.31250000


[batch = 6900] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.31250000


[epoch =  47] validation_perplexity =     3.59, validation_loss =  1.28

Saved model to 'e200/ptb_char_lstm.pt'...
[epoch =  47] min_validation_perplexity =     3.59, min_validation_loss =  1.28


[batch =  100] train_perplexity =     3.72, train_loss =  1.31, learning_rate = 0.31250000


[batch =  200] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.31250000


[batch =  300] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.31250000


[batch =  400] train_perplexity =     3.36, train_loss =  1.21, learning_rate = 0.31250000


[batch =  500] train_perplexity =     3.35, train_loss =  1.21, learning_rate = 0.31250000


[batch =  600] train_perplexity =     3.32, train_loss =  1.20, learning_rate = 0.31250000


[batch =  700] train_perplexity =     3.35, train_loss =  1.21, learning_rate = 0.31250000


[batch =  800] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.31250000


[batch =  900] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.31250000


[batch = 1000] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.31250000


[batch = 1100] train_perplexity =     3.60, train_loss =  1.28, learning_rate = 0.31250000


[batch = 1200] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.31250000


[batch = 1300] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.31250000


[batch = 1400] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.31250000


[batch = 1500] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.31250000


[batch = 1600] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.31250000


[batch = 1700] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.31250000


[batch = 1800] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.31250000


[batch = 1900] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.31250000


[batch = 2000] train_perplexity =     3.57, train_loss =  1.27, learning_rate = 0.31250000


[batch = 2100] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.31250000


[batch = 2200] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.31250000


[batch = 2300] train_perplexity =     3.57, train_loss =  1.27, learning_rate = 0.31250000


[batch = 2400] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.31250000


[batch = 2500] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.31250000


[batch = 2600] train_perplexity =     3.56, train_loss =  1.27, learning_rate = 0.31250000


[batch = 2700] train_perplexity =     3.57, train_loss =  1.27, learning_rate = 0.31250000


[batch = 2800] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.31250000


[batch = 2900] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.31250000


[batch = 3000] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.31250000


[batch = 3100] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.31250000


[batch = 3200] train_perplexity =     3.62, train_loss =  1.29, learning_rate = 0.31250000


[batch = 3300] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.31250000


[batch = 3400] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.31250000


[batch = 3500] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.31250000


[batch = 3600] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.31250000


[batch = 3700] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.31250000


[batch = 3800] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.31250000


[batch = 3900] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.31250000


[batch = 4000] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.31250000


[batch = 4100] train_perplexity =     3.36, train_loss =  1.21, learning_rate = 0.31250000


[batch = 4200] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.31250000


[batch = 4300] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.31250000


[batch = 4400] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.31250000


[batch = 4500] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.31250000


[batch = 4600] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.31250000


[batch = 4700] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.31250000


[batch = 4800] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.31250000


[batch = 4900] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.31250000


[batch = 5000] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.31250000


[batch = 5100] train_perplexity =     3.54, train_loss =  1.27, learning_rate = 0.31250000


[batch = 5200] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.31250000


[batch = 5300] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.31250000


[batch = 5400] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.31250000


[batch = 5500] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.31250000


[batch = 5600] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.31250000


[batch = 5700] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.31250000


[batch = 5800] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.31250000


[batch = 5900] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.31250000


[batch = 6000] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.31250000


[batch = 6100] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.31250000


[batch = 6200] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.31250000


[batch = 6300] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.31250000


[batch = 6400] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.31250000


[batch = 6500] train_perplexity =     3.35, train_loss =  1.21, learning_rate = 0.31250000


[batch = 6600] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.31250000


[batch = 6700] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.31250000


[batch = 6800] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.31250000


[batch = 6900] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.31250000


[epoch =  48] validation_perplexity =     3.59, validation_loss =  1.28

Saved model to 'e200/ptb_char_lstm.pt'...
[epoch =  48] min_validation_perplexity =     3.59, min_validation_loss =  1.28


[batch =  100] train_perplexity =     3.72, train_loss =  1.31, learning_rate = 0.31250000


[batch =  200] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.31250000


[batch =  300] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.31250000


[batch =  400] train_perplexity =     3.36, train_loss =  1.21, learning_rate = 0.31250000


[batch =  500] train_perplexity =     3.35, train_loss =  1.21, learning_rate = 0.31250000


[batch =  600] train_perplexity =     3.32, train_loss =  1.20, learning_rate = 0.31250000


[batch =  700] train_perplexity =     3.35, train_loss =  1.21, learning_rate = 0.31250000


[batch =  800] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.31250000


[batch =  900] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.31250000


[batch = 1000] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.31250000


[batch = 1100] train_perplexity =     3.60, train_loss =  1.28, learning_rate = 0.31250000


[batch = 1200] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.31250000


[batch = 1300] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.31250000


[batch = 1400] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.31250000


[batch = 1500] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.31250000


[batch = 1600] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.31250000


[batch = 1700] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.31250000


[batch = 1800] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.31250000


[batch = 1900] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.31250000


[batch = 2000] train_perplexity =     3.57, train_loss =  1.27, learning_rate = 0.31250000


[batch = 2100] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.31250000


[batch = 2200] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.31250000


[batch = 2300] train_perplexity =     3.57, train_loss =  1.27, learning_rate = 0.31250000


[batch = 2400] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.31250000


[batch = 2500] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.31250000


[batch = 2600] train_perplexity =     3.56, train_loss =  1.27, learning_rate = 0.31250000


[batch = 2700] train_perplexity =     3.57, train_loss =  1.27, learning_rate = 0.31250000


[batch = 2800] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.31250000


[batch = 2900] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.31250000


[batch = 3000] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.31250000


[batch = 3100] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.31250000


[batch = 3200] train_perplexity =     3.62, train_loss =  1.29, learning_rate = 0.31250000


[batch = 3300] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.31250000


[batch = 3400] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.31250000


[batch = 3500] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.31250000


[batch = 3600] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.31250000


[batch = 3700] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.31250000


[batch = 3800] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.31250000


[batch = 3900] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.31250000


[batch = 4000] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.31250000


[batch = 4100] train_perplexity =     3.36, train_loss =  1.21, learning_rate = 0.31250000


[batch = 4200] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.31250000


[batch = 4300] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.31250000


[batch = 4400] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.31250000


[batch = 4500] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.31250000


[batch = 4600] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.31250000


[batch = 4700] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.31250000


[batch = 4800] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.31250000


[batch = 4900] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.31250000


[batch = 5000] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.31250000


[batch = 5100] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.31250000


[batch = 5200] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.31250000


[batch = 5300] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.31250000


[batch = 5400] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.31250000


[batch = 5500] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.31250000


[batch = 5600] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.31250000


[batch = 5700] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.31250000


[batch = 5800] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.31250000


[batch = 5900] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.31250000


[batch = 6000] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.31250000


[batch = 6100] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.31250000


[batch = 6200] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.31250000


[batch = 6300] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.31250000


[batch = 6400] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.31250000


[batch = 6500] train_perplexity =     3.35, train_loss =  1.21, learning_rate = 0.31250000


[batch = 6600] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.31250000


[batch = 6700] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.31250000


[batch = 6800] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.31250000


[batch = 6900] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.31250000


[epoch =  49] validation_perplexity =     3.59, validation_loss =  1.28

Saved model to 'e200/ptb_char_lstm.pt'...
[epoch =  49] min_validation_perplexity =     3.59, min_validation_loss =  1.28


[batch =  100] train_perplexity =     3.72, train_loss =  1.31, learning_rate = 0.31250000


[batch =  200] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.31250000


[batch =  300] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.31250000


[batch =  400] train_perplexity =     3.36, train_loss =  1.21, learning_rate = 0.31250000


[batch =  500] train_perplexity =     3.35, train_loss =  1.21, learning_rate = 0.31250000


[batch =  600] train_perplexity =     3.32, train_loss =  1.20, learning_rate = 0.31250000


[batch =  700] train_perplexity =     3.35, train_loss =  1.21, learning_rate = 0.31250000


[batch =  800] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.31250000


[batch =  900] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.31250000


[batch = 1000] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.31250000


[batch = 1100] train_perplexity =     3.60, train_loss =  1.28, learning_rate = 0.31250000


[batch = 1200] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.31250000


[batch = 1300] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.31250000


[batch = 1400] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.31250000


[batch = 1500] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.31250000


[batch = 1600] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.31250000


[batch = 1700] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.31250000


[batch = 1800] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.31250000


[batch = 1900] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.31250000


[batch = 2000] train_perplexity =     3.57, train_loss =  1.27, learning_rate = 0.31250000


[batch = 2100] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.31250000


[batch = 2200] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.31250000


[batch = 2300] train_perplexity =     3.57, train_loss =  1.27, learning_rate = 0.31250000


[batch = 2400] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.31250000


[batch = 2500] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.31250000


[batch = 2600] train_perplexity =     3.56, train_loss =  1.27, learning_rate = 0.31250000


[batch = 2700] train_perplexity =     3.57, train_loss =  1.27, learning_rate = 0.31250000


[batch = 2800] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.31250000


[batch = 2900] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.31250000


[batch = 3000] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.31250000


[batch = 3100] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.31250000


[batch = 3200] train_perplexity =     3.62, train_loss =  1.29, learning_rate = 0.31250000


[batch = 3300] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.31250000


[batch = 3400] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.31250000


[batch = 3500] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.31250000


[batch = 3600] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.31250000


[batch = 3700] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.31250000


[batch = 3800] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.31250000


[batch = 3900] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.31250000


[batch = 4000] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.31250000


[batch = 4100] train_perplexity =     3.36, train_loss =  1.21, learning_rate = 0.31250000


[batch = 4200] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.31250000


[batch = 4300] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.31250000


[batch = 4400] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.31250000


[batch = 4500] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.31250000


[batch = 4600] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.31250000


[batch = 4700] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.31250000


[batch = 4800] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.31250000


[batch = 4900] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.31250000


[batch = 5000] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.31250000


[batch = 5100] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.31250000


[batch = 5200] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.31250000


[batch = 5300] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.31250000


[batch = 5400] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.31250000


[batch = 5500] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.31250000


[batch = 5600] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.31250000


[batch = 5700] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.31250000


[batch = 5800] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.31250000


[batch = 5900] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.31250000


[batch = 6000] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.31250000


[batch = 6100] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.31250000


[batch = 6200] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.31250000


[batch = 6300] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.31250000


[batch = 6400] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.31250000


[batch = 6500] train_perplexity =     3.35, train_loss =  1.21, learning_rate = 0.31250000


[batch = 6600] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.31250000


[batch = 6700] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.31250000


[batch = 6800] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.31250000


[batch = 6900] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.31250000


[epoch =  50] validation_perplexity =     3.59, validation_loss =  1.28

Saved model to 'e200/ptb_char_lstm.pt'...
[epoch =  50] min_validation_perplexity =     3.59, min_validation_loss =  1.28


[batch =  100] train_perplexity =     3.72, train_loss =  1.31, learning_rate = 0.31250000


[batch =  200] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.31250000


[batch =  300] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.31250000


[batch =  400] train_perplexity =     3.35, train_loss =  1.21, learning_rate = 0.31250000


[batch =  500] train_perplexity =     3.35, train_loss =  1.21, learning_rate = 0.31250000


[batch =  600] train_perplexity =     3.32, train_loss =  1.20, learning_rate = 0.31250000


[batch =  700] train_perplexity =     3.35, train_loss =  1.21, learning_rate = 0.31250000


[batch =  800] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.31250000


[batch =  900] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.31250000


[batch = 1000] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.31250000


[batch = 1100] train_perplexity =     3.60, train_loss =  1.28, learning_rate = 0.31250000


[batch = 1200] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.31250000


[batch = 1300] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.31250000


[batch = 1400] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.31250000


[batch = 1500] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.31250000


[batch = 1600] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.31250000


[batch = 1700] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.31250000


[batch = 1800] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.31250000


[batch = 1900] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.31250000


[batch = 2000] train_perplexity =     3.57, train_loss =  1.27, learning_rate = 0.31250000


[batch = 2100] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.31250000


[batch = 2200] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.31250000


[batch = 2300] train_perplexity =     3.57, train_loss =  1.27, learning_rate = 0.31250000


[batch = 2400] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.31250000


[batch = 2500] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.31250000


[batch = 2600] train_perplexity =     3.56, train_loss =  1.27, learning_rate = 0.31250000


[batch = 2700] train_perplexity =     3.57, train_loss =  1.27, learning_rate = 0.31250000


[batch = 2800] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.31250000


[batch = 2900] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.31250000


[batch = 3000] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.31250000


[batch = 3100] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.31250000


[batch = 3200] train_perplexity =     3.62, train_loss =  1.29, learning_rate = 0.31250000


[batch = 3300] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.31250000


[batch = 3400] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.31250000


[batch = 3500] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.31250000


[batch = 3600] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.31250000


[batch = 3700] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.31250000


[batch = 3800] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.31250000


[batch = 3900] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.31250000


[batch = 4000] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.31250000


[batch = 4100] train_perplexity =     3.36, train_loss =  1.21, learning_rate = 0.31250000


[batch = 4200] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.31250000


[batch = 4300] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.31250000


[batch = 4400] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.31250000


[batch = 4500] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.31250000


[batch = 4600] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.31250000


[batch = 4700] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.31250000


[batch = 4800] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.31250000


[batch = 4900] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.31250000


[batch = 5000] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.31250000


[batch = 5100] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.31250000


[batch = 5200] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.31250000


[batch = 5300] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.31250000


[batch = 5400] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.31250000


[batch = 5500] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.31250000


[batch = 5600] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.31250000


[batch = 5700] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.31250000


[batch = 5800] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.31250000


[batch = 5900] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.31250000


[batch = 6000] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.31250000


[batch = 6100] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.31250000


[batch = 6200] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.31250000


[batch = 6300] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.31250000


[batch = 6400] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.31250000


[batch = 6500] train_perplexity =     3.35, train_loss =  1.21, learning_rate = 0.31250000


[batch = 6600] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.31250000


[batch = 6700] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.31250000


[batch = 6800] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.31250000


[batch = 6900] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.31250000


[epoch =  51] validation_perplexity =     3.58, validation_loss =  1.28

Saved model to 'e200/ptb_char_lstm.pt'...
[epoch =  51] min_validation_perplexity =     3.58, min_validation_loss =  1.28


[batch =  100] train_perplexity =     3.72, train_loss =  1.31, learning_rate = 0.31250000


[batch =  200] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.31250000


[batch =  300] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.31250000


[batch =  400] train_perplexity =     3.35, train_loss =  1.21, learning_rate = 0.31250000


[batch =  500] train_perplexity =     3.35, train_loss =  1.21, learning_rate = 0.31250000


[batch =  600] train_perplexity =     3.32, train_loss =  1.20, learning_rate = 0.31250000


[batch =  700] train_perplexity =     3.34, train_loss =  1.21, learning_rate = 0.31250000


[batch =  800] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.31250000


[batch =  900] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.31250000


[batch = 1000] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.31250000


[batch = 1100] train_perplexity =     3.60, train_loss =  1.28, learning_rate = 0.31250000


[batch = 1200] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.31250000


[batch = 1300] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.31250000


[batch = 1400] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.31250000


[batch = 1500] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.31250000


[batch = 1600] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.31250000


[batch = 1700] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.31250000


[batch = 1800] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.31250000


[batch = 1900] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.31250000


[batch = 2000] train_perplexity =     3.57, train_loss =  1.27, learning_rate = 0.31250000


[batch = 2100] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.31250000


[batch = 2200] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.31250000


[batch = 2300] train_perplexity =     3.57, train_loss =  1.27, learning_rate = 0.31250000


[batch = 2400] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.31250000


[batch = 2500] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.31250000


[batch = 2600] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 0.31250000


[batch = 2700] train_perplexity =     3.57, train_loss =  1.27, learning_rate = 0.31250000


[batch = 2800] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.31250000


[batch = 2900] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.31250000


[batch = 3000] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.31250000


[batch = 3100] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.31250000


[batch = 3200] train_perplexity =     3.61, train_loss =  1.28, learning_rate = 0.31250000


[batch = 3300] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.31250000


[batch = 3400] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.31250000


[batch = 3500] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.31250000


[batch = 3600] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.31250000


[batch = 3700] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.31250000


[batch = 3800] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.31250000


[batch = 3900] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.31250000


[batch = 4000] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.31250000


[batch = 4100] train_perplexity =     3.36, train_loss =  1.21, learning_rate = 0.31250000


[batch = 4200] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.31250000


[batch = 4300] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.31250000


[batch = 4400] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.31250000


[batch = 4500] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.31250000


[batch = 4600] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.31250000


[batch = 4700] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.31250000


[batch = 4800] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.31250000


[batch = 4900] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.31250000


[batch = 5000] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.31250000


[batch = 5100] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.31250000


[batch = 5200] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.31250000


[batch = 5300] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.31250000


[batch = 5400] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.31250000


[batch = 5500] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.31250000


[batch = 5600] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.31250000


[batch = 5700] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.31250000


[batch = 5800] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.31250000


[batch = 5900] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.31250000


[batch = 6000] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.31250000


[batch = 6100] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.31250000


[batch = 6200] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.31250000


[batch = 6300] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.31250000


[batch = 6400] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.31250000


[batch = 6500] train_perplexity =     3.35, train_loss =  1.21, learning_rate = 0.31250000


[batch = 6600] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.31250000


[batch = 6700] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.31250000


[batch = 6800] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.31250000


[batch = 6900] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.31250000


[epoch =  52] validation_perplexity =     3.58, validation_loss =  1.28

Saved model to 'e200/ptb_char_lstm.pt'...
[epoch =  52] min_validation_perplexity =     3.58, min_validation_loss =  1.28


[batch =  100] train_perplexity =     3.72, train_loss =  1.31, learning_rate = 0.31250000


[batch =  200] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.31250000


[batch =  300] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.31250000


[batch =  400] train_perplexity =     3.35, train_loss =  1.21, learning_rate = 0.31250000


[batch =  500] train_perplexity =     3.35, train_loss =  1.21, learning_rate = 0.31250000


[batch =  600] train_perplexity =     3.32, train_loss =  1.20, learning_rate = 0.31250000


[batch =  700] train_perplexity =     3.34, train_loss =  1.21, learning_rate = 0.31250000


[batch =  800] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.31250000


[batch =  900] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.31250000


[batch = 1000] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.31250000


[batch = 1100] train_perplexity =     3.60, train_loss =  1.28, learning_rate = 0.31250000


[batch = 1200] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.31250000


[batch = 1300] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.31250000


[batch = 1400] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.31250000


[batch = 1500] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.31250000


[batch = 1600] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.31250000


[batch = 1700] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.31250000


[batch = 1800] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.31250000


[batch = 1900] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.31250000


[batch = 2000] train_perplexity =     3.57, train_loss =  1.27, learning_rate = 0.31250000


[batch = 2100] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.31250000


[batch = 2200] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.31250000


[batch = 2300] train_perplexity =     3.57, train_loss =  1.27, learning_rate = 0.31250000


[batch = 2400] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.31250000


[batch = 2500] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.31250000


[batch = 2600] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 0.31250000


[batch = 2700] train_perplexity =     3.57, train_loss =  1.27, learning_rate = 0.31250000


[batch = 2800] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.31250000


[batch = 2900] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.31250000


[batch = 3000] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.31250000


[batch = 3100] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.31250000


[batch = 3200] train_perplexity =     3.61, train_loss =  1.28, learning_rate = 0.31250000


[batch = 3300] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.31250000


[batch = 3400] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.31250000


[batch = 3500] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.31250000


[batch = 3600] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.31250000


[batch = 3700] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.31250000


[batch = 3800] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.31250000


[batch = 3900] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.31250000


[batch = 4000] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.31250000


[batch = 4100] train_perplexity =     3.36, train_loss =  1.21, learning_rate = 0.31250000


[batch = 4200] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.31250000


[batch = 4300] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.31250000


[batch = 4400] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.31250000


[batch = 4500] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.31250000


[batch = 4600] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.31250000


[batch = 4700] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.31250000


[batch = 4800] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.31250000


[batch = 4900] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.31250000


[batch = 5000] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.31250000


[batch = 5100] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.31250000


[batch = 5200] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.31250000


[batch = 5300] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.31250000


[batch = 5400] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.31250000


[batch = 5500] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.31250000


[batch = 5600] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.31250000


[batch = 5700] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.31250000


[batch = 5800] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.31250000


[batch = 5900] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.31250000


[batch = 6000] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.31250000


[batch = 6100] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.31250000


[batch = 6200] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.31250000


[batch = 6300] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.31250000


[batch = 6400] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.31250000


[batch = 6500] train_perplexity =     3.35, train_loss =  1.21, learning_rate = 0.31250000


[batch = 6600] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.31250000


[batch = 6700] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.31250000


[batch = 6800] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.31250000


[batch = 6900] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.31250000


[epoch =  53] validation_perplexity =     3.58, validation_loss =  1.28

Saved model to 'e200/ptb_char_lstm.pt'...
[epoch =  53] min_validation_perplexity =     3.58, min_validation_loss =  1.28


[batch =  100] train_perplexity =     3.72, train_loss =  1.31, learning_rate = 0.31250000


[batch =  200] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.31250000


[batch =  300] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.31250000


[batch =  400] train_perplexity =     3.35, train_loss =  1.21, learning_rate = 0.31250000


[batch =  500] train_perplexity =     3.35, train_loss =  1.21, learning_rate = 0.31250000


[batch =  600] train_perplexity =     3.32, train_loss =  1.20, learning_rate = 0.31250000


[batch =  700] train_perplexity =     3.34, train_loss =  1.21, learning_rate = 0.31250000


[batch =  800] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.31250000


[batch =  900] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.31250000


[batch = 1000] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.31250000


[batch = 1100] train_perplexity =     3.60, train_loss =  1.28, learning_rate = 0.31250000


[batch = 1200] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.31250000


[batch = 1300] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.31250000


[batch = 1400] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.31250000


[batch = 1500] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.31250000


[batch = 1600] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.31250000


[batch = 1700] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.31250000


[batch = 1800] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.31250000


[batch = 1900] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.31250000


[batch = 2000] train_perplexity =     3.57, train_loss =  1.27, learning_rate = 0.31250000


[batch = 2100] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.31250000


[batch = 2200] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.31250000


[batch = 2300] train_perplexity =     3.56, train_loss =  1.27, learning_rate = 0.31250000


[batch = 2400] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.31250000


[batch = 2500] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.31250000


[batch = 2600] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 0.31250000


[batch = 2700] train_perplexity =     3.57, train_loss =  1.27, learning_rate = 0.31250000


[batch = 2800] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.31250000


[batch = 2900] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.31250000


[batch = 3000] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.31250000


[batch = 3100] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.31250000


[batch = 3200] train_perplexity =     3.61, train_loss =  1.28, learning_rate = 0.31250000


[batch = 3300] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.31250000


[batch = 3400] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.31250000


[batch = 3500] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.31250000


[batch = 3600] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.31250000


[batch = 3700] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.31250000


[batch = 3800] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.31250000


[batch = 3900] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.31250000


[batch = 4000] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.31250000


[batch = 4100] train_perplexity =     3.36, train_loss =  1.21, learning_rate = 0.31250000


[batch = 4200] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.31250000


[batch = 4300] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.31250000


[batch = 4400] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.31250000


[batch = 4500] train_perplexity =     3.47, train_loss =  1.25, learning_rate = 0.31250000


[batch = 4600] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.31250000


[batch = 4700] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.31250000


[batch = 4800] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.31250000


[batch = 4900] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.31250000


[batch = 5000] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.31250000


[batch = 5100] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.31250000


[batch = 5200] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.31250000


[batch = 5300] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.31250000


[batch = 5400] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.31250000


[batch = 5500] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.31250000


[batch = 5600] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.31250000


[batch = 5700] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.31250000


[batch = 5800] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.31250000


[batch = 5900] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.31250000


[batch = 6000] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.31250000


[batch = 6100] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.31250000


[batch = 6200] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.31250000


[batch = 6300] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.31250000


[batch = 6400] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.31250000


[batch = 6500] train_perplexity =     3.35, train_loss =  1.21, learning_rate = 0.31250000


[batch = 6600] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.31250000


[batch = 6700] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.31250000


[batch = 6800] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.31250000


[batch = 6900] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.31250000


[epoch =  54] validation_perplexity =     3.58, validation_loss =  1.28

Saved model to 'e200/ptb_char_lstm.pt'...
[epoch =  54] min_validation_perplexity =     3.58, min_validation_loss =  1.28


[batch =  100] train_perplexity =     3.72, train_loss =  1.31, learning_rate = 0.31250000


[batch =  200] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.31250000


[batch =  300] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.31250000


[batch =  400] train_perplexity =     3.35, train_loss =  1.21, learning_rate = 0.31250000


[batch =  500] train_perplexity =     3.35, train_loss =  1.21, learning_rate = 0.31250000


[batch =  600] train_perplexity =     3.32, train_loss =  1.20, learning_rate = 0.31250000


[batch =  700] train_perplexity =     3.34, train_loss =  1.21, learning_rate = 0.31250000


[batch =  800] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.31250000


[batch =  900] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.31250000


[batch = 1000] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.31250000


[batch = 1100] train_perplexity =     3.60, train_loss =  1.28, learning_rate = 0.31250000


[batch = 1200] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.31250000


[batch = 1300] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.31250000


[batch = 1400] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.31250000


[batch = 1500] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.31250000


[batch = 1600] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.31250000


[batch = 1700] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.31250000


[batch = 1800] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.31250000


[batch = 1900] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.31250000


[batch = 2000] train_perplexity =     3.57, train_loss =  1.27, learning_rate = 0.31250000


[batch = 2100] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.31250000


[batch = 2200] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.31250000


[batch = 2300] train_perplexity =     3.56, train_loss =  1.27, learning_rate = 0.31250000


[batch = 2400] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.31250000


[batch = 2500] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.31250000


[batch = 2600] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 0.31250000


[batch = 2700] train_perplexity =     3.57, train_loss =  1.27, learning_rate = 0.31250000


[batch = 2800] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.31250000


[batch = 2900] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.31250000


[batch = 3000] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.31250000


[batch = 3100] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.31250000


[batch = 3200] train_perplexity =     3.61, train_loss =  1.28, learning_rate = 0.31250000


[batch = 3300] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.31250000


[batch = 3400] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.31250000


[batch = 3500] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.31250000


[batch = 3600] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.31250000


[batch = 3700] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.31250000


[batch = 3800] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.31250000


[batch = 3900] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.31250000


[batch = 4000] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.31250000


[batch = 4100] train_perplexity =     3.36, train_loss =  1.21, learning_rate = 0.31250000


[batch = 4200] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.31250000


[batch = 4300] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.31250000


[batch = 4400] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.31250000


[batch = 4500] train_perplexity =     3.47, train_loss =  1.25, learning_rate = 0.31250000


[batch = 4600] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.31250000


[batch = 4700] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.31250000


[batch = 4800] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.31250000


[batch = 4900] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.31250000


[batch = 5000] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.31250000


[batch = 5100] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.31250000


[batch = 5200] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.31250000


[batch = 5300] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.31250000


[batch = 5400] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.31250000


[batch = 5500] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.31250000


[batch = 5600] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.31250000


[batch = 5700] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.31250000


[batch = 5800] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.31250000


[batch = 5900] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.31250000


[batch = 6000] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.31250000


[batch = 6100] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.31250000


[batch = 6200] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.31250000


[batch = 6300] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.31250000


[batch = 6400] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.31250000


[batch = 6500] train_perplexity =     3.35, train_loss =  1.21, learning_rate = 0.31250000


[batch = 6600] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.31250000


[batch = 6700] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.31250000


[batch = 6800] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.31250000


[batch = 6900] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.31250000


[epoch =  55] validation_perplexity =     3.58, validation_loss =  1.28

Saved model to 'e200/ptb_char_lstm.pt'...
[epoch =  55] min_validation_perplexity =     3.58, min_validation_loss =  1.28


[batch =  100] train_perplexity =     3.72, train_loss =  1.31, learning_rate = 0.31250000


[batch =  200] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.31250000


[batch =  300] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.31250000


[batch =  400] train_perplexity =     3.35, train_loss =  1.21, learning_rate = 0.31250000


[batch =  500] train_perplexity =     3.35, train_loss =  1.21, learning_rate = 0.31250000


[batch =  600] train_perplexity =     3.32, train_loss =  1.20, learning_rate = 0.31250000


[batch =  700] train_perplexity =     3.34, train_loss =  1.21, learning_rate = 0.31250000


[batch =  800] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.31250000


[batch =  900] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.31250000


[batch = 1000] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.31250000


[batch = 1100] train_perplexity =     3.60, train_loss =  1.28, learning_rate = 0.31250000


[batch = 1200] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.31250000


[batch = 1300] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.31250000


[batch = 1400] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.31250000


[batch = 1500] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.31250000


[batch = 1600] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.31250000


[batch = 1700] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.31250000


[batch = 1800] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.31250000


[batch = 1900] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.31250000


[batch = 2000] train_perplexity =     3.56, train_loss =  1.27, learning_rate = 0.31250000


[batch = 2100] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.31250000


[batch = 2200] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.31250000


[batch = 2300] train_perplexity =     3.56, train_loss =  1.27, learning_rate = 0.31250000


[batch = 2400] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.31250000


[batch = 2500] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.31250000


[batch = 2600] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 0.31250000


[batch = 2700] train_perplexity =     3.56, train_loss =  1.27, learning_rate = 0.31250000


[batch = 2800] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.31250000


[batch = 2900] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.31250000


[batch = 3000] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.31250000


[batch = 3100] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.31250000


[batch = 3200] train_perplexity =     3.61, train_loss =  1.28, learning_rate = 0.31250000


[batch = 3300] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.31250000


[batch = 3400] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.31250000


[batch = 3500] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.31250000


[batch = 3600] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.31250000


[batch = 3700] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.31250000


[batch = 3800] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.31250000


[batch = 3900] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.31250000


[batch = 4000] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.31250000


[batch = 4100] train_perplexity =     3.36, train_loss =  1.21, learning_rate = 0.31250000


[batch = 4200] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.31250000


[batch = 4300] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.31250000


[batch = 4400] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.31250000


[batch = 4500] train_perplexity =     3.47, train_loss =  1.25, learning_rate = 0.31250000


[batch = 4600] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.31250000


[batch = 4700] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.31250000


[batch = 4800] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.31250000


[batch = 4900] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.31250000


[batch = 5000] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.31250000


[batch = 5100] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.31250000


[batch = 5200] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.31250000


[batch = 5300] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.31250000


[batch = 5400] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.31250000


[batch = 5500] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.31250000


[batch = 5600] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.31250000


[batch = 5700] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.31250000


[batch = 5800] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.31250000


[batch = 5900] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.31250000


[batch = 6000] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.31250000


[batch = 6100] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.31250000


[batch = 6200] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.31250000


[batch = 6300] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.31250000


[batch = 6400] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.31250000


[batch = 6500] train_perplexity =     3.35, train_loss =  1.21, learning_rate = 0.31250000


[batch = 6600] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.31250000


[batch = 6700] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.31250000


[batch = 6800] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.31250000


[batch = 6900] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.31250000


[epoch =  56] validation_perplexity =     3.58, validation_loss =  1.28

Saved model to 'e200/ptb_char_lstm.pt'...
[epoch =  56] min_validation_perplexity =     3.58, min_validation_loss =  1.28


[batch =  100] train_perplexity =     3.72, train_loss =  1.31, learning_rate = 0.31250000


[batch =  200] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.31250000


[batch =  300] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.31250000


[batch =  400] train_perplexity =     3.35, train_loss =  1.21, learning_rate = 0.31250000


[batch =  500] train_perplexity =     3.35, train_loss =  1.21, learning_rate = 0.31250000


[batch =  600] train_perplexity =     3.32, train_loss =  1.20, learning_rate = 0.31250000


[batch =  700] train_perplexity =     3.34, train_loss =  1.21, learning_rate = 0.31250000


[batch =  800] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.31250000


[batch =  900] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.31250000


[batch = 1000] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.31250000


[batch = 1100] train_perplexity =     3.59, train_loss =  1.28, learning_rate = 0.31250000


[batch = 1200] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.31250000


[batch = 1300] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.31250000


[batch = 1400] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.31250000


[batch = 1500] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.31250000


[batch = 1600] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.31250000


[batch = 1700] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.31250000


[batch = 1800] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.31250000


[batch = 1900] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.31250000


[batch = 2000] train_perplexity =     3.56, train_loss =  1.27, learning_rate = 0.31250000


[batch = 2100] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.31250000


[batch = 2200] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.31250000


[batch = 2300] train_perplexity =     3.56, train_loss =  1.27, learning_rate = 0.31250000


[batch = 2400] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.31250000


[batch = 2500] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.31250000


[batch = 2600] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 0.31250000


[batch = 2700] train_perplexity =     3.56, train_loss =  1.27, learning_rate = 0.31250000


[batch = 2800] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.31250000


[batch = 2900] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.31250000


[batch = 3000] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.31250000


[batch = 3100] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.31250000


[batch = 3200] train_perplexity =     3.61, train_loss =  1.28, learning_rate = 0.31250000


[batch = 3300] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.31250000


[batch = 3400] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.31250000


[batch = 3500] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.31250000


[batch = 3600] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.31250000


[batch = 3700] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.31250000


[batch = 3800] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.31250000


[batch = 3900] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.31250000


[batch = 4000] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.31250000


[batch = 4100] train_perplexity =     3.36, train_loss =  1.21, learning_rate = 0.31250000


[batch = 4200] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.31250000


[batch = 4300] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.31250000


[batch = 4400] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.31250000


[batch = 4500] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.31250000


[batch = 4600] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.31250000


[batch = 4700] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.31250000


[batch = 4800] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.31250000


[batch = 4900] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.31250000


[batch = 5000] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.31250000


[batch = 5100] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.31250000


[batch = 5200] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.31250000


[batch = 5300] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.31250000


[batch = 5400] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.31250000


[batch = 5500] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.31250000


[batch = 5600] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.31250000


[batch = 5700] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.31250000


[batch = 5800] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.31250000


[batch = 5900] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.31250000


[batch = 6000] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.31250000


[batch = 6100] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.31250000


[batch = 6200] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.31250000


[batch = 6300] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.31250000


[batch = 6400] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.31250000


[batch = 6500] train_perplexity =     3.35, train_loss =  1.21, learning_rate = 0.31250000


[batch = 6600] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.31250000


[batch = 6700] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.31250000


[batch = 6800] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.31250000


[batch = 6900] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.31250000


[epoch =  57] validation_perplexity =     3.58, validation_loss =  1.28

Saved model to 'e200/ptb_char_lstm.pt'...
[epoch =  57] min_validation_perplexity =     3.58, min_validation_loss =  1.28


[batch =  100] train_perplexity =     3.72, train_loss =  1.31, learning_rate = 0.31250000


[batch =  200] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.31250000


[batch =  300] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.31250000


[batch =  400] train_perplexity =     3.35, train_loss =  1.21, learning_rate = 0.31250000


[batch =  500] train_perplexity =     3.35, train_loss =  1.21, learning_rate = 0.31250000


[batch =  600] train_perplexity =     3.31, train_loss =  1.20, learning_rate = 0.31250000


[batch =  700] train_perplexity =     3.34, train_loss =  1.21, learning_rate = 0.31250000


[batch =  800] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.31250000


[batch =  900] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.31250000


[batch = 1000] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.31250000


[batch = 1100] train_perplexity =     3.59, train_loss =  1.28, learning_rate = 0.31250000


[batch = 1200] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.31250000


[batch = 1300] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.31250000


[batch = 1400] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.31250000


[batch = 1500] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.31250000


[batch = 1600] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.31250000


[batch = 1700] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.31250000


[batch = 1800] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.31250000


[batch = 1900] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.31250000


[batch = 2000] train_perplexity =     3.56, train_loss =  1.27, learning_rate = 0.31250000


[batch = 2100] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.31250000


[batch = 2200] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.31250000


[batch = 2300] train_perplexity =     3.56, train_loss =  1.27, learning_rate = 0.31250000


[batch = 2400] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.31250000


[batch = 2500] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.31250000


[batch = 2600] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 0.31250000


[batch = 2700] train_perplexity =     3.56, train_loss =  1.27, learning_rate = 0.31250000


[batch = 2800] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.31250000


[batch = 2900] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.31250000


[batch = 3000] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.31250000


[batch = 3100] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.31250000


[batch = 3200] train_perplexity =     3.61, train_loss =  1.28, learning_rate = 0.31250000


[batch = 3300] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.31250000


[batch = 3400] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.31250000


[batch = 3500] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.31250000


[batch = 3600] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.31250000


[batch = 3700] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.31250000


[batch = 3800] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.31250000


[batch = 3900] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.31250000


[batch = 4000] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.31250000


[batch = 4100] train_perplexity =     3.36, train_loss =  1.21, learning_rate = 0.31250000


[batch = 4200] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.31250000


[batch = 4300] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.31250000


[batch = 4400] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.31250000


[batch = 4500] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.31250000


[batch = 4600] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.31250000


[batch = 4700] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.31250000


[batch = 4800] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.31250000


[batch = 4900] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.31250000


[batch = 5000] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.31250000


[batch = 5100] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.31250000


[batch = 5200] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.31250000


[batch = 5300] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.31250000


[batch = 5400] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.31250000


[batch = 5500] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.31250000


[batch = 5600] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.31250000


[batch = 5700] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.31250000


[batch = 5800] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.31250000


[batch = 5900] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.31250000


[batch = 6000] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.31250000


[batch = 6100] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.31250000


[batch = 6200] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.31250000


[batch = 6300] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.31250000


[batch = 6400] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.31250000


[batch = 6500] train_perplexity =     3.35, train_loss =  1.21, learning_rate = 0.31250000


[batch = 6600] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.31250000


[batch = 6700] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.31250000


[batch = 6800] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.31250000


[batch = 6900] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.31250000


[epoch =  58] validation_perplexity =     3.58, validation_loss =  1.28

Saved model to 'e200/ptb_char_lstm.pt'...
[epoch =  58] min_validation_perplexity =     3.58, min_validation_loss =  1.28


[batch =  100] train_perplexity =     3.72, train_loss =  1.31, learning_rate = 0.31250000


[batch =  200] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.31250000


[batch =  300] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.31250000


[batch =  400] train_perplexity =     3.35, train_loss =  1.21, learning_rate = 0.31250000


[batch =  500] train_perplexity =     3.35, train_loss =  1.21, learning_rate = 0.31250000


[batch =  600] train_perplexity =     3.31, train_loss =  1.20, learning_rate = 0.31250000


[batch =  700] train_perplexity =     3.34, train_loss =  1.21, learning_rate = 0.31250000


[batch =  800] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.31250000


[batch =  900] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.31250000


[batch = 1000] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.31250000


[batch = 1100] train_perplexity =     3.59, train_loss =  1.28, learning_rate = 0.31250000


[batch = 1200] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.31250000


[batch = 1300] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.31250000


[batch = 1400] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.31250000


[batch = 1500] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.31250000


[batch = 1600] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.31250000


[batch = 1700] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.31250000


[batch = 1800] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.31250000


[batch = 1900] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.31250000


[batch = 2000] train_perplexity =     3.56, train_loss =  1.27, learning_rate = 0.31250000


[batch = 2100] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.31250000


[batch = 2200] train_perplexity =     3.47, train_loss =  1.25, learning_rate = 0.31250000


[batch = 2300] train_perplexity =     3.56, train_loss =  1.27, learning_rate = 0.31250000


[batch = 2400] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.31250000


[batch = 2500] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.31250000


[batch = 2600] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 0.31250000


[batch = 2700] train_perplexity =     3.56, train_loss =  1.27, learning_rate = 0.31250000


[batch = 2800] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.31250000


[batch = 2900] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.31250000


[batch = 3000] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.31250000


[batch = 3100] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.31250000


[batch = 3200] train_perplexity =     3.61, train_loss =  1.28, learning_rate = 0.31250000


[batch = 3300] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.31250000


[batch = 3400] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.31250000


[batch = 3500] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.31250000


[batch = 3600] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.31250000


[batch = 3700] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.31250000


[batch = 3800] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.31250000


[batch = 3900] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.31250000


[batch = 4000] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.31250000


[batch = 4100] train_perplexity =     3.36, train_loss =  1.21, learning_rate = 0.31250000


[batch = 4200] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.31250000


[batch = 4300] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.31250000


[batch = 4400] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.31250000


[batch = 4500] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.31250000


[batch = 4600] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.31250000


[batch = 4700] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.31250000


[batch = 4800] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.31250000


[batch = 4900] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.31250000


[batch = 5000] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.31250000


[batch = 5100] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.31250000


[batch = 5200] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.31250000


[batch = 5300] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.31250000


[batch = 5400] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.31250000


[batch = 5500] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.31250000


[batch = 5600] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.31250000


[batch = 5700] train_perplexity =     3.32, train_loss =  1.20, learning_rate = 0.31250000


[batch = 5800] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.31250000


[batch = 5900] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.31250000


[batch = 6000] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.31250000


[batch = 6100] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.31250000


[batch = 6200] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.31250000


[batch = 6300] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.31250000


[batch = 6400] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.31250000


[batch = 6500] train_perplexity =     3.35, train_loss =  1.21, learning_rate = 0.31250000


[batch = 6600] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.31250000


[batch = 6700] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.31250000


[batch = 6800] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.31250000


[batch = 6900] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.31250000


[epoch =  59] validation_perplexity =     3.58, validation_loss =  1.28

Saved model to 'e200/ptb_char_lstm.pt'...
[epoch =  59] min_validation_perplexity =     3.58, min_validation_loss =  1.28


[batch =  100] train_perplexity =     3.72, train_loss =  1.31, learning_rate = 0.31250000


[batch =  200] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.31250000


[batch =  300] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.31250000


[batch =  400] train_perplexity =     3.35, train_loss =  1.21, learning_rate = 0.31250000


[batch =  500] train_perplexity =     3.35, train_loss =  1.21, learning_rate = 0.31250000


[batch =  600] train_perplexity =     3.31, train_loss =  1.20, learning_rate = 0.31250000


[batch =  700] train_perplexity =     3.34, train_loss =  1.21, learning_rate = 0.31250000


[batch =  800] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.31250000


[batch =  900] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.31250000


[batch = 1000] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.31250000


[batch = 1100] train_perplexity =     3.59, train_loss =  1.28, learning_rate = 0.31250000


[batch = 1200] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.31250000


[batch = 1300] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.31250000


[batch = 1400] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.31250000


[batch = 1500] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.31250000


[batch = 1600] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.31250000


[batch = 1700] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.31250000


[batch = 1800] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.31250000


[batch = 1900] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.31250000


[batch = 2000] train_perplexity =     3.56, train_loss =  1.27, learning_rate = 0.31250000


[batch = 2100] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.31250000


[batch = 2200] train_perplexity =     3.47, train_loss =  1.25, learning_rate = 0.31250000


[batch = 2300] train_perplexity =     3.56, train_loss =  1.27, learning_rate = 0.31250000


[batch = 2400] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.31250000


[batch = 2500] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.31250000


[batch = 2600] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 0.31250000


[batch = 2700] train_perplexity =     3.56, train_loss =  1.27, learning_rate = 0.31250000


[batch = 2800] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.31250000


[batch = 2900] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.31250000


[batch = 3000] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.31250000


[batch = 3100] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.31250000


[batch = 3200] train_perplexity =     3.61, train_loss =  1.28, learning_rate = 0.31250000


[batch = 3300] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.31250000


[batch = 3400] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.31250000


[batch = 3500] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.31250000


[batch = 3600] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.31250000


[batch = 3700] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.31250000


[batch = 3800] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.31250000


[batch = 3900] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.31250000


[batch = 4000] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.31250000


[batch = 4100] train_perplexity =     3.36, train_loss =  1.21, learning_rate = 0.31250000


[batch = 4200] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.31250000


[batch = 4300] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.31250000


[batch = 4400] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.31250000


[batch = 4500] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.31250000


[batch = 4600] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.31250000


[batch = 4700] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.31250000


[batch = 4800] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.31250000


[batch = 4900] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.31250000


[batch = 5000] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.31250000


[batch = 5100] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.31250000


[batch = 5200] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.31250000


[batch = 5300] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.31250000


[batch = 5400] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.31250000


[batch = 5500] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.31250000


[batch = 5600] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.31250000


[batch = 5700] train_perplexity =     3.32, train_loss =  1.20, learning_rate = 0.31250000


[batch = 5800] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.31250000


[batch = 5900] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.31250000


[batch = 6000] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.31250000


[batch = 6100] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.31250000


[batch = 6200] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.31250000


[batch = 6300] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.31250000


[batch = 6400] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.31250000


[batch = 6500] train_perplexity =     3.35, train_loss =  1.21, learning_rate = 0.31250000


[batch = 6600] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.31250000


[batch = 6700] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.31250000


[batch = 6800] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.31250000


[batch = 6900] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.31250000


[epoch =  60] validation_perplexity =     3.58, validation_loss =  1.28

Saved model to 'e200/ptb_char_lstm.pt'...
[epoch =  60] min_validation_perplexity =     3.58, min_validation_loss =  1.28


[batch =  100] train_perplexity =     3.71, train_loss =  1.31, learning_rate = 0.31250000


[batch =  200] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.31250000


[batch =  300] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.31250000


[batch =  400] train_perplexity =     3.35, train_loss =  1.21, learning_rate = 0.31250000


[batch =  500] train_perplexity =     3.34, train_loss =  1.21, learning_rate = 0.31250000


[batch =  600] train_perplexity =     3.31, train_loss =  1.20, learning_rate = 0.31250000


[batch =  700] train_perplexity =     3.34, train_loss =  1.21, learning_rate = 0.31250000


[batch =  800] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.31250000


[batch =  900] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.31250000


[batch = 1000] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.31250000


[batch = 1100] train_perplexity =     3.59, train_loss =  1.28, learning_rate = 0.31250000


[batch = 1200] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.31250000


[batch = 1300] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.31250000


[batch = 1400] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.31250000


[batch = 1500] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.31250000


[batch = 1600] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.31250000


[batch = 1700] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.31250000


[batch = 1800] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.31250000


[batch = 1900] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.31250000


[batch = 2000] train_perplexity =     3.56, train_loss =  1.27, learning_rate = 0.31250000


[batch = 2100] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.31250000


[batch = 2200] train_perplexity =     3.47, train_loss =  1.25, learning_rate = 0.31250000


[batch = 2300] train_perplexity =     3.56, train_loss =  1.27, learning_rate = 0.31250000


[batch = 2400] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.31250000


[batch = 2500] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.31250000


[batch = 2600] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 0.31250000


[batch = 2700] train_perplexity =     3.56, train_loss =  1.27, learning_rate = 0.31250000


[batch = 2800] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.31250000


[batch = 2900] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.31250000


[batch = 3000] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.31250000


[batch = 3100] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.31250000


[batch = 3200] train_perplexity =     3.61, train_loss =  1.28, learning_rate = 0.31250000


[batch = 3300] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.31250000


[batch = 3400] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.31250000


[batch = 3500] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.31250000


[batch = 3600] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.31250000


[batch = 3700] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.31250000


[batch = 3800] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.31250000


[batch = 3900] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.31250000


[batch = 4000] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.31250000


[batch = 4100] train_perplexity =     3.35, train_loss =  1.21, learning_rate = 0.31250000


[batch = 4200] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.31250000


[batch = 4300] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.31250000


[batch = 4400] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.31250000


[batch = 4500] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.31250000


[batch = 4600] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.31250000


[batch = 4700] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.31250000


[batch = 4800] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.31250000


[batch = 4900] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.31250000


[batch = 5000] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.31250000


[batch = 5100] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.31250000


[batch = 5200] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.31250000


[batch = 5300] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.31250000


[batch = 5400] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.31250000


[batch = 5500] train_perplexity =     3.51, train_loss =  1.25, learning_rate = 0.31250000


[batch = 5600] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.31250000


[batch = 5700] train_perplexity =     3.32, train_loss =  1.20, learning_rate = 0.31250000


[batch = 5800] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.31250000


[batch = 5900] train_perplexity =     3.51, train_loss =  1.25, learning_rate = 0.31250000


[batch = 6000] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.31250000


[batch = 6100] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.31250000


[batch = 6200] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.31250000


[batch = 6300] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.31250000


[batch = 6400] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.31250000


[batch = 6500] train_perplexity =     3.35, train_loss =  1.21, learning_rate = 0.31250000


[batch = 6600] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.31250000


[batch = 6700] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.31250000


[batch = 6800] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.31250000


[batch = 6900] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.31250000


[epoch =  61] validation_perplexity =     3.58, validation_loss =  1.28

Saved model to 'e200/ptb_char_lstm.pt'...
[epoch =  61] min_validation_perplexity =     3.58, min_validation_loss =  1.28


[batch =  100] train_perplexity =     3.71, train_loss =  1.31, learning_rate = 0.31250000


[batch =  200] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.31250000


[batch =  300] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.31250000


[batch =  400] train_perplexity =     3.35, train_loss =  1.21, learning_rate = 0.31250000


[batch =  500] train_perplexity =     3.34, train_loss =  1.21, learning_rate = 0.31250000


[batch =  600] train_perplexity =     3.31, train_loss =  1.20, learning_rate = 0.31250000


[batch =  700] train_perplexity =     3.34, train_loss =  1.21, learning_rate = 0.31250000


[batch =  800] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.31250000


[batch =  900] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.31250000


[batch = 1000] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.31250000


[batch = 1100] train_perplexity =     3.59, train_loss =  1.28, learning_rate = 0.31250000


[batch = 1200] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.31250000


[batch = 1300] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.31250000


[batch = 1400] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.31250000


[batch = 1500] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.31250000


[batch = 1600] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.31250000


[batch = 1700] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.31250000


[batch = 1800] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.31250000


[batch = 1900] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.31250000


[batch = 2000] train_perplexity =     3.56, train_loss =  1.27, learning_rate = 0.31250000


[batch = 2100] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.31250000


[batch = 2200] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.31250000


[batch = 2300] train_perplexity =     3.56, train_loss =  1.27, learning_rate = 0.31250000


[batch = 2400] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.31250000


[batch = 2500] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.31250000


[batch = 2600] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 0.31250000


[batch = 2700] train_perplexity =     3.56, train_loss =  1.27, learning_rate = 0.31250000


[batch = 2800] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.31250000


[batch = 2900] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.31250000


[batch = 3000] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.31250000


[batch = 3100] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.31250000


[batch = 3200] train_perplexity =     3.61, train_loss =  1.28, learning_rate = 0.31250000


[batch = 3300] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.31250000


[batch = 3400] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.31250000


[batch = 3500] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.31250000


[batch = 3600] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.31250000


[batch = 3700] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.31250000


[batch = 3800] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.31250000


[batch = 3900] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.31250000


[batch = 4000] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.31250000


[batch = 4100] train_perplexity =     3.35, train_loss =  1.21, learning_rate = 0.31250000


[batch = 4200] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.31250000


[batch = 4300] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.31250000


[batch = 4400] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.31250000


[batch = 4500] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.31250000


[batch = 4600] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.31250000


[batch = 4700] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.31250000


[batch = 4800] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.31250000


[batch = 4900] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.31250000


[batch = 5000] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.31250000


[batch = 5100] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.31250000


[batch = 5200] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.31250000


[batch = 5300] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.31250000


[batch = 5400] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.31250000


[batch = 5500] train_perplexity =     3.51, train_loss =  1.25, learning_rate = 0.31250000


[batch = 5600] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.31250000


[batch = 5700] train_perplexity =     3.32, train_loss =  1.20, learning_rate = 0.31250000


[batch = 5800] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.31250000


[batch = 5900] train_perplexity =     3.51, train_loss =  1.25, learning_rate = 0.31250000


[batch = 6000] train_perplexity =     3.47, train_loss =  1.25, learning_rate = 0.31250000


[batch = 6100] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.31250000


[batch = 6200] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.31250000


[batch = 6300] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.31250000


[batch = 6400] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.31250000


[batch = 6500] train_perplexity =     3.35, train_loss =  1.21, learning_rate = 0.31250000


[batch = 6600] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.31250000


[batch = 6700] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.31250000


[batch = 6800] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.31250000


[batch = 6900] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.31250000


[epoch =  62] validation_perplexity =     3.58, validation_loss =  1.28

Saved model to 'e200/ptb_char_lstm.pt'...
[epoch =  62] min_validation_perplexity =     3.58, min_validation_loss =  1.28


[batch =  100] train_perplexity =     3.71, train_loss =  1.31, learning_rate = 0.31250000


[batch =  200] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.31250000


[batch =  300] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.31250000


[batch =  400] train_perplexity =     3.35, train_loss =  1.21, learning_rate = 0.31250000


[batch =  500] train_perplexity =     3.34, train_loss =  1.21, learning_rate = 0.31250000


[batch =  600] train_perplexity =     3.31, train_loss =  1.20, learning_rate = 0.31250000


[batch =  700] train_perplexity =     3.34, train_loss =  1.21, learning_rate = 0.31250000


[batch =  800] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.31250000


[batch =  900] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.31250000


[batch = 1000] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.31250000


[batch = 1100] train_perplexity =     3.59, train_loss =  1.28, learning_rate = 0.31250000


[batch = 1200] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.31250000


[batch = 1300] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.31250000


[batch = 1400] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.31250000


[batch = 1500] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.31250000


[batch = 1600] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.31250000


[batch = 1700] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.31250000


[batch = 1800] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.31250000


[batch = 1900] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.31250000


[batch = 2000] train_perplexity =     3.56, train_loss =  1.27, learning_rate = 0.31250000


[batch = 2100] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.31250000


[batch = 2200] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.31250000


[batch = 2300] train_perplexity =     3.56, train_loss =  1.27, learning_rate = 0.31250000


[batch = 2400] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.31250000


[batch = 2500] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.31250000


[batch = 2600] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 0.31250000


[batch = 2700] train_perplexity =     3.56, train_loss =  1.27, learning_rate = 0.31250000


[batch = 2800] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.31250000


[batch = 2900] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.31250000


[batch = 3000] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.31250000


[batch = 3100] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.31250000


[batch = 3200] train_perplexity =     3.61, train_loss =  1.28, learning_rate = 0.31250000


[batch = 3300] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.31250000


[batch = 3400] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.31250000


[batch = 3500] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.31250000


[batch = 3600] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.31250000


[batch = 3700] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.31250000


[batch = 3800] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.31250000


[batch = 3900] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.31250000


[batch = 4000] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.31250000


[batch = 4100] train_perplexity =     3.35, train_loss =  1.21, learning_rate = 0.31250000


[batch = 4200] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.31250000


[batch = 4300] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.31250000


[batch = 4400] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.31250000


[batch = 4500] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.31250000


[batch = 4600] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.31250000


[batch = 4700] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.31250000


[batch = 4800] train_perplexity =     3.51, train_loss =  1.25, learning_rate = 0.31250000


[batch = 4900] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.31250000


[batch = 5000] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.31250000


[batch = 5100] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.31250000


[batch = 5200] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.31250000


[batch = 5300] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.31250000


[batch = 5400] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.31250000


[batch = 5500] train_perplexity =     3.51, train_loss =  1.25, learning_rate = 0.31250000


[batch = 5600] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.31250000


[batch = 5700] train_perplexity =     3.32, train_loss =  1.20, learning_rate = 0.31250000


[batch = 5800] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.31250000


[batch = 5900] train_perplexity =     3.51, train_loss =  1.25, learning_rate = 0.31250000


[batch = 6000] train_perplexity =     3.47, train_loss =  1.25, learning_rate = 0.31250000


[batch = 6100] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.31250000


[batch = 6200] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.31250000


[batch = 6300] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.31250000


[batch = 6400] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.31250000


[batch = 6500] train_perplexity =     3.35, train_loss =  1.21, learning_rate = 0.31250000


[batch = 6600] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.31250000


[batch = 6700] train_perplexity =     3.40, train_loss =  1.23, learning_rate = 0.31250000


[batch = 6800] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.31250000


[batch = 6900] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.31250000


[epoch =  63] validation_perplexity =     3.58, validation_loss =  1.27

Saved model to 'e200/ptb_char_lstm.pt'...
[epoch =  63] min_validation_perplexity =     3.58, min_validation_loss =  1.27


[batch =  100] train_perplexity =     3.71, train_loss =  1.31, learning_rate = 0.31250000


[batch =  200] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.31250000


[batch =  300] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.31250000


[batch =  400] train_perplexity =     3.35, train_loss =  1.21, learning_rate = 0.31250000


[batch =  500] train_perplexity =     3.34, train_loss =  1.21, learning_rate = 0.31250000


[batch =  600] train_perplexity =     3.31, train_loss =  1.20, learning_rate = 0.31250000


[batch =  700] train_perplexity =     3.34, train_loss =  1.21, learning_rate = 0.31250000


[batch =  800] train_perplexity =     3.47, train_loss =  1.25, learning_rate = 0.31250000


[batch =  900] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.31250000


[batch = 1000] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.31250000


[batch = 1100] train_perplexity =     3.59, train_loss =  1.28, learning_rate = 0.31250000


[batch = 1200] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.31250000


[batch = 1300] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.31250000


[batch = 1400] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.31250000


[batch = 1500] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.31250000


[batch = 1600] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.31250000


[batch = 1700] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.31250000


[batch = 1800] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.31250000


[batch = 1900] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.31250000


[batch = 2000] train_perplexity =     3.56, train_loss =  1.27, learning_rate = 0.31250000


[batch = 2100] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.31250000


[batch = 2200] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.31250000


[batch = 2300] train_perplexity =     3.56, train_loss =  1.27, learning_rate = 0.31250000


[batch = 2400] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.31250000


[batch = 2500] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.31250000


[batch = 2600] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 0.31250000


[batch = 2700] train_perplexity =     3.56, train_loss =  1.27, learning_rate = 0.31250000


[batch = 2800] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.31250000


[batch = 2900] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.31250000


[batch = 3000] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.31250000


[batch = 3100] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.31250000


[batch = 3200] train_perplexity =     3.61, train_loss =  1.28, learning_rate = 0.31250000


[batch = 3300] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.31250000


[batch = 3400] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.31250000


[batch = 3500] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.31250000


[batch = 3600] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.31250000


[batch = 3700] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.31250000


[batch = 3800] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.31250000


[batch = 3900] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.31250000


[batch = 4000] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.31250000


[batch = 4100] train_perplexity =     3.35, train_loss =  1.21, learning_rate = 0.31250000


[batch = 4200] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.31250000


[batch = 4300] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.31250000


[batch = 4400] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.31250000


[batch = 4500] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.31250000


[batch = 4600] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.31250000


[batch = 4700] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.31250000


[batch = 4800] train_perplexity =     3.51, train_loss =  1.25, learning_rate = 0.31250000


[batch = 4900] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.31250000


[batch = 5000] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.31250000


[batch = 5100] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.31250000


[batch = 5200] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.31250000


[batch = 5300] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.31250000


[batch = 5400] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.31250000


[batch = 5500] train_perplexity =     3.51, train_loss =  1.25, learning_rate = 0.31250000


[batch = 5600] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.31250000


[batch = 5700] train_perplexity =     3.32, train_loss =  1.20, learning_rate = 0.31250000


[batch = 5800] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.31250000


[batch = 5900] train_perplexity =     3.51, train_loss =  1.25, learning_rate = 0.31250000


[batch = 6000] train_perplexity =     3.47, train_loss =  1.25, learning_rate = 0.31250000


[batch = 6100] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.31250000


[batch = 6200] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.31250000


[batch = 6300] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.31250000


[batch = 6400] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.31250000


[batch = 6500] train_perplexity =     3.35, train_loss =  1.21, learning_rate = 0.31250000


[batch = 6600] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.31250000


[batch = 6700] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.31250000


[batch = 6800] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.31250000


[batch = 6900] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.31250000


[epoch =  64] validation_perplexity =     3.58, validation_loss =  1.27

Saved model to 'e200/ptb_char_lstm.pt'...
[epoch =  64] min_validation_perplexity =     3.58, min_validation_loss =  1.27


[batch =  100] train_perplexity =     3.71, train_loss =  1.31, learning_rate = 0.31250000


[batch =  200] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.31250000


[batch =  300] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.31250000


[batch =  400] train_perplexity =     3.35, train_loss =  1.21, learning_rate = 0.31250000


[batch =  500] train_perplexity =     3.34, train_loss =  1.21, learning_rate = 0.31250000


[batch =  600] train_perplexity =     3.31, train_loss =  1.20, learning_rate = 0.31250000


[batch =  700] train_perplexity =     3.34, train_loss =  1.20, learning_rate = 0.31250000


[batch =  800] train_perplexity =     3.47, train_loss =  1.25, learning_rate = 0.31250000


[batch =  900] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.31250000


[batch = 1000] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.31250000


[batch = 1100] train_perplexity =     3.59, train_loss =  1.28, learning_rate = 0.31250000


[batch = 1200] train_perplexity =     3.47, train_loss =  1.25, learning_rate = 0.31250000


[batch = 1300] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.31250000


[batch = 1400] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.31250000


[batch = 1500] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.31250000


[batch = 1600] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.31250000


[batch = 1700] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.31250000


[batch = 1800] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.31250000


[batch = 1900] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.31250000


[batch = 2000] train_perplexity =     3.56, train_loss =  1.27, learning_rate = 0.31250000


[batch = 2100] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.31250000


[batch = 2200] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.31250000


[batch = 2300] train_perplexity =     3.56, train_loss =  1.27, learning_rate = 0.31250000


[batch = 2400] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.31250000


[batch = 2500] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.31250000


[batch = 2600] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 0.31250000


[batch = 2700] train_perplexity =     3.56, train_loss =  1.27, learning_rate = 0.31250000


[batch = 2800] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.31250000


[batch = 2900] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.31250000


[batch = 3000] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.31250000


[batch = 3100] train_perplexity =     3.51, train_loss =  1.25, learning_rate = 0.31250000


[batch = 3200] train_perplexity =     3.61, train_loss =  1.28, learning_rate = 0.31250000


[batch = 3300] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.31250000


[batch = 3400] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.31250000


[batch = 3500] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.31250000


[batch = 3600] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.31250000


[batch = 3700] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.31250000


[batch = 3800] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.31250000


[batch = 3900] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.31250000


[batch = 4000] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.31250000


[batch = 4100] train_perplexity =     3.35, train_loss =  1.21, learning_rate = 0.31250000


[batch = 4200] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.31250000


[batch = 4300] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.31250000


[batch = 4400] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.31250000


[batch = 4500] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.31250000


[batch = 4600] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.31250000


[batch = 4700] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.31250000


[batch = 4800] train_perplexity =     3.51, train_loss =  1.25, learning_rate = 0.31250000


[batch = 4900] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.31250000


[batch = 5000] train_perplexity =     3.47, train_loss =  1.25, learning_rate = 0.31250000


[batch = 5100] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.31250000


[batch = 5200] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.31250000


[batch = 5300] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.31250000


[batch = 5400] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.31250000


[batch = 5500] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.31250000


[batch = 5600] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.31250000


[batch = 5700] train_perplexity =     3.32, train_loss =  1.20, learning_rate = 0.31250000


[batch = 5800] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.31250000


[batch = 5900] train_perplexity =     3.51, train_loss =  1.25, learning_rate = 0.31250000


[batch = 6000] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.31250000


[batch = 6100] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.31250000


[batch = 6200] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.31250000


[batch = 6300] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.31250000


[batch = 6400] train_perplexity =     3.40, train_loss =  1.23, learning_rate = 0.31250000


[batch = 6500] train_perplexity =     3.35, train_loss =  1.21, learning_rate = 0.31250000


[batch = 6600] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.31250000


[batch = 6700] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.31250000


[batch = 6800] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.31250000


[batch = 6900] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.31250000


[epoch =  65] validation_perplexity =     3.58, validation_loss =  1.27

[epoch =  65] annealing learning_rate = 0.07812500

[batch =  100] train_perplexity =     3.72, train_loss =  1.31, learning_rate = 0.07812500


[batch =  200] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.07812500


[batch =  300] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch =  400] train_perplexity =     3.35, train_loss =  1.21, learning_rate = 0.07812500


[batch =  500] train_perplexity =     3.34, train_loss =  1.21, learning_rate = 0.07812500


[batch =  600] train_perplexity =     3.31, train_loss =  1.20, learning_rate = 0.07812500


[batch =  700] train_perplexity =     3.34, train_loss =  1.21, learning_rate = 0.07812500


[batch =  800] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.07812500


[batch =  900] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1000] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.07812500


[batch = 1100] train_perplexity =     3.59, train_loss =  1.28, learning_rate = 0.07812500


[batch = 1200] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.07812500


[batch = 1300] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.07812500


[batch = 1400] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 1500] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 1600] train_perplexity =     3.40, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1700] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1800] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 1900] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2000] train_perplexity =     3.56, train_loss =  1.27, learning_rate = 0.07812500


[batch = 2100] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2200] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2300] train_perplexity =     3.56, train_loss =  1.27, learning_rate = 0.07812500


[batch = 2400] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2500] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.07812500


[batch = 2600] train_perplexity =     3.54, train_loss =  1.27, learning_rate = 0.07812500


[batch = 2700] train_perplexity =     3.56, train_loss =  1.27, learning_rate = 0.07812500


[batch = 2800] train_perplexity =     3.47, train_loss =  1.25, learning_rate = 0.07812500


[batch = 2900] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3000] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 3100] train_perplexity =     3.51, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3200] train_perplexity =     3.60, train_loss =  1.28, learning_rate = 0.07812500


[batch = 3300] train_perplexity =     3.51, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3400] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3500] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch = 3600] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch = 3700] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3800] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3900] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4000] train_perplexity =     3.47, train_loss =  1.25, learning_rate = 0.07812500


[batch = 4100] train_perplexity =     3.35, train_loss =  1.21, learning_rate = 0.07812500


[batch = 4200] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4300] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4400] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4500] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4600] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4700] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4800] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 4900] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5000] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 5100] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5200] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5300] train_perplexity =     3.51, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5400] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 5500] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5600] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5700] train_perplexity =     3.32, train_loss =  1.20, learning_rate = 0.07812500


[batch = 5800] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5900] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 6000] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 6100] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6200] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6300] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6400] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6500] train_perplexity =     3.34, train_loss =  1.21, learning_rate = 0.07812500


[batch = 6600] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6700] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6800] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 6900] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.07812500


[epoch =  66] validation_perplexity =     3.56, validation_loss =  1.27

Saved model to 'e200/ptb_char_lstm.pt'...
[epoch =  66] min_validation_perplexity =     3.56, min_validation_loss =  1.27


[batch =  100] train_perplexity =     3.71, train_loss =  1.31, learning_rate = 0.07812500


[batch =  200] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.07812500


[batch =  300] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch =  400] train_perplexity =     3.35, train_loss =  1.21, learning_rate = 0.07812500


[batch =  500] train_perplexity =     3.34, train_loss =  1.21, learning_rate = 0.07812500


[batch =  600] train_perplexity =     3.31, train_loss =  1.20, learning_rate = 0.07812500


[batch =  700] train_perplexity =     3.34, train_loss =  1.20, learning_rate = 0.07812500


[batch =  800] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch =  900] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1000] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.07812500


[batch = 1100] train_perplexity =     3.59, train_loss =  1.28, learning_rate = 0.07812500


[batch = 1200] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 1300] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.07812500


[batch = 1400] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1500] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 1600] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 1700] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1800] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 1900] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2000] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 0.07812500


[batch = 2100] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2200] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2300] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 0.07812500


[batch = 2400] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2500] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.07812500


[batch = 2600] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2700] train_perplexity =     3.56, train_loss =  1.27, learning_rate = 0.07812500


[batch = 2800] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2900] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3000] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 3100] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3200] train_perplexity =     3.60, train_loss =  1.28, learning_rate = 0.07812500


[batch = 3300] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3400] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3500] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch = 3600] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch = 3700] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3800] train_perplexity =     3.47, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3900] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4000] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4100] train_perplexity =     3.34, train_loss =  1.21, learning_rate = 0.07812500


[batch = 4200] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4300] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4400] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4500] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4600] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4700] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4800] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 4900] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5000] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 5100] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5200] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5300] train_perplexity =     3.51, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5400] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5500] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5600] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5700] train_perplexity =     3.32, train_loss =  1.20, learning_rate = 0.07812500


[batch = 5800] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5900] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 6000] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 6100] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6200] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6300] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6400] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6500] train_perplexity =     3.34, train_loss =  1.21, learning_rate = 0.07812500


[batch = 6600] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6700] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6800] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 6900] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.07812500


[epoch =  67] validation_perplexity =     3.56, validation_loss =  1.27

Saved model to 'e200/ptb_char_lstm.pt'...
[epoch =  67] min_validation_perplexity =     3.56, min_validation_loss =  1.27


[batch =  100] train_perplexity =     3.71, train_loss =  1.31, learning_rate = 0.07812500


[batch =  200] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.07812500


[batch =  300] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch =  400] train_perplexity =     3.35, train_loss =  1.21, learning_rate = 0.07812500


[batch =  500] train_perplexity =     3.34, train_loss =  1.21, learning_rate = 0.07812500


[batch =  600] train_perplexity =     3.31, train_loss =  1.20, learning_rate = 0.07812500


[batch =  700] train_perplexity =     3.34, train_loss =  1.20, learning_rate = 0.07812500


[batch =  800] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch =  900] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1000] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.07812500


[batch = 1100] train_perplexity =     3.59, train_loss =  1.28, learning_rate = 0.07812500


[batch = 1200] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 1300] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.07812500


[batch = 1400] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1500] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 1600] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 1700] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1800] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 1900] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2000] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 0.07812500


[batch = 2100] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2200] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2300] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 0.07812500


[batch = 2400] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2500] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.07812500


[batch = 2600] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2700] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 0.07812500


[batch = 2800] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2900] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3000] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 3100] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3200] train_perplexity =     3.60, train_loss =  1.28, learning_rate = 0.07812500


[batch = 3300] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3400] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3500] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch = 3600] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch = 3700] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3800] train_perplexity =     3.47, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3900] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4000] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4100] train_perplexity =     3.34, train_loss =  1.21, learning_rate = 0.07812500


[batch = 4200] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4300] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4400] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4500] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4600] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4700] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4800] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 4900] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5000] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 5100] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5200] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5300] train_perplexity =     3.51, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5400] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5500] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5600] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5700] train_perplexity =     3.32, train_loss =  1.20, learning_rate = 0.07812500


[batch = 5800] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5900] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 6000] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 6100] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6200] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6300] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6400] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6500] train_perplexity =     3.34, train_loss =  1.21, learning_rate = 0.07812500


[batch = 6600] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6700] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6800] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 6900] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.07812500


[epoch =  68] validation_perplexity =     3.56, validation_loss =  1.27

Saved model to 'e200/ptb_char_lstm.pt'...
[epoch =  68] min_validation_perplexity =     3.56, min_validation_loss =  1.27


[batch =  100] train_perplexity =     3.71, train_loss =  1.31, learning_rate = 0.07812500


[batch =  200] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.07812500


[batch =  300] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch =  400] train_perplexity =     3.34, train_loss =  1.21, learning_rate = 0.07812500


[batch =  500] train_perplexity =     3.34, train_loss =  1.20, learning_rate = 0.07812500


[batch =  600] train_perplexity =     3.31, train_loss =  1.20, learning_rate = 0.07812500


[batch =  700] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch =  800] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch =  900] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1000] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.07812500


[batch = 1100] train_perplexity =     3.59, train_loss =  1.28, learning_rate = 0.07812500


[batch = 1200] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 1300] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.07812500


[batch = 1400] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1500] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 1600] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 1700] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1800] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 1900] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2000] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 0.07812500


[batch = 2100] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2200] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2300] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 0.07812500


[batch = 2400] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2500] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.07812500


[batch = 2600] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2700] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 0.07812500


[batch = 2800] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2900] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3000] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 3100] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3200] train_perplexity =     3.60, train_loss =  1.28, learning_rate = 0.07812500


[batch = 3300] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3400] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3500] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch = 3600] train_perplexity =     3.51, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3700] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3800] train_perplexity =     3.47, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3900] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4000] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4100] train_perplexity =     3.34, train_loss =  1.21, learning_rate = 0.07812500


[batch = 4200] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4300] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4400] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4500] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4600] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4700] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4800] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 4900] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5000] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 5100] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5200] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5300] train_perplexity =     3.51, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5400] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5500] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5600] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5700] train_perplexity =     3.32, train_loss =  1.20, learning_rate = 0.07812500


[batch = 5800] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5900] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 6000] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 6100] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6200] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6300] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6400] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6500] train_perplexity =     3.34, train_loss =  1.21, learning_rate = 0.07812500


[batch = 6600] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6700] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6800] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 6900] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.07812500


[epoch =  69] validation_perplexity =     3.56, validation_loss =  1.27

Saved model to 'e200/ptb_char_lstm.pt'...
[epoch =  69] min_validation_perplexity =     3.56, min_validation_loss =  1.27


[batch =  100] train_perplexity =     3.71, train_loss =  1.31, learning_rate = 0.07812500


[batch =  200] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.07812500


[batch =  300] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch =  400] train_perplexity =     3.34, train_loss =  1.21, learning_rate = 0.07812500


[batch =  500] train_perplexity =     3.34, train_loss =  1.20, learning_rate = 0.07812500


[batch =  600] train_perplexity =     3.30, train_loss =  1.20, learning_rate = 0.07812500


[batch =  700] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch =  800] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch =  900] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1000] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.07812500


[batch = 1100] train_perplexity =     3.59, train_loss =  1.28, learning_rate = 0.07812500


[batch = 1200] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 1300] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.07812500


[batch = 1400] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1500] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 1600] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 1700] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1800] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 1900] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2000] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 0.07812500


[batch = 2100] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2200] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2300] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 0.07812500


[batch = 2400] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2500] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.07812500


[batch = 2600] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2700] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 0.07812500


[batch = 2800] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2900] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3000] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 3100] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3200] train_perplexity =     3.60, train_loss =  1.28, learning_rate = 0.07812500


[batch = 3300] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3400] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3500] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch = 3600] train_perplexity =     3.51, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3700] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3800] train_perplexity =     3.47, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3900] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4000] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4100] train_perplexity =     3.34, train_loss =  1.21, learning_rate = 0.07812500


[batch = 4200] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4300] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4400] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4500] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4600] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4700] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4800] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 4900] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5000] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 5100] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5200] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5300] train_perplexity =     3.51, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5400] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5500] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5600] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5700] train_perplexity =     3.32, train_loss =  1.20, learning_rate = 0.07812500


[batch = 5800] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5900] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 6000] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 6100] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6200] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6300] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6400] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6500] train_perplexity =     3.34, train_loss =  1.21, learning_rate = 0.07812500


[batch = 6600] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6700] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6800] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 6900] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.07812500


[epoch =  70] validation_perplexity =     3.56, validation_loss =  1.27

Saved model to 'e200/ptb_char_lstm.pt'...
[epoch =  70] min_validation_perplexity =     3.56, min_validation_loss =  1.27


[batch =  100] train_perplexity =     3.71, train_loss =  1.31, learning_rate = 0.07812500


[batch =  200] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.07812500


[batch =  300] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch =  400] train_perplexity =     3.34, train_loss =  1.21, learning_rate = 0.07812500


[batch =  500] train_perplexity =     3.34, train_loss =  1.20, learning_rate = 0.07812500


[batch =  600] train_perplexity =     3.30, train_loss =  1.20, learning_rate = 0.07812500


[batch =  700] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch =  800] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch =  900] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1000] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch = 1100] train_perplexity =     3.59, train_loss =  1.28, learning_rate = 0.07812500


[batch = 1200] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 1300] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.07812500


[batch = 1400] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1500] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 1600] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 1700] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1800] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 1900] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2000] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 0.07812500


[batch = 2100] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2200] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2300] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 0.07812500


[batch = 2400] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2500] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.07812500


[batch = 2600] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2700] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 0.07812500


[batch = 2800] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2900] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3000] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 3100] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3200] train_perplexity =     3.60, train_loss =  1.28, learning_rate = 0.07812500


[batch = 3300] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3400] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3500] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch = 3600] train_perplexity =     3.51, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3700] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3800] train_perplexity =     3.47, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3900] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4000] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4100] train_perplexity =     3.34, train_loss =  1.21, learning_rate = 0.07812500


[batch = 4200] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4300] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4400] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4500] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4600] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4700] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4800] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 4900] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5000] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 5100] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5200] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5300] train_perplexity =     3.51, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5400] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5500] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5600] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5700] train_perplexity =     3.32, train_loss =  1.20, learning_rate = 0.07812500


[batch = 5800] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5900] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 6000] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 6100] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6200] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6300] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6400] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6500] train_perplexity =     3.34, train_loss =  1.21, learning_rate = 0.07812500


[batch = 6600] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6700] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6800] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 6900] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.07812500


[epoch =  71] validation_perplexity =     3.56, validation_loss =  1.27

Saved model to 'e200/ptb_char_lstm.pt'...
[epoch =  71] min_validation_perplexity =     3.56, min_validation_loss =  1.27


[batch =  100] train_perplexity =     3.71, train_loss =  1.31, learning_rate = 0.07812500


[batch =  200] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.07812500


[batch =  300] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch =  400] train_perplexity =     3.34, train_loss =  1.21, learning_rate = 0.07812500


[batch =  500] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch =  600] train_perplexity =     3.30, train_loss =  1.20, learning_rate = 0.07812500


[batch =  700] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch =  800] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch =  900] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1000] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch = 1100] train_perplexity =     3.59, train_loss =  1.28, learning_rate = 0.07812500


[batch = 1200] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 1300] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.07812500


[batch = 1400] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1500] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 1600] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 1700] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1800] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 1900] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2000] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 0.07812500


[batch = 2100] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2200] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2300] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 0.07812500


[batch = 2400] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2500] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.07812500


[batch = 2600] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2700] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 0.07812500


[batch = 2800] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2900] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3000] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 3100] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3200] train_perplexity =     3.60, train_loss =  1.28, learning_rate = 0.07812500


[batch = 3300] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3400] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3500] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch = 3600] train_perplexity =     3.51, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3700] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3800] train_perplexity =     3.47, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3900] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4000] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4100] train_perplexity =     3.34, train_loss =  1.21, learning_rate = 0.07812500


[batch = 4200] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4300] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4400] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4500] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4600] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4700] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4800] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 4900] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5000] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 5100] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5200] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5300] train_perplexity =     3.51, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5400] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5500] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5600] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5700] train_perplexity =     3.32, train_loss =  1.20, learning_rate = 0.07812500


[batch = 5800] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5900] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 6000] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 6100] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6200] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6300] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6400] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6500] train_perplexity =     3.34, train_loss =  1.21, learning_rate = 0.07812500


[batch = 6600] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6700] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6800] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 6900] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.07812500


[epoch =  72] validation_perplexity =     3.56, validation_loss =  1.27

Saved model to 'e200/ptb_char_lstm.pt'...
[epoch =  72] min_validation_perplexity =     3.56, min_validation_loss =  1.27


[batch =  100] train_perplexity =     3.71, train_loss =  1.31, learning_rate = 0.07812500


[batch =  200] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.07812500


[batch =  300] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch =  400] train_perplexity =     3.34, train_loss =  1.21, learning_rate = 0.07812500


[batch =  500] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch =  600] train_perplexity =     3.30, train_loss =  1.20, learning_rate = 0.07812500


[batch =  700] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch =  800] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch =  900] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1000] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch = 1100] train_perplexity =     3.58, train_loss =  1.28, learning_rate = 0.07812500


[batch = 1200] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 1300] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.07812500


[batch = 1400] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1500] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 1600] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 1700] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1800] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 1900] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2000] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 0.07812500


[batch = 2100] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2200] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2300] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 0.07812500


[batch = 2400] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2500] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.07812500


[batch = 2600] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2700] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 0.07812500


[batch = 2800] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2900] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3000] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 3100] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3200] train_perplexity =     3.60, train_loss =  1.28, learning_rate = 0.07812500


[batch = 3300] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3400] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3500] train_perplexity =     3.51, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3600] train_perplexity =     3.51, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3700] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3800] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3900] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4000] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4100] train_perplexity =     3.34, train_loss =  1.21, learning_rate = 0.07812500


[batch = 4200] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4300] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4400] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4500] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4600] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4700] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4800] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 4900] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5000] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 5100] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5200] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5300] train_perplexity =     3.51, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5400] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5500] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5600] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5700] train_perplexity =     3.31, train_loss =  1.20, learning_rate = 0.07812500


[batch = 5800] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5900] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 6000] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 6100] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6200] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6300] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6400] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6500] train_perplexity =     3.34, train_loss =  1.21, learning_rate = 0.07812500


[batch = 6600] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6700] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6800] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 6900] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.07812500


[epoch =  73] validation_perplexity =     3.56, validation_loss =  1.27

Saved model to 'e200/ptb_char_lstm.pt'...
[epoch =  73] min_validation_perplexity =     3.56, min_validation_loss =  1.27


[batch =  100] train_perplexity =     3.70, train_loss =  1.31, learning_rate = 0.07812500


[batch =  200] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.07812500


[batch =  300] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch =  400] train_perplexity =     3.34, train_loss =  1.21, learning_rate = 0.07812500


[batch =  500] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch =  600] train_perplexity =     3.30, train_loss =  1.19, learning_rate = 0.07812500


[batch =  700] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch =  800] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch =  900] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1000] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch = 1100] train_perplexity =     3.58, train_loss =  1.28, learning_rate = 0.07812500


[batch = 1200] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 1300] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch = 1400] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1500] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 1600] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 1700] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1800] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 1900] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2000] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 0.07812500


[batch = 2100] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2200] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2300] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 0.07812500


[batch = 2400] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2500] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.07812500


[batch = 2600] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2700] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 0.07812500


[batch = 2800] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2900] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3000] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 3100] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3200] train_perplexity =     3.60, train_loss =  1.28, learning_rate = 0.07812500


[batch = 3300] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3400] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3500] train_perplexity =     3.51, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3600] train_perplexity =     3.51, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3700] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3800] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3900] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4000] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4100] train_perplexity =     3.34, train_loss =  1.21, learning_rate = 0.07812500


[batch = 4200] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4300] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4400] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4500] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4600] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4700] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4800] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 4900] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5000] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 5100] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5200] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5300] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5400] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5500] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5600] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5700] train_perplexity =     3.31, train_loss =  1.20, learning_rate = 0.07812500


[batch = 5800] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5900] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 6000] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 6100] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6200] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6300] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6400] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6500] train_perplexity =     3.34, train_loss =  1.21, learning_rate = 0.07812500


[batch = 6600] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6700] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6800] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 6900] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.07812500


[epoch =  74] validation_perplexity =     3.56, validation_loss =  1.27

Saved model to 'e200/ptb_char_lstm.pt'...
[epoch =  74] min_validation_perplexity =     3.56, min_validation_loss =  1.27


[batch =  100] train_perplexity =     3.70, train_loss =  1.31, learning_rate = 0.07812500


[batch =  200] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.07812500


[batch =  300] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch =  400] train_perplexity =     3.34, train_loss =  1.21, learning_rate = 0.07812500


[batch =  500] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch =  600] train_perplexity =     3.30, train_loss =  1.19, learning_rate = 0.07812500


[batch =  700] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch =  800] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch =  900] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1000] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch = 1100] train_perplexity =     3.58, train_loss =  1.28, learning_rate = 0.07812500


[batch = 1200] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 1300] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch = 1400] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1500] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 1600] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 1700] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1800] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 1900] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2000] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 0.07812500


[batch = 2100] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2200] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2300] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 0.07812500


[batch = 2400] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2500] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.07812500


[batch = 2600] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2700] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 0.07812500


[batch = 2800] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2900] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3000] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 3100] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3200] train_perplexity =     3.60, train_loss =  1.28, learning_rate = 0.07812500


[batch = 3300] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3400] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3500] train_perplexity =     3.51, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3600] train_perplexity =     3.51, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3700] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3800] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3900] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4000] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4100] train_perplexity =     3.34, train_loss =  1.21, learning_rate = 0.07812500


[batch = 4200] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4300] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4400] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4500] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4600] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4700] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4800] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 4900] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5000] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 5100] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5200] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5300] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5400] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5500] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5600] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5700] train_perplexity =     3.31, train_loss =  1.20, learning_rate = 0.07812500


[batch = 5800] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5900] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 6000] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 6100] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6200] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6300] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6400] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6500] train_perplexity =     3.34, train_loss =  1.21, learning_rate = 0.07812500


[batch = 6600] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6700] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6800] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 6900] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.07812500


[epoch =  75] validation_perplexity =     3.56, validation_loss =  1.27

Saved model to 'e200/ptb_char_lstm.pt'...
[epoch =  75] min_validation_perplexity =     3.56, min_validation_loss =  1.27


[batch =  100] train_perplexity =     3.70, train_loss =  1.31, learning_rate = 0.07812500


[batch =  200] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.07812500


[batch =  300] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch =  400] train_perplexity =     3.34, train_loss =  1.21, learning_rate = 0.07812500


[batch =  500] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch =  600] train_perplexity =     3.30, train_loss =  1.19, learning_rate = 0.07812500


[batch =  700] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch =  800] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch =  900] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1000] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch = 1100] train_perplexity =     3.58, train_loss =  1.28, learning_rate = 0.07812500


[batch = 1200] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 1300] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch = 1400] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1500] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 1600] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 1700] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1800] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 1900] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2000] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 0.07812500


[batch = 2100] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2200] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2300] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 0.07812500


[batch = 2400] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2500] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.07812500


[batch = 2600] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2700] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 0.07812500


[batch = 2800] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2900] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3000] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 3100] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3200] train_perplexity =     3.60, train_loss =  1.28, learning_rate = 0.07812500


[batch = 3300] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3400] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3500] train_perplexity =     3.51, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3600] train_perplexity =     3.51, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3700] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3800] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3900] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4000] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4100] train_perplexity =     3.34, train_loss =  1.21, learning_rate = 0.07812500


[batch = 4200] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4300] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4400] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4500] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4600] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4700] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4800] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 4900] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5000] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 5100] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5200] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5300] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5400] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5500] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5600] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5700] train_perplexity =     3.31, train_loss =  1.20, learning_rate = 0.07812500


[batch = 5800] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5900] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 6000] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 6100] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6200] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6300] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6400] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6500] train_perplexity =     3.34, train_loss =  1.21, learning_rate = 0.07812500


[batch = 6600] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6700] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6800] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 6900] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.07812500


[epoch =  76] validation_perplexity =     3.56, validation_loss =  1.27

Saved model to 'e200/ptb_char_lstm.pt'...
[epoch =  76] min_validation_perplexity =     3.56, min_validation_loss =  1.27


[batch =  100] train_perplexity =     3.70, train_loss =  1.31, learning_rate = 0.07812500


[batch =  200] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.07812500


[batch =  300] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch =  400] train_perplexity =     3.34, train_loss =  1.21, learning_rate = 0.07812500


[batch =  500] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch =  600] train_perplexity =     3.30, train_loss =  1.19, learning_rate = 0.07812500


[batch =  700] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch =  800] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch =  900] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1000] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch = 1100] train_perplexity =     3.58, train_loss =  1.28, learning_rate = 0.07812500


[batch = 1200] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 1300] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch = 1400] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1500] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 1600] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 1700] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1800] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 1900] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2000] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 0.07812500


[batch = 2100] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2200] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2300] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 0.07812500


[batch = 2400] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2500] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.07812500


[batch = 2600] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2700] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 0.07812500


[batch = 2800] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2900] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3000] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 3100] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3200] train_perplexity =     3.60, train_loss =  1.28, learning_rate = 0.07812500


[batch = 3300] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3400] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3500] train_perplexity =     3.51, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3600] train_perplexity =     3.51, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3700] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3800] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3900] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4000] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4100] train_perplexity =     3.34, train_loss =  1.21, learning_rate = 0.07812500


[batch = 4200] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4300] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4400] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4500] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4600] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4700] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4800] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 4900] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5000] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 5100] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5200] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5300] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5400] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5500] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5600] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5700] train_perplexity =     3.31, train_loss =  1.20, learning_rate = 0.07812500


[batch = 5800] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5900] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 6000] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 6100] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6200] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6300] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6400] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6500] train_perplexity =     3.34, train_loss =  1.21, learning_rate = 0.07812500


[batch = 6600] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6700] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6800] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 6900] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.07812500


[epoch =  77] validation_perplexity =     3.56, validation_loss =  1.27

Saved model to 'e200/ptb_char_lstm.pt'...
[epoch =  77] min_validation_perplexity =     3.56, min_validation_loss =  1.27


[batch =  100] train_perplexity =     3.70, train_loss =  1.31, learning_rate = 0.07812500


[batch =  200] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.07812500


[batch =  300] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch =  400] train_perplexity =     3.34, train_loss =  1.21, learning_rate = 0.07812500


[batch =  500] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch =  600] train_perplexity =     3.30, train_loss =  1.19, learning_rate = 0.07812500


[batch =  700] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch =  800] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch =  900] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1000] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch = 1100] train_perplexity =     3.58, train_loss =  1.28, learning_rate = 0.07812500


[batch = 1200] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 1300] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch = 1400] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1500] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 1600] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 1700] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1800] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 1900] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2000] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 0.07812500


[batch = 2100] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2200] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2300] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 0.07812500


[batch = 2400] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2500] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.07812500


[batch = 2600] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2700] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 0.07812500


[batch = 2800] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2900] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3000] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 3100] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3200] train_perplexity =     3.60, train_loss =  1.28, learning_rate = 0.07812500


[batch = 3300] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3400] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3500] train_perplexity =     3.51, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3600] train_perplexity =     3.51, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3700] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3800] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3900] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4000] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4100] train_perplexity =     3.34, train_loss =  1.21, learning_rate = 0.07812500


[batch = 4200] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4300] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4400] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4500] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4600] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4700] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4800] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 4900] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5000] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 5100] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5200] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5300] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5400] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5500] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5600] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5700] train_perplexity =     3.31, train_loss =  1.20, learning_rate = 0.07812500


[batch = 5800] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5900] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 6000] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 6100] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6200] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6300] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6400] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6500] train_perplexity =     3.34, train_loss =  1.21, learning_rate = 0.07812500


[batch = 6600] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6700] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6800] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 6900] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.07812500


[epoch =  78] validation_perplexity =     3.56, validation_loss =  1.27

Saved model to 'e200/ptb_char_lstm.pt'...
[epoch =  78] min_validation_perplexity =     3.56, min_validation_loss =  1.27


[batch =  100] train_perplexity =     3.70, train_loss =  1.31, learning_rate = 0.07812500


[batch =  200] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.07812500


[batch =  300] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch =  400] train_perplexity =     3.34, train_loss =  1.21, learning_rate = 0.07812500


[batch =  500] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch =  600] train_perplexity =     3.30, train_loss =  1.19, learning_rate = 0.07812500


[batch =  700] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch =  800] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch =  900] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1000] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch = 1100] train_perplexity =     3.58, train_loss =  1.28, learning_rate = 0.07812500


[batch = 1200] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 1300] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch = 1400] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1500] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 1600] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 1700] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1800] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 1900] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2000] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 0.07812500


[batch = 2100] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2200] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2300] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 0.07812500


[batch = 2400] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2500] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.07812500


[batch = 2600] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2700] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 0.07812500


[batch = 2800] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2900] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3000] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 3100] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3200] train_perplexity =     3.60, train_loss =  1.28, learning_rate = 0.07812500


[batch = 3300] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3400] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3500] train_perplexity =     3.51, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3600] train_perplexity =     3.51, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3700] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3800] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3900] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4000] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4100] train_perplexity =     3.34, train_loss =  1.21, learning_rate = 0.07812500


[batch = 4200] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4300] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4400] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4500] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4600] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4700] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4800] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 4900] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5000] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 5100] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5200] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5300] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5400] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5500] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5600] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5700] train_perplexity =     3.31, train_loss =  1.20, learning_rate = 0.07812500


[batch = 5800] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5900] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 6000] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 6100] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6200] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6300] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6400] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6500] train_perplexity =     3.34, train_loss =  1.21, learning_rate = 0.07812500


[batch = 6600] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6700] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6800] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 6900] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.07812500


[epoch =  79] validation_perplexity =     3.56, validation_loss =  1.27

Saved model to 'e200/ptb_char_lstm.pt'...
[epoch =  79] min_validation_perplexity =     3.56, min_validation_loss =  1.27


[batch =  100] train_perplexity =     3.70, train_loss =  1.31, learning_rate = 0.07812500


[batch =  200] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.07812500


[batch =  300] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch =  400] train_perplexity =     3.34, train_loss =  1.21, learning_rate = 0.07812500


[batch =  500] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch =  600] train_perplexity =     3.30, train_loss =  1.19, learning_rate = 0.07812500


[batch =  700] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch =  800] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch =  900] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1000] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch = 1100] train_perplexity =     3.58, train_loss =  1.28, learning_rate = 0.07812500


[batch = 1200] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 1300] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch = 1400] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1500] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 1600] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 1700] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1800] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 1900] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2000] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 0.07812500


[batch = 2100] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2200] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2300] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 0.07812500


[batch = 2400] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2500] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.07812500


[batch = 2600] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2700] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 0.07812500


[batch = 2800] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2900] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3000] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 3100] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3200] train_perplexity =     3.60, train_loss =  1.28, learning_rate = 0.07812500


[batch = 3300] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3400] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3500] train_perplexity =     3.51, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3600] train_perplexity =     3.51, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3700] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3800] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3900] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4000] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4100] train_perplexity =     3.34, train_loss =  1.21, learning_rate = 0.07812500


[batch = 4200] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4300] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4400] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4500] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4600] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4700] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4800] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 4900] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5000] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 5100] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5200] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5300] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5400] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5500] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5600] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5700] train_perplexity =     3.31, train_loss =  1.20, learning_rate = 0.07812500


[batch = 5800] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5900] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 6000] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 6100] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6200] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6300] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6400] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6500] train_perplexity =     3.34, train_loss =  1.21, learning_rate = 0.07812500


[batch = 6600] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6700] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6800] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 6900] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.07812500


[epoch =  80] validation_perplexity =     3.56, validation_loss =  1.27

Saved model to 'e200/ptb_char_lstm.pt'...
[epoch =  80] min_validation_perplexity =     3.56, min_validation_loss =  1.27


[batch =  100] train_perplexity =     3.70, train_loss =  1.31, learning_rate = 0.07812500


[batch =  200] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.07812500


[batch =  300] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch =  400] train_perplexity =     3.34, train_loss =  1.21, learning_rate = 0.07812500


[batch =  500] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch =  600] train_perplexity =     3.30, train_loss =  1.19, learning_rate = 0.07812500


[batch =  700] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch =  800] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch =  900] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1000] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch = 1100] train_perplexity =     3.58, train_loss =  1.28, learning_rate = 0.07812500


[batch = 1200] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 1300] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch = 1400] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1500] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 1600] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 1700] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1800] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 1900] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2000] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 0.07812500


[batch = 2100] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2200] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2300] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 0.07812500


[batch = 2400] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2500] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.07812500


[batch = 2600] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2700] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 0.07812500


[batch = 2800] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2900] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3000] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 3100] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3200] train_perplexity =     3.60, train_loss =  1.28, learning_rate = 0.07812500


[batch = 3300] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3400] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3500] train_perplexity =     3.51, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3600] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3700] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3800] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3900] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4000] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4100] train_perplexity =     3.34, train_loss =  1.21, learning_rate = 0.07812500


[batch = 4200] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4300] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4400] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4500] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4600] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4700] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4800] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 4900] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5000] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 5100] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5200] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5300] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5400] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5500] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5600] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5700] train_perplexity =     3.31, train_loss =  1.20, learning_rate = 0.07812500


[batch = 5800] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5900] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 6000] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 6100] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6200] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6300] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6400] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6500] train_perplexity =     3.34, train_loss =  1.21, learning_rate = 0.07812500


[batch = 6600] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6700] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6800] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 6900] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.07812500


[epoch =  81] validation_perplexity =     3.56, validation_loss =  1.27

Saved model to 'e200/ptb_char_lstm.pt'...
[epoch =  81] min_validation_perplexity =     3.56, min_validation_loss =  1.27


[batch =  100] train_perplexity =     3.70, train_loss =  1.31, learning_rate = 0.07812500


[batch =  200] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.07812500


[batch =  300] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch =  400] train_perplexity =     3.34, train_loss =  1.21, learning_rate = 0.07812500


[batch =  500] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch =  600] train_perplexity =     3.30, train_loss =  1.19, learning_rate = 0.07812500


[batch =  700] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch =  800] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch =  900] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1000] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch = 1100] train_perplexity =     3.58, train_loss =  1.28, learning_rate = 0.07812500


[batch = 1200] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 1300] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch = 1400] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1500] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 1600] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 1700] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1800] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 1900] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2000] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 0.07812500


[batch = 2100] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2200] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2300] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 0.07812500


[batch = 2400] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2500] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.07812500


[batch = 2600] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2700] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 0.07812500


[batch = 2800] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2900] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3000] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 3100] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3200] train_perplexity =     3.60, train_loss =  1.28, learning_rate = 0.07812500


[batch = 3300] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3400] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3500] train_perplexity =     3.51, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3600] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3700] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3800] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3900] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4000] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4100] train_perplexity =     3.34, train_loss =  1.21, learning_rate = 0.07812500


[batch = 4200] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4300] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4400] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4500] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4600] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4700] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4800] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 4900] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5000] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 5100] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5200] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5300] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5400] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5500] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5600] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5700] train_perplexity =     3.31, train_loss =  1.20, learning_rate = 0.07812500


[batch = 5800] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5900] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 6000] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 6100] train_perplexity =     3.40, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6200] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6300] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6400] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6500] train_perplexity =     3.34, train_loss =  1.21, learning_rate = 0.07812500


[batch = 6600] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6700] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6800] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 6900] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.07812500


[epoch =  82] validation_perplexity =     3.56, validation_loss =  1.27

Saved model to 'e200/ptb_char_lstm.pt'...
[epoch =  82] min_validation_perplexity =     3.56, min_validation_loss =  1.27


[batch =  100] train_perplexity =     3.70, train_loss =  1.31, learning_rate = 0.07812500


[batch =  200] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.07812500


[batch =  300] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch =  400] train_perplexity =     3.34, train_loss =  1.21, learning_rate = 0.07812500


[batch =  500] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch =  600] train_perplexity =     3.30, train_loss =  1.19, learning_rate = 0.07812500


[batch =  700] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch =  800] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch =  900] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1000] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch = 1100] train_perplexity =     3.58, train_loss =  1.28, learning_rate = 0.07812500


[batch = 1200] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 1300] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch = 1400] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1500] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 1600] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 1700] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1800] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 1900] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2000] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 0.07812500


[batch = 2100] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2200] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2300] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 0.07812500


[batch = 2400] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2500] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.07812500


[batch = 2600] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2700] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 0.07812500


[batch = 2800] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2900] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3000] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 3100] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3200] train_perplexity =     3.60, train_loss =  1.28, learning_rate = 0.07812500


[batch = 3300] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3400] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3500] train_perplexity =     3.51, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3600] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3700] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3800] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3900] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4000] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4100] train_perplexity =     3.34, train_loss =  1.21, learning_rate = 0.07812500


[batch = 4200] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4300] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4400] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4500] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4600] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4700] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4800] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 4900] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5000] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 5100] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5200] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5300] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5400] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5500] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5600] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5700] train_perplexity =     3.31, train_loss =  1.20, learning_rate = 0.07812500


[batch = 5800] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5900] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 6000] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 6100] train_perplexity =     3.40, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6200] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6300] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6400] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6500] train_perplexity =     3.34, train_loss =  1.21, learning_rate = 0.07812500


[batch = 6600] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6700] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6800] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 6900] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.07812500


[epoch =  83] validation_perplexity =     3.56, validation_loss =  1.27

Saved model to 'e200/ptb_char_lstm.pt'...
[epoch =  83] min_validation_perplexity =     3.56, min_validation_loss =  1.27


[batch =  100] train_perplexity =     3.70, train_loss =  1.31, learning_rate = 0.07812500


[batch =  200] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.07812500


[batch =  300] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch =  400] train_perplexity =     3.34, train_loss =  1.21, learning_rate = 0.07812500


[batch =  500] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch =  600] train_perplexity =     3.30, train_loss =  1.19, learning_rate = 0.07812500


[batch =  700] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch =  800] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch =  900] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1000] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch = 1100] train_perplexity =     3.58, train_loss =  1.28, learning_rate = 0.07812500


[batch = 1200] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 1300] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch = 1400] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1500] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 1600] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 1700] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1800] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 1900] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2000] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 0.07812500


[batch = 2100] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2200] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2300] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 0.07812500


[batch = 2400] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2500] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.07812500


[batch = 2600] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2700] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 0.07812500


[batch = 2800] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2900] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3000] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 3100] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3200] train_perplexity =     3.60, train_loss =  1.28, learning_rate = 0.07812500


[batch = 3300] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3400] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3500] train_perplexity =     3.51, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3600] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3700] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3800] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3900] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4000] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4100] train_perplexity =     3.34, train_loss =  1.21, learning_rate = 0.07812500


[batch = 4200] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4300] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4400] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4500] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4600] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4700] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4800] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 4900] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5000] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 5100] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5200] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5300] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5400] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5500] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5600] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5700] train_perplexity =     3.31, train_loss =  1.20, learning_rate = 0.07812500


[batch = 5800] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5900] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 6000] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 6100] train_perplexity =     3.40, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6200] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6300] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6400] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6500] train_perplexity =     3.34, train_loss =  1.21, learning_rate = 0.07812500


[batch = 6600] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6700] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6800] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 6900] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.07812500


[epoch =  84] validation_perplexity =     3.56, validation_loss =  1.27

Saved model to 'e200/ptb_char_lstm.pt'...
[epoch =  84] min_validation_perplexity =     3.56, min_validation_loss =  1.27


[batch =  100] train_perplexity =     3.70, train_loss =  1.31, learning_rate = 0.07812500


[batch =  200] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch =  300] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch =  400] train_perplexity =     3.34, train_loss =  1.21, learning_rate = 0.07812500


[batch =  500] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch =  600] train_perplexity =     3.30, train_loss =  1.19, learning_rate = 0.07812500


[batch =  700] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch =  800] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch =  900] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1000] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch = 1100] train_perplexity =     3.58, train_loss =  1.28, learning_rate = 0.07812500


[batch = 1200] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 1300] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch = 1400] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1500] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 1600] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 1700] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1800] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 1900] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2000] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 0.07812500


[batch = 2100] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2200] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2300] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 0.07812500


[batch = 2400] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2500] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.07812500


[batch = 2600] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2700] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 0.07812500


[batch = 2800] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2900] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3000] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 3100] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3200] train_perplexity =     3.60, train_loss =  1.28, learning_rate = 0.07812500


[batch = 3300] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3400] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3500] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3600] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3700] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3800] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3900] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4000] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4100] train_perplexity =     3.34, train_loss =  1.21, learning_rate = 0.07812500


[batch = 4200] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4300] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4400] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4500] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4600] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4700] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4800] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 4900] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5000] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 5100] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5200] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5300] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5400] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5500] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5600] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5700] train_perplexity =     3.31, train_loss =  1.20, learning_rate = 0.07812500


[batch = 5800] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5900] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 6000] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 6100] train_perplexity =     3.40, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6200] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6300] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6400] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6500] train_perplexity =     3.34, train_loss =  1.21, learning_rate = 0.07812500


[batch = 6600] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6700] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6800] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 6900] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.07812500


[epoch =  85] validation_perplexity =     3.56, validation_loss =  1.27

Saved model to 'e200/ptb_char_lstm.pt'...
[epoch =  85] min_validation_perplexity =     3.56, min_validation_loss =  1.27


[batch =  100] train_perplexity =     3.70, train_loss =  1.31, learning_rate = 0.07812500


[batch =  200] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch =  300] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch =  400] train_perplexity =     3.34, train_loss =  1.21, learning_rate = 0.07812500


[batch =  500] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch =  600] train_perplexity =     3.30, train_loss =  1.19, learning_rate = 0.07812500


[batch =  700] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch =  800] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch =  900] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1000] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch = 1100] train_perplexity =     3.58, train_loss =  1.28, learning_rate = 0.07812500


[batch = 1200] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 1300] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch = 1400] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1500] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 1600] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 1700] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1800] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 1900] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2000] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 0.07812500


[batch = 2100] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2200] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2300] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 0.07812500


[batch = 2400] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2500] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.07812500


[batch = 2600] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2700] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 0.07812500


[batch = 2800] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2900] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3000] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 3100] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3200] train_perplexity =     3.60, train_loss =  1.28, learning_rate = 0.07812500


[batch = 3300] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3400] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3500] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3600] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3700] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3800] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3900] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4000] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4100] train_perplexity =     3.34, train_loss =  1.21, learning_rate = 0.07812500


[batch = 4200] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4300] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4400] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4500] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4600] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4700] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4800] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 4900] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5000] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 5100] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5200] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5300] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5400] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5500] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5600] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5700] train_perplexity =     3.31, train_loss =  1.20, learning_rate = 0.07812500


[batch = 5800] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5900] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 6000] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 6100] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6200] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6300] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6400] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6500] train_perplexity =     3.34, train_loss =  1.21, learning_rate = 0.07812500


[batch = 6600] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6700] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6800] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 6900] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.07812500


[epoch =  86] validation_perplexity =     3.56, validation_loss =  1.27

Saved model to 'e200/ptb_char_lstm.pt'...
[epoch =  86] min_validation_perplexity =     3.56, min_validation_loss =  1.27


[batch =  100] train_perplexity =     3.70, train_loss =  1.31, learning_rate = 0.07812500


[batch =  200] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch =  300] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch =  400] train_perplexity =     3.34, train_loss =  1.21, learning_rate = 0.07812500


[batch =  500] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch =  600] train_perplexity =     3.30, train_loss =  1.19, learning_rate = 0.07812500


[batch =  700] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch =  800] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch =  900] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1000] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch = 1100] train_perplexity =     3.58, train_loss =  1.28, learning_rate = 0.07812500


[batch = 1200] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 1300] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch = 1400] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1500] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 1600] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 1700] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1800] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 1900] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2000] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 0.07812500


[batch = 2100] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2200] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2300] train_perplexity =     3.54, train_loss =  1.27, learning_rate = 0.07812500


[batch = 2400] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2500] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.07812500


[batch = 2600] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2700] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 0.07812500


[batch = 2800] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2900] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3000] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 3100] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3200] train_perplexity =     3.60, train_loss =  1.28, learning_rate = 0.07812500


[batch = 3300] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3400] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3500] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3600] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3700] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3800] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3900] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4000] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4100] train_perplexity =     3.34, train_loss =  1.21, learning_rate = 0.07812500


[batch = 4200] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4300] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4400] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4500] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4600] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4700] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4800] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 4900] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5000] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 5100] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5200] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5300] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5400] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5500] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5600] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5700] train_perplexity =     3.31, train_loss =  1.20, learning_rate = 0.07812500


[batch = 5800] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5900] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 6000] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 6100] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6200] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6300] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6400] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6500] train_perplexity =     3.34, train_loss =  1.21, learning_rate = 0.07812500


[batch = 6600] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6700] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6800] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 6900] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.07812500


[epoch =  87] validation_perplexity =     3.56, validation_loss =  1.27

Saved model to 'e200/ptb_char_lstm.pt'...
[epoch =  87] min_validation_perplexity =     3.56, min_validation_loss =  1.27


[batch =  100] train_perplexity =     3.70, train_loss =  1.31, learning_rate = 0.07812500


[batch =  200] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch =  300] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch =  400] train_perplexity =     3.34, train_loss =  1.21, learning_rate = 0.07812500


[batch =  500] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch =  600] train_perplexity =     3.30, train_loss =  1.19, learning_rate = 0.07812500


[batch =  700] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch =  800] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch =  900] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1000] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch = 1100] train_perplexity =     3.58, train_loss =  1.28, learning_rate = 0.07812500


[batch = 1200] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 1300] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch = 1400] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1500] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 1600] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 1700] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1800] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 1900] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2000] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 0.07812500


[batch = 2100] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2200] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2300] train_perplexity =     3.54, train_loss =  1.27, learning_rate = 0.07812500


[batch = 2400] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2500] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.07812500


[batch = 2600] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2700] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 0.07812500


[batch = 2800] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2900] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3000] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 3100] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3200] train_perplexity =     3.60, train_loss =  1.28, learning_rate = 0.07812500


[batch = 3300] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3400] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3500] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3600] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3700] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3800] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3900] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4000] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4100] train_perplexity =     3.34, train_loss =  1.21, learning_rate = 0.07812500


[batch = 4200] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4300] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4400] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4500] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4600] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4700] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4800] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 4900] train_perplexity =     3.51, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5000] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 5100] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5200] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5300] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5400] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5500] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5600] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5700] train_perplexity =     3.31, train_loss =  1.20, learning_rate = 0.07812500


[batch = 5800] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5900] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 6000] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 6100] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6200] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6300] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6400] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6500] train_perplexity =     3.34, train_loss =  1.21, learning_rate = 0.07812500


[batch = 6600] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6700] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6800] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 6900] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.07812500


[epoch =  88] validation_perplexity =     3.56, validation_loss =  1.27

Saved model to 'e200/ptb_char_lstm.pt'...
[epoch =  88] min_validation_perplexity =     3.56, min_validation_loss =  1.27


[batch =  100] train_perplexity =     3.70, train_loss =  1.31, learning_rate = 0.07812500


[batch =  200] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch =  300] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch =  400] train_perplexity =     3.34, train_loss =  1.21, learning_rate = 0.07812500


[batch =  500] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch =  600] train_perplexity =     3.30, train_loss =  1.19, learning_rate = 0.07812500


[batch =  700] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch =  800] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch =  900] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1000] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch = 1100] train_perplexity =     3.58, train_loss =  1.28, learning_rate = 0.07812500


[batch = 1200] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 1300] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch = 1400] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1500] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 1600] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 1700] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1800] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1900] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2000] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 0.07812500


[batch = 2100] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2200] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2300] train_perplexity =     3.54, train_loss =  1.27, learning_rate = 0.07812500


[batch = 2400] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2500] train_perplexity =     3.47, train_loss =  1.25, learning_rate = 0.07812500


[batch = 2600] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2700] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 0.07812500


[batch = 2800] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2900] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3000] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 3100] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3200] train_perplexity =     3.60, train_loss =  1.28, learning_rate = 0.07812500


[batch = 3300] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3400] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3500] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3600] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3700] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3800] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3900] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4000] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4100] train_perplexity =     3.34, train_loss =  1.21, learning_rate = 0.07812500


[batch = 4200] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4300] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4400] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4500] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4600] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4700] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4800] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 4900] train_perplexity =     3.51, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5000] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 5100] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5200] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5300] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5400] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5500] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5600] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5700] train_perplexity =     3.31, train_loss =  1.20, learning_rate = 0.07812500


[batch = 5800] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5900] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 6000] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 6100] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6200] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6300] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6400] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6500] train_perplexity =     3.34, train_loss =  1.20, learning_rate = 0.07812500


[batch = 6600] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6700] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6800] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 6900] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.07812500


[epoch =  89] validation_perplexity =     3.56, validation_loss =  1.27

Saved model to 'e200/ptb_char_lstm.pt'...
[epoch =  89] min_validation_perplexity =     3.56, min_validation_loss =  1.27


[batch =  100] train_perplexity =     3.70, train_loss =  1.31, learning_rate = 0.07812500


[batch =  200] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch =  300] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch =  400] train_perplexity =     3.34, train_loss =  1.21, learning_rate = 0.07812500


[batch =  500] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch =  600] train_perplexity =     3.30, train_loss =  1.19, learning_rate = 0.07812500


[batch =  700] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch =  800] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch =  900] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1000] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch = 1100] train_perplexity =     3.58, train_loss =  1.28, learning_rate = 0.07812500


[batch = 1200] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 1300] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch = 1400] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1500] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 1600] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 1700] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1800] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1900] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2000] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 0.07812500


[batch = 2100] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2200] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2300] train_perplexity =     3.54, train_loss =  1.27, learning_rate = 0.07812500


[batch = 2400] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2500] train_perplexity =     3.47, train_loss =  1.25, learning_rate = 0.07812500


[batch = 2600] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2700] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 0.07812500


[batch = 2800] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2900] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3000] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 3100] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3200] train_perplexity =     3.60, train_loss =  1.28, learning_rate = 0.07812500


[batch = 3300] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3400] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3500] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3600] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3700] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3800] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3900] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4000] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4100] train_perplexity =     3.34, train_loss =  1.21, learning_rate = 0.07812500


[batch = 4200] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4300] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4400] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4500] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4600] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4700] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4800] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 4900] train_perplexity =     3.51, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5000] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 5100] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5200] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5300] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5400] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5500] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5600] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5700] train_perplexity =     3.31, train_loss =  1.20, learning_rate = 0.07812500


[batch = 5800] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5900] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 6000] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 6100] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6200] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6300] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6400] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6500] train_perplexity =     3.34, train_loss =  1.20, learning_rate = 0.07812500


[batch = 6600] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6700] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6800] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 6900] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.07812500


[epoch =  90] validation_perplexity =     3.56, validation_loss =  1.27

Saved model to 'e200/ptb_char_lstm.pt'...
[epoch =  90] min_validation_perplexity =     3.56, min_validation_loss =  1.27


[batch =  100] train_perplexity =     3.70, train_loss =  1.31, learning_rate = 0.07812500


[batch =  200] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch =  300] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch =  400] train_perplexity =     3.34, train_loss =  1.21, learning_rate = 0.07812500


[batch =  500] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch =  600] train_perplexity =     3.30, train_loss =  1.19, learning_rate = 0.07812500


[batch =  700] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch =  800] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch =  900] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1000] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch = 1100] train_perplexity =     3.58, train_loss =  1.28, learning_rate = 0.07812500


[batch = 1200] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 1300] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch = 1400] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1500] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 1600] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 1700] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1800] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1900] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2000] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 0.07812500


[batch = 2100] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2200] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2300] train_perplexity =     3.54, train_loss =  1.27, learning_rate = 0.07812500


[batch = 2400] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2500] train_perplexity =     3.47, train_loss =  1.25, learning_rate = 0.07812500


[batch = 2600] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2700] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 0.07812500


[batch = 2800] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2900] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3000] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 3100] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3200] train_perplexity =     3.60, train_loss =  1.28, learning_rate = 0.07812500


[batch = 3300] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3400] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3500] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3600] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3700] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3800] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3900] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4000] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4100] train_perplexity =     3.34, train_loss =  1.21, learning_rate = 0.07812500


[batch = 4200] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4300] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4400] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4500] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4600] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4700] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4800] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 4900] train_perplexity =     3.51, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5000] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 5100] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5200] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5300] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5400] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5500] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5600] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5700] train_perplexity =     3.31, train_loss =  1.20, learning_rate = 0.07812500


[batch = 5800] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5900] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 6000] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 6100] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6200] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6300] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6400] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6500] train_perplexity =     3.34, train_loss =  1.20, learning_rate = 0.07812500


[batch = 6600] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6700] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6800] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 6900] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.07812500


[epoch =  91] validation_perplexity =     3.56, validation_loss =  1.27

Saved model to 'e200/ptb_char_lstm.pt'...
[epoch =  91] min_validation_perplexity =     3.56, min_validation_loss =  1.27


[batch =  100] train_perplexity =     3.70, train_loss =  1.31, learning_rate = 0.07812500


[batch =  200] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch =  300] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch =  400] train_perplexity =     3.34, train_loss =  1.21, learning_rate = 0.07812500


[batch =  500] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch =  600] train_perplexity =     3.30, train_loss =  1.19, learning_rate = 0.07812500


[batch =  700] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch =  800] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch =  900] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1000] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch = 1100] train_perplexity =     3.58, train_loss =  1.28, learning_rate = 0.07812500


[batch = 1200] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 1300] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch = 1400] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1500] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 1600] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 1700] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1800] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1900] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2000] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 0.07812500


[batch = 2100] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2200] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2300] train_perplexity =     3.54, train_loss =  1.27, learning_rate = 0.07812500


[batch = 2400] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2500] train_perplexity =     3.47, train_loss =  1.25, learning_rate = 0.07812500


[batch = 2600] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2700] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 0.07812500


[batch = 2800] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2900] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3000] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 3100] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3200] train_perplexity =     3.60, train_loss =  1.28, learning_rate = 0.07812500


[batch = 3300] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3400] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3500] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3600] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3700] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3800] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3900] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4000] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4100] train_perplexity =     3.34, train_loss =  1.21, learning_rate = 0.07812500


[batch = 4200] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4300] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4400] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4500] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4600] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4700] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4800] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 4900] train_perplexity =     3.51, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5000] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 5100] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5200] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5300] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5400] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5500] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5600] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5700] train_perplexity =     3.31, train_loss =  1.20, learning_rate = 0.07812500


[batch = 5800] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5900] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 6000] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 6100] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6200] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6300] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6400] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6500] train_perplexity =     3.34, train_loss =  1.20, learning_rate = 0.07812500


[batch = 6600] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6700] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6800] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 6900] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.07812500


[epoch =  92] validation_perplexity =     3.56, validation_loss =  1.27

Saved model to 'e200/ptb_char_lstm.pt'...
[epoch =  92] min_validation_perplexity =     3.56, min_validation_loss =  1.27


[batch =  100] train_perplexity =     3.70, train_loss =  1.31, learning_rate = 0.07812500


[batch =  200] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch =  300] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch =  400] train_perplexity =     3.34, train_loss =  1.21, learning_rate = 0.07812500


[batch =  500] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch =  600] train_perplexity =     3.30, train_loss =  1.19, learning_rate = 0.07812500


[batch =  700] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch =  800] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch =  900] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1000] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch = 1100] train_perplexity =     3.58, train_loss =  1.28, learning_rate = 0.07812500


[batch = 1200] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 1300] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch = 1400] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1500] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 1600] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 1700] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1800] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1900] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2000] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 0.07812500


[batch = 2100] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2200] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2300] train_perplexity =     3.54, train_loss =  1.27, learning_rate = 0.07812500


[batch = 2400] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2500] train_perplexity =     3.47, train_loss =  1.25, learning_rate = 0.07812500


[batch = 2600] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2700] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 0.07812500


[batch = 2800] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2900] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3000] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 3100] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3200] train_perplexity =     3.60, train_loss =  1.28, learning_rate = 0.07812500


[batch = 3300] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3400] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3500] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3600] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3700] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3800] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3900] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4000] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4100] train_perplexity =     3.34, train_loss =  1.21, learning_rate = 0.07812500


[batch = 4200] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4300] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4400] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4500] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4600] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4700] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4800] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 4900] train_perplexity =     3.51, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5000] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 5100] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5200] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5300] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5400] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5500] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5600] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5700] train_perplexity =     3.31, train_loss =  1.20, learning_rate = 0.07812500


[batch = 5800] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5900] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 6000] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 6100] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6200] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6300] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6400] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6500] train_perplexity =     3.34, train_loss =  1.20, learning_rate = 0.07812500


[batch = 6600] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6700] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6800] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 6900] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.07812500


[epoch =  93] validation_perplexity =     3.56, validation_loss =  1.27

Saved model to 'e200/ptb_char_lstm.pt'...
[epoch =  93] min_validation_perplexity =     3.56, min_validation_loss =  1.27


[batch =  100] train_perplexity =     3.70, train_loss =  1.31, learning_rate = 0.07812500


[batch =  200] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch =  300] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch =  400] train_perplexity =     3.34, train_loss =  1.21, learning_rate = 0.07812500


[batch =  500] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch =  600] train_perplexity =     3.30, train_loss =  1.19, learning_rate = 0.07812500


[batch =  700] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch =  800] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch =  900] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1000] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch = 1100] train_perplexity =     3.58, train_loss =  1.28, learning_rate = 0.07812500


[batch = 1200] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 1300] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch = 1400] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1500] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 1600] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 1700] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1800] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1900] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2000] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 0.07812500


[batch = 2100] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2200] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2300] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2400] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2500] train_perplexity =     3.47, train_loss =  1.25, learning_rate = 0.07812500


[batch = 2600] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2700] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 0.07812500


[batch = 2800] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2900] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3000] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 3100] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3200] train_perplexity =     3.60, train_loss =  1.28, learning_rate = 0.07812500


[batch = 3300] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3400] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3500] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3600] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3700] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3800] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3900] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4000] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4100] train_perplexity =     3.34, train_loss =  1.21, learning_rate = 0.07812500


[batch = 4200] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4300] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4400] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4500] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4600] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4700] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4800] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 4900] train_perplexity =     3.51, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5000] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 5100] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5200] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5300] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5400] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5500] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5600] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5700] train_perplexity =     3.31, train_loss =  1.20, learning_rate = 0.07812500


[batch = 5800] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5900] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 6000] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 6100] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6200] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6300] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6400] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6500] train_perplexity =     3.34, train_loss =  1.20, learning_rate = 0.07812500


[batch = 6600] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6700] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6800] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 6900] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.07812500


[epoch =  94] validation_perplexity =     3.56, validation_loss =  1.27

Saved model to 'e200/ptb_char_lstm.pt'...
[epoch =  94] min_validation_perplexity =     3.56, min_validation_loss =  1.27


[batch =  100] train_perplexity =     3.70, train_loss =  1.31, learning_rate = 0.07812500


[batch =  200] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch =  300] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch =  400] train_perplexity =     3.34, train_loss =  1.21, learning_rate = 0.07812500


[batch =  500] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch =  600] train_perplexity =     3.30, train_loss =  1.19, learning_rate = 0.07812500


[batch =  700] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch =  800] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch =  900] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1000] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch = 1100] train_perplexity =     3.58, train_loss =  1.28, learning_rate = 0.07812500


[batch = 1200] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 1300] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch = 1400] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1500] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 1600] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 1700] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1800] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1900] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2000] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 0.07812500


[batch = 2100] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2200] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2300] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2400] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2500] train_perplexity =     3.47, train_loss =  1.25, learning_rate = 0.07812500


[batch = 2600] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2700] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 0.07812500


[batch = 2800] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2900] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3000] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 3100] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3200] train_perplexity =     3.59, train_loss =  1.28, learning_rate = 0.07812500


[batch = 3300] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3400] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3500] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3600] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3700] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3800] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3900] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4000] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4100] train_perplexity =     3.34, train_loss =  1.21, learning_rate = 0.07812500


[batch = 4200] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4300] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4400] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4500] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4600] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4700] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4800] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 4900] train_perplexity =     3.51, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5000] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 5100] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5200] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5300] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5400] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5500] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5600] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5700] train_perplexity =     3.31, train_loss =  1.20, learning_rate = 0.07812500


[batch = 5800] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5900] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 6000] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 6100] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6200] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6300] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6400] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6500] train_perplexity =     3.34, train_loss =  1.20, learning_rate = 0.07812500


[batch = 6600] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6700] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6800] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 6900] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.07812500


[epoch =  95] validation_perplexity =     3.56, validation_loss =  1.27

Saved model to 'e200/ptb_char_lstm.pt'...
[epoch =  95] min_validation_perplexity =     3.56, min_validation_loss =  1.27


[batch =  100] train_perplexity =     3.70, train_loss =  1.31, learning_rate = 0.07812500


[batch =  200] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch =  300] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch =  400] train_perplexity =     3.34, train_loss =  1.21, learning_rate = 0.07812500


[batch =  500] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch =  600] train_perplexity =     3.30, train_loss =  1.19, learning_rate = 0.07812500


[batch =  700] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch =  800] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch =  900] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1000] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch = 1100] train_perplexity =     3.58, train_loss =  1.28, learning_rate = 0.07812500


[batch = 1200] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 1300] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch = 1400] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1500] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 1600] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 1700] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1800] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1900] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2000] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 0.07812500


[batch = 2100] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2200] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2300] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2400] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2500] train_perplexity =     3.47, train_loss =  1.25, learning_rate = 0.07812500


[batch = 2600] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2700] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 0.07812500


[batch = 2800] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2900] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3000] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 3100] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3200] train_perplexity =     3.59, train_loss =  1.28, learning_rate = 0.07812500


[batch = 3300] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3400] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3500] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3600] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3700] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3800] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3900] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4000] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4100] train_perplexity =     3.34, train_loss =  1.21, learning_rate = 0.07812500


[batch = 4200] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4300] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4400] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4500] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4600] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4700] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4800] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 4900] train_perplexity =     3.51, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5000] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 5100] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5200] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5300] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5400] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5500] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5600] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5700] train_perplexity =     3.31, train_loss =  1.20, learning_rate = 0.07812500


[batch = 5800] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5900] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 6000] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 6100] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6200] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6300] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6400] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6500] train_perplexity =     3.34, train_loss =  1.20, learning_rate = 0.07812500


[batch = 6600] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6700] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6800] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 6900] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.07812500


[epoch =  96] validation_perplexity =     3.56, validation_loss =  1.27

Saved model to 'e200/ptb_char_lstm.pt'...
[epoch =  96] min_validation_perplexity =     3.56, min_validation_loss =  1.27


[batch =  100] train_perplexity =     3.70, train_loss =  1.31, learning_rate = 0.07812500


[batch =  200] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch =  300] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch =  400] train_perplexity =     3.34, train_loss =  1.21, learning_rate = 0.07812500


[batch =  500] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch =  600] train_perplexity =     3.30, train_loss =  1.19, learning_rate = 0.07812500


[batch =  700] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch =  800] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch =  900] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1000] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch = 1100] train_perplexity =     3.58, train_loss =  1.28, learning_rate = 0.07812500


[batch = 1200] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 1300] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch = 1400] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1500] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 1600] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 1700] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1800] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1900] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2000] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 0.07812500


[batch = 2100] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2200] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2300] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2400] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2500] train_perplexity =     3.47, train_loss =  1.25, learning_rate = 0.07812500


[batch = 2600] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2700] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 0.07812500


[batch = 2800] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2900] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.07812500


[batch = 3000] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 3100] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3200] train_perplexity =     3.59, train_loss =  1.28, learning_rate = 0.07812500


[batch = 3300] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3400] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3500] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3600] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3700] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3800] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3900] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4000] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4100] train_perplexity =     3.34, train_loss =  1.21, learning_rate = 0.07812500


[batch = 4200] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4300] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4400] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4500] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4600] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4700] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4800] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 4900] train_perplexity =     3.51, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5000] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 5100] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5200] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5300] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5400] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5500] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5600] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5700] train_perplexity =     3.31, train_loss =  1.20, learning_rate = 0.07812500


[batch = 5800] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5900] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 6000] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 6100] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6200] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6300] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6400] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6500] train_perplexity =     3.34, train_loss =  1.20, learning_rate = 0.07812500


[batch = 6600] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6700] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6800] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 6900] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.07812500


[epoch =  97] validation_perplexity =     3.56, validation_loss =  1.27

Saved model to 'e200/ptb_char_lstm.pt'...
[epoch =  97] min_validation_perplexity =     3.56, min_validation_loss =  1.27


[batch =  100] train_perplexity =     3.70, train_loss =  1.31, learning_rate = 0.07812500


[batch =  200] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch =  300] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch =  400] train_perplexity =     3.34, train_loss =  1.21, learning_rate = 0.07812500


[batch =  500] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch =  600] train_perplexity =     3.30, train_loss =  1.19, learning_rate = 0.07812500


[batch =  700] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch =  800] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch =  900] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1000] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch = 1100] train_perplexity =     3.58, train_loss =  1.28, learning_rate = 0.07812500


[batch = 1200] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 1300] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch = 1400] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1500] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 1600] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 1700] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1800] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1900] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2000] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 0.07812500


[batch = 2100] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2200] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2300] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2400] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2500] train_perplexity =     3.47, train_loss =  1.25, learning_rate = 0.07812500


[batch = 2600] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2700] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 0.07812500


[batch = 2800] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2900] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.07812500


[batch = 3000] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 3100] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3200] train_perplexity =     3.59, train_loss =  1.28, learning_rate = 0.07812500


[batch = 3300] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3400] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3500] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3600] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3700] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3800] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3900] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4000] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4100] train_perplexity =     3.34, train_loss =  1.21, learning_rate = 0.07812500


[batch = 4200] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4300] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4400] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4500] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4600] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4700] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4800] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 4900] train_perplexity =     3.51, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5000] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 5100] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5200] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5300] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5400] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5500] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5600] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5700] train_perplexity =     3.31, train_loss =  1.20, learning_rate = 0.07812500


[batch = 5800] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5900] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 6000] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 6100] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6200] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6300] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6400] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6500] train_perplexity =     3.34, train_loss =  1.20, learning_rate = 0.07812500


[batch = 6600] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6700] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6800] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 6900] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.07812500


[epoch =  98] validation_perplexity =     3.56, validation_loss =  1.27

Saved model to 'e200/ptb_char_lstm.pt'...
[epoch =  98] min_validation_perplexity =     3.56, min_validation_loss =  1.27


[batch =  100] train_perplexity =     3.70, train_loss =  1.31, learning_rate = 0.07812500


[batch =  200] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch =  300] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch =  400] train_perplexity =     3.34, train_loss =  1.21, learning_rate = 0.07812500


[batch =  500] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch =  600] train_perplexity =     3.30, train_loss =  1.19, learning_rate = 0.07812500


[batch =  700] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch =  800] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch =  900] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1000] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch = 1100] train_perplexity =     3.58, train_loss =  1.28, learning_rate = 0.07812500


[batch = 1200] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 1300] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch = 1400] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1500] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 1600] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 1700] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1800] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1900] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2000] train_perplexity =     3.54, train_loss =  1.27, learning_rate = 0.07812500


[batch = 2100] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2200] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2300] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2400] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2500] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2600] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2700] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 0.07812500


[batch = 2800] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2900] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.07812500


[batch = 3000] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 3100] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3200] train_perplexity =     3.59, train_loss =  1.28, learning_rate = 0.07812500


[batch = 3300] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3400] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3500] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3600] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3700] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3800] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3900] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4000] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4100] train_perplexity =     3.34, train_loss =  1.21, learning_rate = 0.07812500


[batch = 4200] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4300] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4400] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4500] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4600] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4700] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4800] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 4900] train_perplexity =     3.51, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5000] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 5100] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5200] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5300] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5400] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5500] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5600] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5700] train_perplexity =     3.31, train_loss =  1.20, learning_rate = 0.07812500


[batch = 5800] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5900] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 6000] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 6100] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6200] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6300] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6400] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6500] train_perplexity =     3.34, train_loss =  1.20, learning_rate = 0.07812500


[batch = 6600] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6700] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6800] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 6900] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.07812500


[epoch =  99] validation_perplexity =     3.56, validation_loss =  1.27

Saved model to 'e200/ptb_char_lstm.pt'...
[epoch =  99] min_validation_perplexity =     3.56, min_validation_loss =  1.27


[batch =  100] train_perplexity =     3.70, train_loss =  1.31, learning_rate = 0.07812500


[batch =  200] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch =  300] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch =  400] train_perplexity =     3.34, train_loss =  1.21, learning_rate = 0.07812500


[batch =  500] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch =  600] train_perplexity =     3.30, train_loss =  1.19, learning_rate = 0.07812500


[batch =  700] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch =  800] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch =  900] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1000] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch = 1100] train_perplexity =     3.58, train_loss =  1.28, learning_rate = 0.07812500


[batch = 1200] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 1300] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch = 1400] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1500] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 1600] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 1700] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1800] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1900] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2000] train_perplexity =     3.54, train_loss =  1.27, learning_rate = 0.07812500


[batch = 2100] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2200] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2300] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2400] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2500] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2600] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2700] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 0.07812500


[batch = 2800] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2900] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.07812500


[batch = 3000] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 3100] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3200] train_perplexity =     3.59, train_loss =  1.28, learning_rate = 0.07812500


[batch = 3300] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3400] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3500] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3600] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3700] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3800] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3900] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4000] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4100] train_perplexity =     3.34, train_loss =  1.21, learning_rate = 0.07812500


[batch = 4200] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4300] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4400] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4500] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4600] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4700] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4800] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 4900] train_perplexity =     3.51, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5000] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 5100] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5200] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5300] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5400] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5500] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5600] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5700] train_perplexity =     3.31, train_loss =  1.20, learning_rate = 0.07812500


[batch = 5800] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5900] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 6000] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 6100] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6200] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6300] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6400] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6500] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch = 6600] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6700] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6800] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 6900] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.07812500


[epoch = 100] validation_perplexity =     3.56, validation_loss =  1.27

Saved model to 'e200/ptb_char_lstm.pt'...
[epoch = 100] min_validation_perplexity =     3.56, min_validation_loss =  1.27


[batch =  100] train_perplexity =     3.70, train_loss =  1.31, learning_rate = 0.07812500


[batch =  200] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch =  300] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch =  400] train_perplexity =     3.34, train_loss =  1.21, learning_rate = 0.07812500


[batch =  500] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch =  600] train_perplexity =     3.30, train_loss =  1.19, learning_rate = 0.07812500


[batch =  700] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch =  800] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch =  900] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1000] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch = 1100] train_perplexity =     3.58, train_loss =  1.27, learning_rate = 0.07812500


[batch = 1200] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 1300] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch = 1400] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1500] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 1600] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 1700] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1800] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1900] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2000] train_perplexity =     3.54, train_loss =  1.27, learning_rate = 0.07812500


[batch = 2100] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2200] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2300] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2400] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2500] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2600] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2700] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 0.07812500


[batch = 2800] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2900] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.07812500


[batch = 3000] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 3100] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3200] train_perplexity =     3.59, train_loss =  1.28, learning_rate = 0.07812500


[batch = 3300] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3400] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3500] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3600] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3700] train_perplexity =     3.47, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3800] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3900] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4000] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4100] train_perplexity =     3.34, train_loss =  1.21, learning_rate = 0.07812500


[batch = 4200] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4300] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4400] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4500] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4600] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4700] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4800] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 4900] train_perplexity =     3.51, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5000] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 5100] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5200] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5300] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5400] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5500] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5600] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5700] train_perplexity =     3.31, train_loss =  1.20, learning_rate = 0.07812500


[batch = 5800] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5900] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 6000] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 6100] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6200] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6300] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6400] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6500] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch = 6600] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6700] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6800] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 6900] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.07812500


[epoch = 101] validation_perplexity =     3.56, validation_loss =  1.27

Saved model to 'e200/ptb_char_lstm.pt'...
[epoch = 101] min_validation_perplexity =     3.56, min_validation_loss =  1.27


[batch =  100] train_perplexity =     3.70, train_loss =  1.31, learning_rate = 0.07812500


[batch =  200] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch =  300] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch =  400] train_perplexity =     3.34, train_loss =  1.21, learning_rate = 0.07812500


[batch =  500] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch =  600] train_perplexity =     3.30, train_loss =  1.19, learning_rate = 0.07812500


[batch =  700] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch =  800] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch =  900] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1000] train_perplexity =     3.51, train_loss =  1.25, learning_rate = 0.07812500


[batch = 1100] train_perplexity =     3.58, train_loss =  1.27, learning_rate = 0.07812500


[batch = 1200] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 1300] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch = 1400] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1500] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 1600] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 1700] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1800] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1900] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2000] train_perplexity =     3.54, train_loss =  1.27, learning_rate = 0.07812500


[batch = 2100] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2200] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2300] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2400] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2500] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2600] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2700] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 0.07812500


[batch = 2800] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2900] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.07812500


[batch = 3000] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 3100] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3200] train_perplexity =     3.59, train_loss =  1.28, learning_rate = 0.07812500


[batch = 3300] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3400] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3500] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3600] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3700] train_perplexity =     3.47, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3800] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3900] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4000] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4100] train_perplexity =     3.34, train_loss =  1.21, learning_rate = 0.07812500


[batch = 4200] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4300] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4400] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4500] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4600] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4700] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4800] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 4900] train_perplexity =     3.51, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5000] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 5100] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5200] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5300] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5400] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5500] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5600] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5700] train_perplexity =     3.31, train_loss =  1.20, learning_rate = 0.07812500


[batch = 5800] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5900] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 6000] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 6100] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6200] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6300] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6400] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6500] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch = 6600] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6700] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6800] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 6900] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.07812500


[epoch = 102] validation_perplexity =     3.56, validation_loss =  1.27

Saved model to 'e200/ptb_char_lstm.pt'...
[epoch = 102] min_validation_perplexity =     3.56, min_validation_loss =  1.27


[batch =  100] train_perplexity =     3.70, train_loss =  1.31, learning_rate = 0.07812500


[batch =  200] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch =  300] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch =  400] train_perplexity =     3.34, train_loss =  1.21, learning_rate = 0.07812500


[batch =  500] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch =  600] train_perplexity =     3.30, train_loss =  1.19, learning_rate = 0.07812500


[batch =  700] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch =  800] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch =  900] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1000] train_perplexity =     3.51, train_loss =  1.25, learning_rate = 0.07812500


[batch = 1100] train_perplexity =     3.58, train_loss =  1.27, learning_rate = 0.07812500


[batch = 1200] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 1300] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch = 1400] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1500] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 1600] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 1700] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1800] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1900] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2000] train_perplexity =     3.54, train_loss =  1.27, learning_rate = 0.07812500


[batch = 2100] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2200] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2300] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2400] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2500] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2600] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2700] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 0.07812500


[batch = 2800] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2900] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.07812500


[batch = 3000] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 3100] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3200] train_perplexity =     3.59, train_loss =  1.28, learning_rate = 0.07812500


[batch = 3300] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3400] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3500] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3600] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3700] train_perplexity =     3.47, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3800] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3900] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4000] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4100] train_perplexity =     3.34, train_loss =  1.21, learning_rate = 0.07812500


[batch = 4200] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4300] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4400] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4500] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4600] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4700] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4800] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 4900] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5000] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 5100] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5200] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5300] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5400] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5500] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5600] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5700] train_perplexity =     3.31, train_loss =  1.20, learning_rate = 0.07812500


[batch = 5800] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5900] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 6000] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 6100] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6200] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6300] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6400] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6500] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch = 6600] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6700] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6800] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 6900] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.07812500


[epoch = 103] validation_perplexity =     3.56, validation_loss =  1.27

Saved model to 'e200/ptb_char_lstm.pt'...
[epoch = 103] min_validation_perplexity =     3.56, min_validation_loss =  1.27


[batch =  100] train_perplexity =     3.70, train_loss =  1.31, learning_rate = 0.07812500


[batch =  200] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch =  300] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch =  400] train_perplexity =     3.34, train_loss =  1.21, learning_rate = 0.07812500


[batch =  500] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch =  600] train_perplexity =     3.30, train_loss =  1.19, learning_rate = 0.07812500


[batch =  700] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch =  800] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch =  900] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1000] train_perplexity =     3.51, train_loss =  1.25, learning_rate = 0.07812500


[batch = 1100] train_perplexity =     3.58, train_loss =  1.27, learning_rate = 0.07812500


[batch = 1200] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 1300] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch = 1400] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1500] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 1600] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 1700] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1800] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1900] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2000] train_perplexity =     3.54, train_loss =  1.27, learning_rate = 0.07812500


[batch = 2100] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2200] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2300] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2400] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2500] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2600] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2700] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 0.07812500


[batch = 2800] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2900] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.07812500


[batch = 3000] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 3100] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3200] train_perplexity =     3.59, train_loss =  1.28, learning_rate = 0.07812500


[batch = 3300] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3400] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3500] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3600] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3700] train_perplexity =     3.47, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3800] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3900] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4000] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4100] train_perplexity =     3.34, train_loss =  1.21, learning_rate = 0.07812500


[batch = 4200] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4300] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4400] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4500] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4600] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4700] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4800] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 4900] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5000] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 5100] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5200] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5300] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5400] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5500] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5600] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5700] train_perplexity =     3.31, train_loss =  1.20, learning_rate = 0.07812500


[batch = 5800] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5900] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 6000] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 6100] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6200] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6300] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6400] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6500] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch = 6600] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6700] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6800] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 6900] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.07812500


[epoch = 104] validation_perplexity =     3.56, validation_loss =  1.27

Saved model to 'e200/ptb_char_lstm.pt'...
[epoch = 104] min_validation_perplexity =     3.56, min_validation_loss =  1.27


[batch =  100] train_perplexity =     3.70, train_loss =  1.31, learning_rate = 0.07812500


[batch =  200] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch =  300] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch =  400] train_perplexity =     3.34, train_loss =  1.21, learning_rate = 0.07812500


[batch =  500] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch =  600] train_perplexity =     3.30, train_loss =  1.19, learning_rate = 0.07812500


[batch =  700] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch =  800] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch =  900] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1000] train_perplexity =     3.51, train_loss =  1.25, learning_rate = 0.07812500


[batch = 1100] train_perplexity =     3.58, train_loss =  1.27, learning_rate = 0.07812500


[batch = 1200] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 1300] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch = 1400] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1500] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1600] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 1700] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1800] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1900] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2000] train_perplexity =     3.54, train_loss =  1.27, learning_rate = 0.07812500


[batch = 2100] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2200] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2300] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2400] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2500] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2600] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2700] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 0.07812500


[batch = 2800] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2900] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.07812500


[batch = 3000] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 3100] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3200] train_perplexity =     3.59, train_loss =  1.28, learning_rate = 0.07812500


[batch = 3300] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3400] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3500] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3600] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3700] train_perplexity =     3.47, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3800] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3900] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4000] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4100] train_perplexity =     3.34, train_loss =  1.21, learning_rate = 0.07812500


[batch = 4200] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4300] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4400] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4500] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4600] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4700] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4800] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 4900] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5000] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 5100] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5200] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5300] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5400] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5500] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5600] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5700] train_perplexity =     3.31, train_loss =  1.20, learning_rate = 0.07812500


[batch = 5800] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5900] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 6000] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 6100] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6200] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6300] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6400] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6500] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch = 6600] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6700] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6800] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 6900] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.07812500


[epoch = 105] validation_perplexity =     3.56, validation_loss =  1.27

Saved model to 'e200/ptb_char_lstm.pt'...
[epoch = 105] min_validation_perplexity =     3.56, min_validation_loss =  1.27


[batch =  100] train_perplexity =     3.70, train_loss =  1.31, learning_rate = 0.07812500


[batch =  200] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch =  300] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch =  400] train_perplexity =     3.34, train_loss =  1.21, learning_rate = 0.07812500


[batch =  500] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch =  600] train_perplexity =     3.30, train_loss =  1.19, learning_rate = 0.07812500


[batch =  700] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch =  800] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch =  900] train_perplexity =     3.40, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1000] train_perplexity =     3.51, train_loss =  1.25, learning_rate = 0.07812500


[batch = 1100] train_perplexity =     3.58, train_loss =  1.27, learning_rate = 0.07812500


[batch = 1200] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 1300] train_perplexity =     3.51, train_loss =  1.25, learning_rate = 0.07812500


[batch = 1400] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1500] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1600] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 1700] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1800] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1900] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2000] train_perplexity =     3.54, train_loss =  1.27, learning_rate = 0.07812500


[batch = 2100] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2200] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2300] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2400] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2500] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2600] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2700] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 0.07812500


[batch = 2800] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2900] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.07812500


[batch = 3000] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 3100] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3200] train_perplexity =     3.59, train_loss =  1.28, learning_rate = 0.07812500


[batch = 3300] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3400] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3500] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3600] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3700] train_perplexity =     3.47, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3800] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3900] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4000] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4100] train_perplexity =     3.34, train_loss =  1.20, learning_rate = 0.07812500


[batch = 4200] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4300] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4400] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4500] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4600] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4700] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4800] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 4900] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5000] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 5100] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5200] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5300] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5400] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5500] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5600] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5700] train_perplexity =     3.31, train_loss =  1.20, learning_rate = 0.07812500


[batch = 5800] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5900] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 6000] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 6100] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6200] train_perplexity =     3.40, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6300] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6400] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6500] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch = 6600] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6700] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6800] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 6900] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.07812500


[epoch = 106] validation_perplexity =     3.56, validation_loss =  1.27

Saved model to 'e200/ptb_char_lstm.pt'...
[epoch = 106] min_validation_perplexity =     3.56, min_validation_loss =  1.27


[batch =  100] train_perplexity =     3.70, train_loss =  1.31, learning_rate = 0.07812500


[batch =  200] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch =  300] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch =  400] train_perplexity =     3.34, train_loss =  1.21, learning_rate = 0.07812500


[batch =  500] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch =  600] train_perplexity =     3.30, train_loss =  1.19, learning_rate = 0.07812500


[batch =  700] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch =  800] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch =  900] train_perplexity =     3.40, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1000] train_perplexity =     3.51, train_loss =  1.25, learning_rate = 0.07812500


[batch = 1100] train_perplexity =     3.58, train_loss =  1.27, learning_rate = 0.07812500


[batch = 1200] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 1300] train_perplexity =     3.51, train_loss =  1.25, learning_rate = 0.07812500


[batch = 1400] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1500] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1600] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 1700] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1800] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1900] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2000] train_perplexity =     3.54, train_loss =  1.27, learning_rate = 0.07812500


[batch = 2100] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2200] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2300] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2400] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2500] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2600] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2700] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 0.07812500


[batch = 2800] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2900] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.07812500


[batch = 3000] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 3100] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3200] train_perplexity =     3.59, train_loss =  1.28, learning_rate = 0.07812500


[batch = 3300] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3400] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3500] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3600] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3700] train_perplexity =     3.47, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3800] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3900] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4000] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4100] train_perplexity =     3.34, train_loss =  1.20, learning_rate = 0.07812500


[batch = 4200] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4300] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4400] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4500] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4600] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4700] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4800] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 4900] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5000] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 5100] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5200] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5300] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5400] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5500] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5600] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5700] train_perplexity =     3.31, train_loss =  1.20, learning_rate = 0.07812500


[batch = 5800] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5900] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 6000] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 6100] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6200] train_perplexity =     3.40, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6300] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6400] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6500] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch = 6600] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6700] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6800] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 6900] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.07812500


[epoch = 107] validation_perplexity =     3.56, validation_loss =  1.27

Saved model to 'e200/ptb_char_lstm.pt'...
[epoch = 107] min_validation_perplexity =     3.56, min_validation_loss =  1.27


[batch =  100] train_perplexity =     3.70, train_loss =  1.31, learning_rate = 0.07812500


[batch =  200] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch =  300] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch =  400] train_perplexity =     3.34, train_loss =  1.21, learning_rate = 0.07812500


[batch =  500] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch =  600] train_perplexity =     3.30, train_loss =  1.19, learning_rate = 0.07812500


[batch =  700] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch =  800] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch =  900] train_perplexity =     3.40, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1000] train_perplexity =     3.51, train_loss =  1.25, learning_rate = 0.07812500


[batch = 1100] train_perplexity =     3.58, train_loss =  1.27, learning_rate = 0.07812500


[batch = 1200] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 1300] train_perplexity =     3.51, train_loss =  1.25, learning_rate = 0.07812500


[batch = 1400] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1500] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1600] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 1700] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1800] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1900] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2000] train_perplexity =     3.54, train_loss =  1.27, learning_rate = 0.07812500


[batch = 2100] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2200] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2300] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2400] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2500] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2600] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2700] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 0.07812500


[batch = 2800] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2900] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.07812500


[batch = 3000] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 3100] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3200] train_perplexity =     3.59, train_loss =  1.28, learning_rate = 0.07812500


[batch = 3300] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3400] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3500] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3600] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3700] train_perplexity =     3.47, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3800] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3900] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4000] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4100] train_perplexity =     3.34, train_loss =  1.20, learning_rate = 0.07812500


[batch = 4200] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4300] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4400] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4500] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4600] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4700] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4800] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 4900] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5000] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 5100] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5200] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5300] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5400] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5500] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5600] train_perplexity =     3.51, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5700] train_perplexity =     3.31, train_loss =  1.20, learning_rate = 0.07812500


[batch = 5800] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5900] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 6000] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 6100] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6200] train_perplexity =     3.40, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6300] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6400] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6500] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch = 6600] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6700] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6800] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 6900] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.07812500


[epoch = 108] validation_perplexity =     3.56, validation_loss =  1.27

Saved model to 'e200/ptb_char_lstm.pt'...
[epoch = 108] min_validation_perplexity =     3.56, min_validation_loss =  1.27


[batch =  100] train_perplexity =     3.70, train_loss =  1.31, learning_rate = 0.07812500


[batch =  200] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch =  300] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch =  400] train_perplexity =     3.34, train_loss =  1.21, learning_rate = 0.07812500


[batch =  500] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch =  600] train_perplexity =     3.30, train_loss =  1.19, learning_rate = 0.07812500


[batch =  700] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch =  800] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch =  900] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 1000] train_perplexity =     3.51, train_loss =  1.25, learning_rate = 0.07812500


[batch = 1100] train_perplexity =     3.58, train_loss =  1.27, learning_rate = 0.07812500


[batch = 1200] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 1300] train_perplexity =     3.51, train_loss =  1.25, learning_rate = 0.07812500


[batch = 1400] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1500] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1600] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 1700] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1800] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1900] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2000] train_perplexity =     3.54, train_loss =  1.27, learning_rate = 0.07812500


[batch = 2100] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2200] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2300] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2400] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2500] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2600] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2700] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 0.07812500


[batch = 2800] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2900] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.07812500


[batch = 3000] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 3100] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3200] train_perplexity =     3.59, train_loss =  1.28, learning_rate = 0.07812500


[batch = 3300] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3400] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3500] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3600] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3700] train_perplexity =     3.47, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3800] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3900] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4000] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4100] train_perplexity =     3.34, train_loss =  1.20, learning_rate = 0.07812500


[batch = 4200] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4300] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4400] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4500] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4600] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4700] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4800] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 4900] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5000] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 5100] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5200] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5300] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5400] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5500] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5600] train_perplexity =     3.51, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5700] train_perplexity =     3.31, train_loss =  1.20, learning_rate = 0.07812500


[batch = 5800] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5900] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 6000] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 6100] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6200] train_perplexity =     3.40, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6300] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6400] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6500] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch = 6600] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6700] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6800] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 6900] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.07812500


[epoch = 109] validation_perplexity =     3.56, validation_loss =  1.27

Saved model to 'e200/ptb_char_lstm.pt'...
[epoch = 109] min_validation_perplexity =     3.56, min_validation_loss =  1.27


[batch =  100] train_perplexity =     3.70, train_loss =  1.31, learning_rate = 0.07812500


[batch =  200] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch =  300] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch =  400] train_perplexity =     3.34, train_loss =  1.21, learning_rate = 0.07812500


[batch =  500] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch =  600] train_perplexity =     3.30, train_loss =  1.19, learning_rate = 0.07812500


[batch =  700] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch =  800] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch =  900] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 1000] train_perplexity =     3.51, train_loss =  1.25, learning_rate = 0.07812500


[batch = 1100] train_perplexity =     3.58, train_loss =  1.27, learning_rate = 0.07812500


[batch = 1200] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 1300] train_perplexity =     3.51, train_loss =  1.25, learning_rate = 0.07812500


[batch = 1400] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1500] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1600] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 1700] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1800] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1900] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2000] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2100] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2200] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2300] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2400] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2500] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2600] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2700] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 0.07812500


[batch = 2800] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2900] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.07812500


[batch = 3000] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 3100] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3200] train_perplexity =     3.59, train_loss =  1.28, learning_rate = 0.07812500


[batch = 3300] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3400] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3500] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3600] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3700] train_perplexity =     3.47, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3800] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3900] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4000] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4100] train_perplexity =     3.34, train_loss =  1.20, learning_rate = 0.07812500


[batch = 4200] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4300] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4400] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4500] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4600] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4700] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4800] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 4900] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5000] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 5100] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5200] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5300] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5400] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5500] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5600] train_perplexity =     3.51, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5700] train_perplexity =     3.31, train_loss =  1.20, learning_rate = 0.07812500


[batch = 5800] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5900] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 6000] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 6100] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6200] train_perplexity =     3.40, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6300] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6400] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6500] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch = 6600] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6700] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6800] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 6900] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.07812500


[epoch = 110] validation_perplexity =     3.56, validation_loss =  1.27

Saved model to 'e200/ptb_char_lstm.pt'...
[epoch = 110] min_validation_perplexity =     3.56, min_validation_loss =  1.27


[batch =  100] train_perplexity =     3.70, train_loss =  1.31, learning_rate = 0.07812500


[batch =  200] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch =  300] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch =  400] train_perplexity =     3.34, train_loss =  1.21, learning_rate = 0.07812500


[batch =  500] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch =  600] train_perplexity =     3.30, train_loss =  1.19, learning_rate = 0.07812500


[batch =  700] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch =  800] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch =  900] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 1000] train_perplexity =     3.51, train_loss =  1.25, learning_rate = 0.07812500


[batch = 1100] train_perplexity =     3.58, train_loss =  1.27, learning_rate = 0.07812500


[batch = 1200] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 1300] train_perplexity =     3.51, train_loss =  1.25, learning_rate = 0.07812500


[batch = 1400] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1500] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1600] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 1700] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1800] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1900] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2000] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2100] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2200] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2300] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2400] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2500] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2600] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2700] train_perplexity =     3.55, train_loss =  1.27, learning_rate = 0.07812500


[batch = 2800] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2900] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.07812500


[batch = 3000] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 3100] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3200] train_perplexity =     3.59, train_loss =  1.28, learning_rate = 0.07812500


[batch = 3300] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3400] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3500] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3600] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3700] train_perplexity =     3.47, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3800] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3900] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4000] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4100] train_perplexity =     3.34, train_loss =  1.20, learning_rate = 0.07812500


[batch = 4200] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4300] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4400] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4500] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4600] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4700] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4800] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 4900] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5000] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 5100] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5200] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5300] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5400] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5500] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5600] train_perplexity =     3.51, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5700] train_perplexity =     3.31, train_loss =  1.20, learning_rate = 0.07812500


[batch = 5800] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5900] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 6000] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 6100] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6200] train_perplexity =     3.40, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6300] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6400] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6500] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch = 6600] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6700] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6800] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 6900] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.07812500


[epoch = 111] validation_perplexity =     3.56, validation_loss =  1.27

Saved model to 'e200/ptb_char_lstm.pt'...
[epoch = 111] min_validation_perplexity =     3.56, min_validation_loss =  1.27


[batch =  100] train_perplexity =     3.70, train_loss =  1.31, learning_rate = 0.07812500


[batch =  200] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch =  300] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch =  400] train_perplexity =     3.34, train_loss =  1.21, learning_rate = 0.07812500


[batch =  500] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch =  600] train_perplexity =     3.30, train_loss =  1.19, learning_rate = 0.07812500


[batch =  700] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch =  800] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch =  900] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 1000] train_perplexity =     3.51, train_loss =  1.25, learning_rate = 0.07812500


[batch = 1100] train_perplexity =     3.58, train_loss =  1.27, learning_rate = 0.07812500


[batch = 1200] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 1300] train_perplexity =     3.51, train_loss =  1.25, learning_rate = 0.07812500


[batch = 1400] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1500] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1600] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 1700] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1800] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1900] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2000] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2100] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2200] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2300] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2400] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2500] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2600] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2700] train_perplexity =     3.54, train_loss =  1.27, learning_rate = 0.07812500


[batch = 2800] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2900] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.07812500


[batch = 3000] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 3100] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3200] train_perplexity =     3.59, train_loss =  1.28, learning_rate = 0.07812500


[batch = 3300] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3400] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3500] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3600] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3700] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3800] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3900] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4000] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4100] train_perplexity =     3.34, train_loss =  1.20, learning_rate = 0.07812500


[batch = 4200] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4300] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4400] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4500] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4600] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4700] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4800] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 4900] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5000] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 5100] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5200] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5300] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5400] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5500] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5600] train_perplexity =     3.51, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5700] train_perplexity =     3.31, train_loss =  1.20, learning_rate = 0.07812500


[batch = 5800] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5900] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 6000] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 6100] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6200] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6300] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6400] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6500] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch = 6600] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6700] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6800] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 6900] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.07812500


[epoch = 112] validation_perplexity =     3.56, validation_loss =  1.27

Saved model to 'e200/ptb_char_lstm.pt'...
[epoch = 112] min_validation_perplexity =     3.56, min_validation_loss =  1.27


[batch =  100] train_perplexity =     3.70, train_loss =  1.31, learning_rate = 0.07812500


[batch =  200] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch =  300] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch =  400] train_perplexity =     3.34, train_loss =  1.21, learning_rate = 0.07812500


[batch =  500] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch =  600] train_perplexity =     3.30, train_loss =  1.19, learning_rate = 0.07812500


[batch =  700] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch =  800] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch =  900] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 1000] train_perplexity =     3.51, train_loss =  1.25, learning_rate = 0.07812500


[batch = 1100] train_perplexity =     3.58, train_loss =  1.27, learning_rate = 0.07812500


[batch = 1200] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 1300] train_perplexity =     3.51, train_loss =  1.25, learning_rate = 0.07812500


[batch = 1400] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1500] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1600] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 1700] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1800] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1900] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2000] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2100] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2200] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2300] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2400] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2500] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2600] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2700] train_perplexity =     3.54, train_loss =  1.27, learning_rate = 0.07812500


[batch = 2800] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2900] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.07812500


[batch = 3000] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 3100] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3200] train_perplexity =     3.59, train_loss =  1.28, learning_rate = 0.07812500


[batch = 3300] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3400] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3500] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3600] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3700] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3800] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3900] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4000] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4100] train_perplexity =     3.34, train_loss =  1.20, learning_rate = 0.07812500


[batch = 4200] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4300] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4400] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4500] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4600] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4700] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4800] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 4900] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5000] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 5100] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5200] train_perplexity =     3.51, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5300] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5400] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5500] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5600] train_perplexity =     3.51, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5700] train_perplexity =     3.31, train_loss =  1.20, learning_rate = 0.07812500


[batch = 5800] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5900] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 6000] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 6100] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6200] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6300] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6400] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6500] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch = 6600] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6700] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6800] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 6900] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.07812500


[epoch = 113] validation_perplexity =     3.56, validation_loss =  1.27

Saved model to 'e200/ptb_char_lstm.pt'...
[epoch = 113] min_validation_perplexity =     3.56, min_validation_loss =  1.27


[batch =  100] train_perplexity =     3.70, train_loss =  1.31, learning_rate = 0.07812500


[batch =  200] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch =  300] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch =  400] train_perplexity =     3.34, train_loss =  1.20, learning_rate = 0.07812500


[batch =  500] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch =  600] train_perplexity =     3.30, train_loss =  1.19, learning_rate = 0.07812500


[batch =  700] train_perplexity =     3.32, train_loss =  1.20, learning_rate = 0.07812500


[batch =  800] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch =  900] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 1000] train_perplexity =     3.51, train_loss =  1.25, learning_rate = 0.07812500


[batch = 1100] train_perplexity =     3.58, train_loss =  1.27, learning_rate = 0.07812500


[batch = 1200] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 1300] train_perplexity =     3.51, train_loss =  1.25, learning_rate = 0.07812500


[batch = 1400] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1500] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1600] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 1700] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1800] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1900] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2000] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2100] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2200] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2300] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2400] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2500] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2600] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2700] train_perplexity =     3.54, train_loss =  1.27, learning_rate = 0.07812500


[batch = 2800] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2900] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.07812500


[batch = 3000] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 3100] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3200] train_perplexity =     3.59, train_loss =  1.28, learning_rate = 0.07812500


[batch = 3300] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3400] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3500] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3600] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3700] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3800] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3900] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4000] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4100] train_perplexity =     3.34, train_loss =  1.20, learning_rate = 0.07812500


[batch = 4200] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4300] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4400] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4500] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4600] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4700] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4800] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 4900] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5000] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 5100] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5200] train_perplexity =     3.51, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5300] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5400] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5500] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5600] train_perplexity =     3.51, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5700] train_perplexity =     3.31, train_loss =  1.20, learning_rate = 0.07812500


[batch = 5800] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5900] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 6000] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 6100] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6200] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6300] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6400] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6500] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch = 6600] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6700] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6800] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 6900] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.07812500


[epoch = 114] validation_perplexity =     3.56, validation_loss =  1.27

Saved model to 'e200/ptb_char_lstm.pt'...
[epoch = 114] min_validation_perplexity =     3.56, min_validation_loss =  1.27


[batch =  100] train_perplexity =     3.70, train_loss =  1.31, learning_rate = 0.07812500


[batch =  200] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch =  300] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch =  400] train_perplexity =     3.34, train_loss =  1.20, learning_rate = 0.07812500


[batch =  500] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch =  600] train_perplexity =     3.30, train_loss =  1.19, learning_rate = 0.07812500


[batch =  700] train_perplexity =     3.32, train_loss =  1.20, learning_rate = 0.07812500


[batch =  800] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch =  900] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 1000] train_perplexity =     3.51, train_loss =  1.25, learning_rate = 0.07812500


[batch = 1100] train_perplexity =     3.58, train_loss =  1.27, learning_rate = 0.07812500


[batch = 1200] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 1300] train_perplexity =     3.51, train_loss =  1.25, learning_rate = 0.07812500


[batch = 1400] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1500] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1600] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 1700] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1800] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1900] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2000] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2100] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2200] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2300] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2400] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2500] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2600] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2700] train_perplexity =     3.54, train_loss =  1.27, learning_rate = 0.07812500


[batch = 2800] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2900] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.07812500


[batch = 3000] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 3100] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3200] train_perplexity =     3.59, train_loss =  1.28, learning_rate = 0.07812500


[batch = 3300] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3400] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3500] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3600] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3700] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3800] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3900] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4000] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4100] train_perplexity =     3.34, train_loss =  1.20, learning_rate = 0.07812500


[batch = 4200] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4300] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4400] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4500] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4600] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4700] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4800] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 4900] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5000] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 5100] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5200] train_perplexity =     3.51, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5300] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5400] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5500] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5600] train_perplexity =     3.51, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5700] train_perplexity =     3.31, train_loss =  1.20, learning_rate = 0.07812500


[batch = 5800] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5900] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 6000] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 6100] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6200] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6300] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6400] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6500] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch = 6600] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6700] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6800] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 6900] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.07812500


[epoch = 115] validation_perplexity =     3.56, validation_loss =  1.27

Saved model to 'e200/ptb_char_lstm.pt'...
[epoch = 115] min_validation_perplexity =     3.56, min_validation_loss =  1.27


[batch =  100] train_perplexity =     3.70, train_loss =  1.31, learning_rate = 0.07812500


[batch =  200] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch =  300] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch =  400] train_perplexity =     3.34, train_loss =  1.20, learning_rate = 0.07812500


[batch =  500] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch =  600] train_perplexity =     3.30, train_loss =  1.19, learning_rate = 0.07812500


[batch =  700] train_perplexity =     3.32, train_loss =  1.20, learning_rate = 0.07812500


[batch =  800] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch =  900] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 1000] train_perplexity =     3.51, train_loss =  1.25, learning_rate = 0.07812500


[batch = 1100] train_perplexity =     3.58, train_loss =  1.27, learning_rate = 0.07812500


[batch = 1200] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 1300] train_perplexity =     3.51, train_loss =  1.25, learning_rate = 0.07812500


[batch = 1400] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1500] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1600] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 1700] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1800] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1900] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2000] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2100] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2200] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2300] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2400] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2500] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2600] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2700] train_perplexity =     3.54, train_loss =  1.27, learning_rate = 0.07812500


[batch = 2800] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2900] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.07812500


[batch = 3000] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 3100] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3200] train_perplexity =     3.59, train_loss =  1.28, learning_rate = 0.07812500


[batch = 3300] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3400] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3500] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3600] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3700] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3800] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3900] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4000] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4100] train_perplexity =     3.34, train_loss =  1.20, learning_rate = 0.07812500


[batch = 4200] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4300] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4400] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4500] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4600] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4700] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4800] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 4900] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5000] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 5100] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5200] train_perplexity =     3.51, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5300] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5400] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5500] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5600] train_perplexity =     3.51, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5700] train_perplexity =     3.31, train_loss =  1.20, learning_rate = 0.07812500


[batch = 5800] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5900] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 6000] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 6100] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6200] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6300] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6400] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6500] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch = 6600] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6700] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6800] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 6900] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.07812500


[epoch = 116] validation_perplexity =     3.56, validation_loss =  1.27

Saved model to 'e200/ptb_char_lstm.pt'...
[epoch = 116] min_validation_perplexity =     3.56, min_validation_loss =  1.27


[batch =  100] train_perplexity =     3.70, train_loss =  1.31, learning_rate = 0.07812500


[batch =  200] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch =  300] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch =  400] train_perplexity =     3.34, train_loss =  1.20, learning_rate = 0.07812500


[batch =  500] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch =  600] train_perplexity =     3.30, train_loss =  1.19, learning_rate = 0.07812500


[batch =  700] train_perplexity =     3.32, train_loss =  1.20, learning_rate = 0.07812500


[batch =  800] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch =  900] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 1000] train_perplexity =     3.51, train_loss =  1.25, learning_rate = 0.07812500


[batch = 1100] train_perplexity =     3.58, train_loss =  1.27, learning_rate = 0.07812500


[batch = 1200] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 1300] train_perplexity =     3.51, train_loss =  1.25, learning_rate = 0.07812500


[batch = 1400] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1500] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1600] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 1700] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1800] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1900] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2000] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2100] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2200] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2300] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2400] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2500] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2600] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2700] train_perplexity =     3.54, train_loss =  1.27, learning_rate = 0.07812500


[batch = 2800] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2900] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.07812500


[batch = 3000] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 3100] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3200] train_perplexity =     3.59, train_loss =  1.28, learning_rate = 0.07812500


[batch = 3300] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3400] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3500] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3600] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3700] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3800] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3900] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4000] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4100] train_perplexity =     3.34, train_loss =  1.20, learning_rate = 0.07812500


[batch = 4200] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4300] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4400] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4500] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4600] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4700] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4800] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 4900] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5000] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 5100] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5200] train_perplexity =     3.51, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5300] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5400] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5500] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5600] train_perplexity =     3.51, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5700] train_perplexity =     3.31, train_loss =  1.20, learning_rate = 0.07812500


[batch = 5800] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5900] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 6000] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 6100] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6200] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6300] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6400] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6500] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch = 6600] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6700] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6800] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 6900] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.07812500


[epoch = 117] validation_perplexity =     3.56, validation_loss =  1.27

Saved model to 'e200/ptb_char_lstm.pt'...
[epoch = 117] min_validation_perplexity =     3.56, min_validation_loss =  1.27


[batch =  100] train_perplexity =     3.70, train_loss =  1.31, learning_rate = 0.07812500


[batch =  200] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch =  300] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch =  400] train_perplexity =     3.34, train_loss =  1.20, learning_rate = 0.07812500


[batch =  500] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch =  600] train_perplexity =     3.30, train_loss =  1.19, learning_rate = 0.07812500


[batch =  700] train_perplexity =     3.32, train_loss =  1.20, learning_rate = 0.07812500


[batch =  800] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch =  900] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 1000] train_perplexity =     3.51, train_loss =  1.25, learning_rate = 0.07812500


[batch = 1100] train_perplexity =     3.58, train_loss =  1.27, learning_rate = 0.07812500


[batch = 1200] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 1300] train_perplexity =     3.51, train_loss =  1.25, learning_rate = 0.07812500


[batch = 1400] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1500] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1600] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 1700] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1800] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1900] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2000] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2100] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2200] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2300] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2400] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2500] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2600] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2700] train_perplexity =     3.54, train_loss =  1.27, learning_rate = 0.07812500


[batch = 2800] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2900] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.07812500


[batch = 3000] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 3100] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3200] train_perplexity =     3.59, train_loss =  1.28, learning_rate = 0.07812500


[batch = 3300] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3400] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3500] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3600] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3700] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3800] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3900] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4000] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4100] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch = 4200] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4300] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4400] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4500] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4600] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4700] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4800] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 4900] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5000] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 5100] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5200] train_perplexity =     3.51, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5300] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5400] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5500] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5600] train_perplexity =     3.51, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5700] train_perplexity =     3.31, train_loss =  1.20, learning_rate = 0.07812500


[batch = 5800] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5900] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 6000] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 6100] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6200] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6300] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6400] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6500] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch = 6600] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6700] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6800] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 6900] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.07812500


[epoch = 118] validation_perplexity =     3.56, validation_loss =  1.27

Saved model to 'e200/ptb_char_lstm.pt'...
[epoch = 118] min_validation_perplexity =     3.56, min_validation_loss =  1.27


[batch =  100] train_perplexity =     3.70, train_loss =  1.31, learning_rate = 0.07812500


[batch =  200] train_perplexity =     3.51, train_loss =  1.25, learning_rate = 0.07812500


[batch =  300] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch =  400] train_perplexity =     3.34, train_loss =  1.20, learning_rate = 0.07812500


[batch =  500] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch =  600] train_perplexity =     3.30, train_loss =  1.19, learning_rate = 0.07812500


[batch =  700] train_perplexity =     3.32, train_loss =  1.20, learning_rate = 0.07812500


[batch =  800] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch =  900] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 1000] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 1100] train_perplexity =     3.58, train_loss =  1.27, learning_rate = 0.07812500


[batch = 1200] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 1300] train_perplexity =     3.51, train_loss =  1.25, learning_rate = 0.07812500


[batch = 1400] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1500] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1600] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 1700] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1800] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1900] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2000] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2100] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2200] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2300] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2400] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2500] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2600] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2700] train_perplexity =     3.54, train_loss =  1.27, learning_rate = 0.07812500


[batch = 2800] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2900] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.07812500


[batch = 3000] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 3100] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3200] train_perplexity =     3.59, train_loss =  1.28, learning_rate = 0.07812500


[batch = 3300] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3400] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3500] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3600] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3700] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3800] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3900] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4000] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4100] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch = 4200] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4300] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4400] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4500] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4600] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4700] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4800] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 4900] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5000] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 5100] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5200] train_perplexity =     3.51, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5300] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5400] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5500] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5600] train_perplexity =     3.51, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5700] train_perplexity =     3.31, train_loss =  1.20, learning_rate = 0.07812500


[batch = 5800] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5900] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 6000] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 6100] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6200] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6300] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6400] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6500] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch = 6600] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6700] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6800] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 6900] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[epoch = 119] validation_perplexity =     3.56, validation_loss =  1.27

Saved model to 'e200/ptb_char_lstm.pt'...
[epoch = 119] min_validation_perplexity =     3.56, min_validation_loss =  1.27


[batch =  100] train_perplexity =     3.70, train_loss =  1.31, learning_rate = 0.07812500


[batch =  200] train_perplexity =     3.51, train_loss =  1.25, learning_rate = 0.07812500


[batch =  300] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch =  400] train_perplexity =     3.34, train_loss =  1.20, learning_rate = 0.07812500


[batch =  500] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch =  600] train_perplexity =     3.30, train_loss =  1.19, learning_rate = 0.07812500


[batch =  700] train_perplexity =     3.32, train_loss =  1.20, learning_rate = 0.07812500


[batch =  800] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch =  900] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 1000] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 1100] train_perplexity =     3.58, train_loss =  1.27, learning_rate = 0.07812500


[batch = 1200] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 1300] train_perplexity =     3.51, train_loss =  1.25, learning_rate = 0.07812500


[batch = 1400] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1500] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1600] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 1700] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1800] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1900] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2000] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2100] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2200] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2300] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2400] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2500] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2600] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2700] train_perplexity =     3.54, train_loss =  1.27, learning_rate = 0.07812500


[batch = 2800] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2900] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.07812500


[batch = 3000] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 3100] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3200] train_perplexity =     3.59, train_loss =  1.28, learning_rate = 0.07812500


[batch = 3300] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3400] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3500] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3600] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3700] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3800] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3900] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4000] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4100] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch = 4200] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4300] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4400] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4500] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4600] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4700] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4800] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 4900] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5000] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 5100] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5200] train_perplexity =     3.51, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5300] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5400] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5500] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5600] train_perplexity =     3.51, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5700] train_perplexity =     3.31, train_loss =  1.20, learning_rate = 0.07812500


[batch = 5800] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5900] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 6000] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 6100] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6200] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6300] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6400] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6500] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch = 6600] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6700] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6800] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 6900] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[epoch = 120] validation_perplexity =     3.56, validation_loss =  1.27

Saved model to 'e200/ptb_char_lstm.pt'...
[epoch = 120] min_validation_perplexity =     3.56, min_validation_loss =  1.27


[batch =  100] train_perplexity =     3.70, train_loss =  1.31, learning_rate = 0.07812500


[batch =  200] train_perplexity =     3.51, train_loss =  1.25, learning_rate = 0.07812500


[batch =  300] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch =  400] train_perplexity =     3.34, train_loss =  1.20, learning_rate = 0.07812500


[batch =  500] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch =  600] train_perplexity =     3.29, train_loss =  1.19, learning_rate = 0.07812500


[batch =  700] train_perplexity =     3.32, train_loss =  1.20, learning_rate = 0.07812500


[batch =  800] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch =  900] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 1000] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 1100] train_perplexity =     3.58, train_loss =  1.27, learning_rate = 0.07812500


[batch = 1200] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 1300] train_perplexity =     3.51, train_loss =  1.25, learning_rate = 0.07812500


[batch = 1400] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1500] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1600] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 1700] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1800] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1900] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2000] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2100] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2200] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2300] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2400] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2500] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2600] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2700] train_perplexity =     3.54, train_loss =  1.27, learning_rate = 0.07812500


[batch = 2800] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2900] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.07812500


[batch = 3000] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 3100] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3200] train_perplexity =     3.59, train_loss =  1.28, learning_rate = 0.07812500


[batch = 3300] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3400] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3500] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3600] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3700] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3800] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3900] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4000] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4100] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch = 4200] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4300] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4400] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4500] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4600] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4700] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4800] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 4900] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5000] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 5100] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5200] train_perplexity =     3.51, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5300] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5400] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5500] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5600] train_perplexity =     3.51, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5700] train_perplexity =     3.31, train_loss =  1.20, learning_rate = 0.07812500


[batch = 5800] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5900] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 6000] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 6100] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6200] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6300] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6400] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6500] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch = 6600] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6700] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6800] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 6900] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[epoch = 121] validation_perplexity =     3.56, validation_loss =  1.27

Saved model to 'e200/ptb_char_lstm.pt'...
[epoch = 121] min_validation_perplexity =     3.56, min_validation_loss =  1.27


[batch =  100] train_perplexity =     3.70, train_loss =  1.31, learning_rate = 0.07812500


[batch =  200] train_perplexity =     3.51, train_loss =  1.25, learning_rate = 0.07812500


[batch =  300] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch =  400] train_perplexity =     3.34, train_loss =  1.20, learning_rate = 0.07812500


[batch =  500] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch =  600] train_perplexity =     3.29, train_loss =  1.19, learning_rate = 0.07812500


[batch =  700] train_perplexity =     3.32, train_loss =  1.20, learning_rate = 0.07812500


[batch =  800] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch =  900] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 1000] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 1100] train_perplexity =     3.58, train_loss =  1.27, learning_rate = 0.07812500


[batch = 1200] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 1300] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 1400] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1500] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1600] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 1700] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1800] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1900] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2000] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2100] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2200] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2300] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2400] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2500] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2600] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2700] train_perplexity =     3.54, train_loss =  1.27, learning_rate = 0.07812500


[batch = 2800] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2900] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 3000] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 3100] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3200] train_perplexity =     3.59, train_loss =  1.28, learning_rate = 0.07812500


[batch = 3300] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3400] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3500] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3600] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3700] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3800] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3900] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4000] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4100] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch = 4200] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4300] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4400] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4500] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4600] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4700] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4800] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 4900] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5000] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 5100] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5200] train_perplexity =     3.51, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5300] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5400] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5500] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5600] train_perplexity =     3.51, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5700] train_perplexity =     3.31, train_loss =  1.20, learning_rate = 0.07812500


[batch = 5800] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5900] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 6000] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 6100] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6200] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6300] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6400] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6500] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch = 6600] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6700] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6800] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 6900] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[epoch = 122] validation_perplexity =     3.55, validation_loss =  1.27

Saved model to 'e200/ptb_char_lstm.pt'...
[epoch = 122] min_validation_perplexity =     3.55, min_validation_loss =  1.27


[batch =  100] train_perplexity =     3.70, train_loss =  1.31, learning_rate = 0.07812500


[batch =  200] train_perplexity =     3.51, train_loss =  1.25, learning_rate = 0.07812500


[batch =  300] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch =  400] train_perplexity =     3.34, train_loss =  1.20, learning_rate = 0.07812500


[batch =  500] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch =  600] train_perplexity =     3.29, train_loss =  1.19, learning_rate = 0.07812500


[batch =  700] train_perplexity =     3.32, train_loss =  1.20, learning_rate = 0.07812500


[batch =  800] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch =  900] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 1000] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 1100] train_perplexity =     3.57, train_loss =  1.27, learning_rate = 0.07812500


[batch = 1200] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 1300] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 1400] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1500] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1600] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 1700] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1800] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1900] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2000] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2100] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2200] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2300] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2400] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2500] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2600] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2700] train_perplexity =     3.54, train_loss =  1.27, learning_rate = 0.07812500


[batch = 2800] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2900] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 3000] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 3100] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3200] train_perplexity =     3.59, train_loss =  1.28, learning_rate = 0.07812500


[batch = 3300] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3400] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3500] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3600] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3700] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3800] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3900] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4000] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4100] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch = 4200] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4300] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4400] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4500] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4600] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4700] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4800] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 4900] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5000] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 5100] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5200] train_perplexity =     3.51, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5300] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5400] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5500] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5600] train_perplexity =     3.51, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5700] train_perplexity =     3.31, train_loss =  1.20, learning_rate = 0.07812500


[batch = 5800] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5900] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 6000] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 6100] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6200] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6300] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6400] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6500] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch = 6600] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6700] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6800] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 6900] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[epoch = 123] validation_perplexity =     3.55, validation_loss =  1.27

Saved model to 'e200/ptb_char_lstm.pt'...
[epoch = 123] min_validation_perplexity =     3.55, min_validation_loss =  1.27


[batch =  100] train_perplexity =     3.70, train_loss =  1.31, learning_rate = 0.07812500


[batch =  200] train_perplexity =     3.51, train_loss =  1.25, learning_rate = 0.07812500


[batch =  300] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch =  400] train_perplexity =     3.34, train_loss =  1.20, learning_rate = 0.07812500


[batch =  500] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch =  600] train_perplexity =     3.29, train_loss =  1.19, learning_rate = 0.07812500


[batch =  700] train_perplexity =     3.32, train_loss =  1.20, learning_rate = 0.07812500


[batch =  800] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch =  900] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 1000] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 1100] train_perplexity =     3.57, train_loss =  1.27, learning_rate = 0.07812500


[batch = 1200] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 1300] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 1400] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1500] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1600] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 1700] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1800] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1900] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2000] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2100] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2200] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2300] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2400] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2500] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2600] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2700] train_perplexity =     3.54, train_loss =  1.27, learning_rate = 0.07812500


[batch = 2800] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2900] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 3000] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 3100] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3200] train_perplexity =     3.59, train_loss =  1.28, learning_rate = 0.07812500


[batch = 3300] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3400] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3500] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3600] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3700] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3800] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3900] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4000] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4100] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch = 4200] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4300] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4400] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4500] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4600] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4700] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4800] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 4900] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5000] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 5100] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5200] train_perplexity =     3.51, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5300] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5400] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5500] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5600] train_perplexity =     3.51, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5700] train_perplexity =     3.31, train_loss =  1.20, learning_rate = 0.07812500


[batch = 5800] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5900] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 6000] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 6100] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6200] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6300] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6400] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6500] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch = 6600] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6700] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6800] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 6900] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[epoch = 124] validation_perplexity =     3.55, validation_loss =  1.27

Saved model to 'e200/ptb_char_lstm.pt'...
[epoch = 124] min_validation_perplexity =     3.55, min_validation_loss =  1.27


[batch =  100] train_perplexity =     3.70, train_loss =  1.31, learning_rate = 0.07812500


[batch =  200] train_perplexity =     3.51, train_loss =  1.25, learning_rate = 0.07812500


[batch =  300] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch =  400] train_perplexity =     3.34, train_loss =  1.20, learning_rate = 0.07812500


[batch =  500] train_perplexity =     3.32, train_loss =  1.20, learning_rate = 0.07812500


[batch =  600] train_perplexity =     3.29, train_loss =  1.19, learning_rate = 0.07812500


[batch =  700] train_perplexity =     3.32, train_loss =  1.20, learning_rate = 0.07812500


[batch =  800] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch =  900] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 1000] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 1100] train_perplexity =     3.57, train_loss =  1.27, learning_rate = 0.07812500


[batch = 1200] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 1300] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 1400] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1500] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1600] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 1700] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1800] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1900] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2000] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2100] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2200] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2300] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2400] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2500] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2600] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2700] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2800] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2900] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 3000] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 3100] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3200] train_perplexity =     3.59, train_loss =  1.28, learning_rate = 0.07812500


[batch = 3300] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3400] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3500] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3600] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3700] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3800] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3900] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4000] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4100] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch = 4200] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4300] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4400] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4500] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4600] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4700] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4800] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 4900] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5000] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 5100] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5200] train_perplexity =     3.51, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5300] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5400] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5500] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5600] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5700] train_perplexity =     3.31, train_loss =  1.20, learning_rate = 0.07812500


[batch = 5800] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5900] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 6000] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 6100] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6200] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6300] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6400] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6500] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch = 6600] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6700] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6800] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6900] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[epoch = 125] validation_perplexity =     3.55, validation_loss =  1.27

Saved model to 'e200/ptb_char_lstm.pt'...
[epoch = 125] min_validation_perplexity =     3.55, min_validation_loss =  1.27


[batch =  100] train_perplexity =     3.70, train_loss =  1.31, learning_rate = 0.07812500


[batch =  200] train_perplexity =     3.51, train_loss =  1.25, learning_rate = 0.07812500


[batch =  300] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch =  400] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch =  500] train_perplexity =     3.32, train_loss =  1.20, learning_rate = 0.07812500


[batch =  600] train_perplexity =     3.29, train_loss =  1.19, learning_rate = 0.07812500


[batch =  700] train_perplexity =     3.32, train_loss =  1.20, learning_rate = 0.07812500


[batch =  800] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch =  900] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 1000] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 1100] train_perplexity =     3.57, train_loss =  1.27, learning_rate = 0.07812500


[batch = 1200] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 1300] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 1400] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1500] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1600] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 1700] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1800] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1900] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2000] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2100] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2200] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2300] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2400] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2500] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2600] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2700] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2800] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2900] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 3000] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 3100] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3200] train_perplexity =     3.59, train_loss =  1.28, learning_rate = 0.07812500


[batch = 3300] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3400] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3500] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3600] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3700] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3800] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3900] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4000] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4100] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch = 4200] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4300] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4400] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4500] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4600] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4700] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4800] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 4900] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5000] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 5100] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5200] train_perplexity =     3.51, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5300] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5400] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5500] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5600] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5700] train_perplexity =     3.31, train_loss =  1.20, learning_rate = 0.07812500


[batch = 5800] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5900] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 6000] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 6100] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6200] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6300] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6400] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6500] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch = 6600] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6700] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6800] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6900] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[epoch = 126] validation_perplexity =     3.55, validation_loss =  1.27

Saved model to 'e200/ptb_char_lstm.pt'...
[epoch = 126] min_validation_perplexity =     3.55, min_validation_loss =  1.27


[batch =  100] train_perplexity =     3.70, train_loss =  1.31, learning_rate = 0.07812500


[batch =  200] train_perplexity =     3.51, train_loss =  1.25, learning_rate = 0.07812500


[batch =  300] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch =  400] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch =  500] train_perplexity =     3.32, train_loss =  1.20, learning_rate = 0.07812500


[batch =  600] train_perplexity =     3.29, train_loss =  1.19, learning_rate = 0.07812500


[batch =  700] train_perplexity =     3.32, train_loss =  1.20, learning_rate = 0.07812500


[batch =  800] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch =  900] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 1000] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 1100] train_perplexity =     3.57, train_loss =  1.27, learning_rate = 0.07812500


[batch = 1200] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 1300] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 1400] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1500] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1600] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 1700] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1800] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1900] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2000] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2100] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2200] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2300] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2400] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2500] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2600] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2700] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2800] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2900] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 3000] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 3100] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3200] train_perplexity =     3.59, train_loss =  1.28, learning_rate = 0.07812500


[batch = 3300] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3400] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3500] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3600] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3700] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3800] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3900] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4000] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4100] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch = 4200] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4300] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4400] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4500] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4600] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4700] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4800] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 4900] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5000] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 5100] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5200] train_perplexity =     3.51, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5300] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5400] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5500] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5600] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5700] train_perplexity =     3.31, train_loss =  1.20, learning_rate = 0.07812500


[batch = 5800] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5900] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 6000] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 6100] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6200] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6300] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6400] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6500] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch = 6600] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6700] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6800] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6900] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[epoch = 127] validation_perplexity =     3.55, validation_loss =  1.27

Saved model to 'e200/ptb_char_lstm.pt'...
[epoch = 127] min_validation_perplexity =     3.55, min_validation_loss =  1.27


[batch =  100] train_perplexity =     3.69, train_loss =  1.31, learning_rate = 0.07812500


[batch =  200] train_perplexity =     3.51, train_loss =  1.25, learning_rate = 0.07812500


[batch =  300] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch =  400] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch =  500] train_perplexity =     3.32, train_loss =  1.20, learning_rate = 0.07812500


[batch =  600] train_perplexity =     3.29, train_loss =  1.19, learning_rate = 0.07812500


[batch =  700] train_perplexity =     3.32, train_loss =  1.20, learning_rate = 0.07812500


[batch =  800] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch =  900] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 1000] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 1100] train_perplexity =     3.57, train_loss =  1.27, learning_rate = 0.07812500


[batch = 1200] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 1300] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 1400] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1500] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1600] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 1700] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1800] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1900] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2000] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2100] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2200] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2300] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2400] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2500] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2600] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2700] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2800] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2900] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 3000] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 3100] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3200] train_perplexity =     3.59, train_loss =  1.28, learning_rate = 0.07812500


[batch = 3300] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3400] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3500] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3600] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3700] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3800] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3900] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4000] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4100] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch = 4200] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4300] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4400] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4500] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4600] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4700] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4800] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 4900] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5000] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 5100] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5200] train_perplexity =     3.51, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5300] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5400] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5500] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5600] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5700] train_perplexity =     3.31, train_loss =  1.20, learning_rate = 0.07812500


[batch = 5800] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5900] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 6000] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 6100] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6200] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6300] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6400] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6500] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch = 6600] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6700] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6800] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6900] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[epoch = 128] validation_perplexity =     3.55, validation_loss =  1.27

Saved model to 'e200/ptb_char_lstm.pt'...
[epoch = 128] min_validation_perplexity =     3.55, min_validation_loss =  1.27


[batch =  100] train_perplexity =     3.69, train_loss =  1.31, learning_rate = 0.07812500


[batch =  200] train_perplexity =     3.51, train_loss =  1.25, learning_rate = 0.07812500


[batch =  300] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch =  400] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch =  500] train_perplexity =     3.32, train_loss =  1.20, learning_rate = 0.07812500


[batch =  600] train_perplexity =     3.29, train_loss =  1.19, learning_rate = 0.07812500


[batch =  700] train_perplexity =     3.32, train_loss =  1.20, learning_rate = 0.07812500


[batch =  800] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch =  900] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 1000] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 1100] train_perplexity =     3.57, train_loss =  1.27, learning_rate = 0.07812500


[batch = 1200] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 1300] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 1400] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1500] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1600] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 1700] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1800] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1900] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2000] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2100] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2200] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2300] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2400] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2500] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2600] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2700] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2800] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2900] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 3000] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 3100] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3200] train_perplexity =     3.59, train_loss =  1.28, learning_rate = 0.07812500


[batch = 3300] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3400] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3500] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3600] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3700] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3800] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3900] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4000] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4100] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch = 4200] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4300] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4400] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4500] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4600] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4700] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4800] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 4900] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5000] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 5100] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5200] train_perplexity =     3.51, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5300] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5400] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5500] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5600] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5700] train_perplexity =     3.31, train_loss =  1.20, learning_rate = 0.07812500


[batch = 5800] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5900] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 6000] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 6100] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6200] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6300] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6400] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6500] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch = 6600] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6700] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6800] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6900] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[epoch = 129] validation_perplexity =     3.55, validation_loss =  1.27

Saved model to 'e200/ptb_char_lstm.pt'...
[epoch = 129] min_validation_perplexity =     3.55, min_validation_loss =  1.27


[batch =  100] train_perplexity =     3.69, train_loss =  1.31, learning_rate = 0.07812500


[batch =  200] train_perplexity =     3.51, train_loss =  1.25, learning_rate = 0.07812500


[batch =  300] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch =  400] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch =  500] train_perplexity =     3.32, train_loss =  1.20, learning_rate = 0.07812500


[batch =  600] train_perplexity =     3.29, train_loss =  1.19, learning_rate = 0.07812500


[batch =  700] train_perplexity =     3.32, train_loss =  1.20, learning_rate = 0.07812500


[batch =  800] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch =  900] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 1000] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 1100] train_perplexity =     3.57, train_loss =  1.27, learning_rate = 0.07812500


[batch = 1200] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 1300] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 1400] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1500] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1600] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 1700] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1800] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1900] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2000] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2100] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2200] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2300] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2400] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2500] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2600] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2700] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2800] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2900] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 3000] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 3100] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3200] train_perplexity =     3.59, train_loss =  1.28, learning_rate = 0.07812500


[batch = 3300] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3400] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3500] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3600] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3700] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3800] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3900] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4000] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4100] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch = 4200] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4300] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4400] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4500] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4600] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4700] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4800] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 4900] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5000] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 5100] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5200] train_perplexity =     3.51, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5300] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5400] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5500] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5600] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5700] train_perplexity =     3.31, train_loss =  1.20, learning_rate = 0.07812500


[batch = 5800] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5900] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 6000] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 6100] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6200] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6300] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6400] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6500] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch = 6600] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6700] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6800] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6900] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[epoch = 130] validation_perplexity =     3.55, validation_loss =  1.27

Saved model to 'e200/ptb_char_lstm.pt'...
[epoch = 130] min_validation_perplexity =     3.55, min_validation_loss =  1.27


[batch =  100] train_perplexity =     3.69, train_loss =  1.31, learning_rate = 0.07812500


[batch =  200] train_perplexity =     3.51, train_loss =  1.25, learning_rate = 0.07812500


[batch =  300] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch =  400] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch =  500] train_perplexity =     3.32, train_loss =  1.20, learning_rate = 0.07812500


[batch =  600] train_perplexity =     3.29, train_loss =  1.19, learning_rate = 0.07812500


[batch =  700] train_perplexity =     3.32, train_loss =  1.20, learning_rate = 0.07812500


[batch =  800] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch =  900] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 1000] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 1100] train_perplexity =     3.57, train_loss =  1.27, learning_rate = 0.07812500


[batch = 1200] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 1300] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 1400] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1500] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1600] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 1700] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1800] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1900] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2000] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2100] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2200] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2300] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2400] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2500] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2600] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2700] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2800] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2900] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 3000] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 3100] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3200] train_perplexity =     3.59, train_loss =  1.28, learning_rate = 0.07812500


[batch = 3300] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3400] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3500] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3600] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3700] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3800] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3900] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4000] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4100] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch = 4200] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4300] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4400] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4500] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4600] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4700] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4800] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 4900] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5000] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 5100] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5200] train_perplexity =     3.51, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5300] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5400] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5500] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5600] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5700] train_perplexity =     3.31, train_loss =  1.20, learning_rate = 0.07812500


[batch = 5800] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5900] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 6000] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 6100] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6200] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6300] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6400] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6500] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch = 6600] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6700] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6800] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6900] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[epoch = 131] validation_perplexity =     3.55, validation_loss =  1.27

Saved model to 'e200/ptb_char_lstm.pt'...
[epoch = 131] min_validation_perplexity =     3.55, min_validation_loss =  1.27


[batch =  100] train_perplexity =     3.69, train_loss =  1.31, learning_rate = 0.07812500


[batch =  200] train_perplexity =     3.51, train_loss =  1.25, learning_rate = 0.07812500


[batch =  300] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch =  400] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch =  500] train_perplexity =     3.32, train_loss =  1.20, learning_rate = 0.07812500


[batch =  600] train_perplexity =     3.29, train_loss =  1.19, learning_rate = 0.07812500


[batch =  700] train_perplexity =     3.32, train_loss =  1.20, learning_rate = 0.07812500


[batch =  800] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch =  900] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 1000] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 1100] train_perplexity =     3.57, train_loss =  1.27, learning_rate = 0.07812500


[batch = 1200] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 1300] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 1400] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1500] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1600] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 1700] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1800] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1900] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2000] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2100] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2200] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2300] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2400] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2500] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2600] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2700] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2800] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2900] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 3000] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 3100] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3200] train_perplexity =     3.59, train_loss =  1.28, learning_rate = 0.07812500


[batch = 3300] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3400] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3500] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3600] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3700] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3800] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3900] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4000] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4100] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch = 4200] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4300] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4400] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4500] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4600] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4700] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4800] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 4900] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5000] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 5100] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5200] train_perplexity =     3.51, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5300] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5400] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5500] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5600] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5700] train_perplexity =     3.31, train_loss =  1.20, learning_rate = 0.07812500


[batch = 5800] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5900] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 6000] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 6100] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6200] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6300] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6400] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6500] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch = 6600] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6700] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6800] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6900] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[epoch = 132] validation_perplexity =     3.55, validation_loss =  1.27

Saved model to 'e200/ptb_char_lstm.pt'...
[epoch = 132] min_validation_perplexity =     3.55, min_validation_loss =  1.27


[batch =  100] train_perplexity =     3.69, train_loss =  1.31, learning_rate = 0.07812500


[batch =  200] train_perplexity =     3.51, train_loss =  1.25, learning_rate = 0.07812500


[batch =  300] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch =  400] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch =  500] train_perplexity =     3.32, train_loss =  1.20, learning_rate = 0.07812500


[batch =  600] train_perplexity =     3.29, train_loss =  1.19, learning_rate = 0.07812500


[batch =  700] train_perplexity =     3.32, train_loss =  1.20, learning_rate = 0.07812500


[batch =  800] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch =  900] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 1000] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 1100] train_perplexity =     3.57, train_loss =  1.27, learning_rate = 0.07812500


[batch = 1200] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 1300] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 1400] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1500] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1600] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 1700] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1800] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1900] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2000] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2100] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2200] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2300] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2400] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2500] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2600] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2700] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2800] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2900] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 3000] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 3100] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3200] train_perplexity =     3.59, train_loss =  1.28, learning_rate = 0.07812500


[batch = 3300] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3400] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3500] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3600] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3700] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3800] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3900] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4000] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4100] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch = 4200] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4300] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4400] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4500] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4600] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4700] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4800] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 4900] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5000] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 5100] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5200] train_perplexity =     3.51, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5300] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5400] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5500] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5600] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5700] train_perplexity =     3.30, train_loss =  1.20, learning_rate = 0.07812500


[batch = 5800] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5900] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 6000] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 6100] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6200] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6300] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6400] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6500] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch = 6600] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6700] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6800] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6900] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[epoch = 133] validation_perplexity =     3.55, validation_loss =  1.27

Saved model to 'e200/ptb_char_lstm.pt'...
[epoch = 133] min_validation_perplexity =     3.55, min_validation_loss =  1.27


[batch =  100] train_perplexity =     3.69, train_loss =  1.31, learning_rate = 0.07812500


[batch =  200] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch =  300] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch =  400] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch =  500] train_perplexity =     3.32, train_loss =  1.20, learning_rate = 0.07812500


[batch =  600] train_perplexity =     3.29, train_loss =  1.19, learning_rate = 0.07812500


[batch =  700] train_perplexity =     3.32, train_loss =  1.20, learning_rate = 0.07812500


[batch =  800] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch =  900] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 1000] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 1100] train_perplexity =     3.57, train_loss =  1.27, learning_rate = 0.07812500


[batch = 1200] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 1300] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 1400] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1500] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1600] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 1700] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1800] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1900] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2000] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2100] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2200] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2300] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2400] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2500] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2600] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2700] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2800] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2900] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 3000] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 3100] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3200] train_perplexity =     3.59, train_loss =  1.28, learning_rate = 0.07812500


[batch = 3300] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3400] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3500] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3600] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3700] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3800] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3900] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4000] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4100] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch = 4200] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4300] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4400] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4500] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4600] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4700] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4800] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 4900] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5000] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 5100] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5200] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5300] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5400] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5500] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5600] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5700] train_perplexity =     3.30, train_loss =  1.20, learning_rate = 0.07812500


[batch = 5800] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5900] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 6000] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 6100] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6200] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6300] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6400] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6500] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch = 6600] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6700] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6800] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6900] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[epoch = 134] validation_perplexity =     3.55, validation_loss =  1.27

Saved model to 'e200/ptb_char_lstm.pt'...
[epoch = 134] min_validation_perplexity =     3.55, min_validation_loss =  1.27


[batch =  100] train_perplexity =     3.69, train_loss =  1.31, learning_rate = 0.07812500


[batch =  200] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch =  300] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch =  400] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch =  500] train_perplexity =     3.32, train_loss =  1.20, learning_rate = 0.07812500


[batch =  600] train_perplexity =     3.29, train_loss =  1.19, learning_rate = 0.07812500


[batch =  700] train_perplexity =     3.32, train_loss =  1.20, learning_rate = 0.07812500


[batch =  800] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch =  900] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 1000] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 1100] train_perplexity =     3.57, train_loss =  1.27, learning_rate = 0.07812500


[batch = 1200] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 1300] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 1400] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1500] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1600] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 1700] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1800] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1900] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2000] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2100] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2200] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2300] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2400] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2500] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2600] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2700] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2800] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2900] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 3000] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 3100] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3200] train_perplexity =     3.59, train_loss =  1.28, learning_rate = 0.07812500


[batch = 3300] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3400] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3500] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3600] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3700] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3800] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3900] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4000] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4100] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch = 4200] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4300] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4400] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4500] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4600] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4700] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4800] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 4900] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5000] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 5100] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5200] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5300] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5400] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5500] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5600] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5700] train_perplexity =     3.30, train_loss =  1.20, learning_rate = 0.07812500


[batch = 5800] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5900] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 6000] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 6100] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6200] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6300] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6400] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6500] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch = 6600] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6700] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6800] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6900] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[epoch = 135] validation_perplexity =     3.55, validation_loss =  1.27

Saved model to 'e200/ptb_char_lstm.pt'...
[epoch = 135] min_validation_perplexity =     3.55, min_validation_loss =  1.27


[batch =  100] train_perplexity =     3.69, train_loss =  1.31, learning_rate = 0.07812500


[batch =  200] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch =  300] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch =  400] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch =  500] train_perplexity =     3.32, train_loss =  1.20, learning_rate = 0.07812500


[batch =  600] train_perplexity =     3.29, train_loss =  1.19, learning_rate = 0.07812500


[batch =  700] train_perplexity =     3.32, train_loss =  1.20, learning_rate = 0.07812500


[batch =  800] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch =  900] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 1000] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 1100] train_perplexity =     3.57, train_loss =  1.27, learning_rate = 0.07812500


[batch = 1200] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 1300] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 1400] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1500] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1600] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 1700] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1800] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1900] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2000] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2100] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2200] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2300] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2400] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2500] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2600] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2700] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2800] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2900] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 3000] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 3100] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3200] train_perplexity =     3.59, train_loss =  1.28, learning_rate = 0.07812500


[batch = 3300] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3400] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3500] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3600] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3700] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3800] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3900] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4000] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4100] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch = 4200] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4300] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4400] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4500] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4600] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4700] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4800] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 4900] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5000] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 5100] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5200] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5300] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5400] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5500] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5600] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5700] train_perplexity =     3.30, train_loss =  1.20, learning_rate = 0.07812500


[batch = 5800] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5900] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 6000] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 6100] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6200] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6300] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6400] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6500] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch = 6600] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6700] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6800] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6900] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[epoch = 136] validation_perplexity =     3.55, validation_loss =  1.27

Saved model to 'e200/ptb_char_lstm.pt'...
[epoch = 136] min_validation_perplexity =     3.55, min_validation_loss =  1.27


[batch =  100] train_perplexity =     3.69, train_loss =  1.31, learning_rate = 0.07812500


[batch =  200] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch =  300] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch =  400] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch =  500] train_perplexity =     3.32, train_loss =  1.20, learning_rate = 0.07812500


[batch =  600] train_perplexity =     3.29, train_loss =  1.19, learning_rate = 0.07812500


[batch =  700] train_perplexity =     3.32, train_loss =  1.20, learning_rate = 0.07812500


[batch =  800] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch =  900] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 1000] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 1100] train_perplexity =     3.57, train_loss =  1.27, learning_rate = 0.07812500


[batch = 1200] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 1300] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 1400] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1500] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1600] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 1700] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1800] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1900] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2000] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2100] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2200] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2300] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2400] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2500] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2600] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2700] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2800] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2900] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 3000] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 3100] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3200] train_perplexity =     3.59, train_loss =  1.28, learning_rate = 0.07812500


[batch = 3300] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3400] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3500] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3600] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3700] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3800] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3900] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4000] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4100] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch = 4200] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4300] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4400] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4500] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4600] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4700] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4800] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 4900] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5000] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 5100] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5200] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5300] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5400] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5500] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5600] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5700] train_perplexity =     3.30, train_loss =  1.20, learning_rate = 0.07812500


[batch = 5800] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5900] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 6000] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 6100] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6200] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6300] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6400] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6500] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch = 6600] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6700] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6800] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6900] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[epoch = 137] validation_perplexity =     3.55, validation_loss =  1.27

Saved model to 'e200/ptb_char_lstm.pt'...
[epoch = 137] min_validation_perplexity =     3.55, min_validation_loss =  1.27


[batch =  100] train_perplexity =     3.69, train_loss =  1.31, learning_rate = 0.07812500


[batch =  200] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch =  300] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch =  400] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch =  500] train_perplexity =     3.32, train_loss =  1.20, learning_rate = 0.07812500


[batch =  600] train_perplexity =     3.29, train_loss =  1.19, learning_rate = 0.07812500


[batch =  700] train_perplexity =     3.32, train_loss =  1.20, learning_rate = 0.07812500


[batch =  800] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch =  900] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 1000] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 1100] train_perplexity =     3.57, train_loss =  1.27, learning_rate = 0.07812500


[batch = 1200] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 1300] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 1400] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1500] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1600] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 1700] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1800] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1900] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2000] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2100] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2200] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2300] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2400] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2500] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2600] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2700] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2800] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2900] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 3000] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 3100] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3200] train_perplexity =     3.59, train_loss =  1.28, learning_rate = 0.07812500


[batch = 3300] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3400] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3500] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3600] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3700] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3800] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3900] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4000] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4100] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch = 4200] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4300] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4400] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4500] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4600] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4700] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4800] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 4900] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5000] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 5100] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5200] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5300] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5400] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5500] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5600] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5700] train_perplexity =     3.30, train_loss =  1.20, learning_rate = 0.07812500


[batch = 5800] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5900] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 6000] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 6100] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6200] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6300] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6400] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6500] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch = 6600] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6700] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6800] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6900] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[epoch = 138] validation_perplexity =     3.55, validation_loss =  1.27

Saved model to 'e200/ptb_char_lstm.pt'...
[epoch = 138] min_validation_perplexity =     3.55, min_validation_loss =  1.27


[batch =  100] train_perplexity =     3.69, train_loss =  1.31, learning_rate = 0.07812500


[batch =  200] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch =  300] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch =  400] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch =  500] train_perplexity =     3.32, train_loss =  1.20, learning_rate = 0.07812500


[batch =  600] train_perplexity =     3.29, train_loss =  1.19, learning_rate = 0.07812500


[batch =  700] train_perplexity =     3.32, train_loss =  1.20, learning_rate = 0.07812500


[batch =  800] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch =  900] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 1000] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 1100] train_perplexity =     3.57, train_loss =  1.27, learning_rate = 0.07812500


[batch = 1200] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 1300] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 1400] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1500] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1600] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 1700] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1800] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1900] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2000] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2100] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2200] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2300] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2400] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2500] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2600] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2700] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2800] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2900] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 3000] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 3100] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3200] train_perplexity =     3.59, train_loss =  1.28, learning_rate = 0.07812500


[batch = 3300] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3400] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3500] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3600] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3700] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3800] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3900] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4000] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4100] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch = 4200] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4300] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4400] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4500] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4600] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4700] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4800] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 4900] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5000] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 5100] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5200] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5300] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5400] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5500] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5600] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5700] train_perplexity =     3.30, train_loss =  1.20, learning_rate = 0.07812500


[batch = 5800] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5900] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 6000] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 6100] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6200] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6300] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6400] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6500] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch = 6600] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6700] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6800] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6900] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[epoch = 139] validation_perplexity =     3.55, validation_loss =  1.27

Saved model to 'e200/ptb_char_lstm.pt'...
[epoch = 139] min_validation_perplexity =     3.55, min_validation_loss =  1.27


[batch =  100] train_perplexity =     3.69, train_loss =  1.31, learning_rate = 0.07812500


[batch =  200] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch =  300] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch =  400] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch =  500] train_perplexity =     3.32, train_loss =  1.20, learning_rate = 0.07812500


[batch =  600] train_perplexity =     3.29, train_loss =  1.19, learning_rate = 0.07812500


[batch =  700] train_perplexity =     3.32, train_loss =  1.20, learning_rate = 0.07812500


[batch =  800] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch =  900] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 1000] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 1100] train_perplexity =     3.57, train_loss =  1.27, learning_rate = 0.07812500


[batch = 1200] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 1300] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 1400] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1500] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1600] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 1700] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1800] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1900] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2000] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2100] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2200] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2300] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2400] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2500] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2600] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2700] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2800] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2900] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 3000] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 3100] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3200] train_perplexity =     3.59, train_loss =  1.28, learning_rate = 0.07812500


[batch = 3300] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3400] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3500] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3600] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3700] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3800] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3900] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4000] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4100] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch = 4200] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4300] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4400] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4500] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4600] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4700] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4800] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 4900] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5000] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 5100] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5200] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5300] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5400] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5500] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5600] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5700] train_perplexity =     3.30, train_loss =  1.20, learning_rate = 0.07812500


[batch = 5800] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5900] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 6000] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 6100] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6200] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6300] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6400] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6500] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch = 6600] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6700] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6800] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6900] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[epoch = 140] validation_perplexity =     3.55, validation_loss =  1.27

Saved model to 'e200/ptb_char_lstm.pt'...
[epoch = 140] min_validation_perplexity =     3.55, min_validation_loss =  1.27


[batch =  100] train_perplexity =     3.69, train_loss =  1.31, learning_rate = 0.07812500


[batch =  200] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch =  300] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch =  400] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch =  500] train_perplexity =     3.32, train_loss =  1.20, learning_rate = 0.07812500


[batch =  600] train_perplexity =     3.29, train_loss =  1.19, learning_rate = 0.07812500


[batch =  700] train_perplexity =     3.32, train_loss =  1.20, learning_rate = 0.07812500


[batch =  800] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch =  900] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 1000] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 1100] train_perplexity =     3.57, train_loss =  1.27, learning_rate = 0.07812500


[batch = 1200] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 1300] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 1400] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1500] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1600] train_perplexity =     3.38, train_loss =  1.22, learning_rate = 0.07812500


[batch = 1700] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1800] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1900] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2000] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2100] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2200] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2300] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2400] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2500] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2600] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2700] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2800] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2900] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 3000] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 3100] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3200] train_perplexity =     3.59, train_loss =  1.28, learning_rate = 0.07812500


[batch = 3300] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3400] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3500] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3600] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3700] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3800] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3900] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4000] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4100] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch = 4200] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4300] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4400] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4500] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4600] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4700] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4800] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 4900] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5000] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 5100] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5200] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5300] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5400] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5500] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5600] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5700] train_perplexity =     3.30, train_loss =  1.20, learning_rate = 0.07812500


[batch = 5800] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5900] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 6000] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 6100] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6200] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6300] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6400] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6500] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch = 6600] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6700] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6800] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6900] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[epoch = 141] validation_perplexity =     3.55, validation_loss =  1.27

Saved model to 'e200/ptb_char_lstm.pt'...
[epoch = 141] min_validation_perplexity =     3.55, min_validation_loss =  1.27


[batch =  100] train_perplexity =     3.69, train_loss =  1.31, learning_rate = 0.07812500


[batch =  200] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch =  300] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch =  400] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch =  500] train_perplexity =     3.32, train_loss =  1.20, learning_rate = 0.07812500


[batch =  600] train_perplexity =     3.29, train_loss =  1.19, learning_rate = 0.07812500


[batch =  700] train_perplexity =     3.32, train_loss =  1.20, learning_rate = 0.07812500


[batch =  800] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch =  900] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 1000] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 1100] train_perplexity =     3.57, train_loss =  1.27, learning_rate = 0.07812500


[batch = 1200] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 1300] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 1400] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1500] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1600] train_perplexity =     3.38, train_loss =  1.22, learning_rate = 0.07812500


[batch = 1700] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1800] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1900] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2000] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2100] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2200] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2300] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2400] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2500] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2600] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2700] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2800] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2900] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 3000] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 3100] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3200] train_perplexity =     3.59, train_loss =  1.28, learning_rate = 0.07812500


[batch = 3300] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3400] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3500] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3600] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3700] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3800] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3900] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4000] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4100] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch = 4200] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4300] train_perplexity =     3.40, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4400] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4500] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4600] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4700] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4800] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 4900] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5000] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 5100] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5200] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5300] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5400] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5500] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5600] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5700] train_perplexity =     3.30, train_loss =  1.20, learning_rate = 0.07812500


[batch = 5800] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5900] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 6000] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 6100] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6200] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6300] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6400] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6500] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch = 6600] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6700] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6800] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6900] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[epoch = 142] validation_perplexity =     3.55, validation_loss =  1.27

Saved model to 'e200/ptb_char_lstm.pt'...
[epoch = 142] min_validation_perplexity =     3.55, min_validation_loss =  1.27


[batch =  100] train_perplexity =     3.69, train_loss =  1.31, learning_rate = 0.07812500


[batch =  200] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch =  300] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch =  400] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch =  500] train_perplexity =     3.32, train_loss =  1.20, learning_rate = 0.07812500


[batch =  600] train_perplexity =     3.29, train_loss =  1.19, learning_rate = 0.07812500


[batch =  700] train_perplexity =     3.32, train_loss =  1.20, learning_rate = 0.07812500


[batch =  800] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch =  900] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 1000] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 1100] train_perplexity =     3.57, train_loss =  1.27, learning_rate = 0.07812500


[batch = 1200] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 1300] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 1400] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1500] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1600] train_perplexity =     3.38, train_loss =  1.22, learning_rate = 0.07812500


[batch = 1700] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1800] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1900] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2000] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2100] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2200] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2300] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2400] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2500] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2600] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2700] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2800] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2900] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 3000] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 3100] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3200] train_perplexity =     3.59, train_loss =  1.28, learning_rate = 0.07812500


[batch = 3300] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3400] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3500] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3600] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3700] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3800] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3900] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4000] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4100] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch = 4200] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4300] train_perplexity =     3.40, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4400] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4500] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4600] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4700] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4800] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 4900] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5000] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 5100] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5200] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5300] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5400] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5500] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5600] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5700] train_perplexity =     3.30, train_loss =  1.20, learning_rate = 0.07812500


[batch = 5800] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5900] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 6000] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 6100] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6200] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6300] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6400] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6500] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch = 6600] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6700] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6800] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6900] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[epoch = 143] validation_perplexity =     3.55, validation_loss =  1.27

Saved model to 'e200/ptb_char_lstm.pt'...
[epoch = 143] min_validation_perplexity =     3.55, min_validation_loss =  1.27


[batch =  100] train_perplexity =     3.69, train_loss =  1.31, learning_rate = 0.07812500


[batch =  200] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch =  300] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch =  400] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch =  500] train_perplexity =     3.32, train_loss =  1.20, learning_rate = 0.07812500


[batch =  600] train_perplexity =     3.29, train_loss =  1.19, learning_rate = 0.07812500


[batch =  700] train_perplexity =     3.32, train_loss =  1.20, learning_rate = 0.07812500


[batch =  800] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch =  900] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 1000] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 1100] train_perplexity =     3.57, train_loss =  1.27, learning_rate = 0.07812500


[batch = 1200] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 1300] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 1400] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1500] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1600] train_perplexity =     3.38, train_loss =  1.22, learning_rate = 0.07812500


[batch = 1700] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1800] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1900] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2000] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2100] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2200] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2300] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2400] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2500] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2600] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2700] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2800] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2900] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 3000] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 3100] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3200] train_perplexity =     3.59, train_loss =  1.28, learning_rate = 0.07812500


[batch = 3300] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3400] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3500] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3600] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3700] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3800] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3900] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4000] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4100] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch = 4200] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4300] train_perplexity =     3.40, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4400] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4500] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4600] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4700] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4800] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 4900] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5000] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 5100] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5200] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5300] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5400] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5500] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5600] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5700] train_perplexity =     3.30, train_loss =  1.20, learning_rate = 0.07812500


[batch = 5800] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5900] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 6000] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 6100] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6200] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6300] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6400] train_perplexity =     3.38, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6500] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch = 6600] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6700] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6800] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6900] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[epoch = 144] validation_perplexity =     3.55, validation_loss =  1.27

Saved model to 'e200/ptb_char_lstm.pt'...
[epoch = 144] min_validation_perplexity =     3.55, min_validation_loss =  1.27


[batch =  100] train_perplexity =     3.69, train_loss =  1.31, learning_rate = 0.07812500


[batch =  200] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch =  300] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch =  400] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch =  500] train_perplexity =     3.32, train_loss =  1.20, learning_rate = 0.07812500


[batch =  600] train_perplexity =     3.29, train_loss =  1.19, learning_rate = 0.07812500


[batch =  700] train_perplexity =     3.32, train_loss =  1.20, learning_rate = 0.07812500


[batch =  800] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch =  900] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 1000] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 1100] train_perplexity =     3.57, train_loss =  1.27, learning_rate = 0.07812500


[batch = 1200] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 1300] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 1400] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1500] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1600] train_perplexity =     3.38, train_loss =  1.22, learning_rate = 0.07812500


[batch = 1700] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1800] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1900] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2000] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2100] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2200] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2300] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2400] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2500] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2600] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2700] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2800] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2900] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 3000] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 3100] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3200] train_perplexity =     3.59, train_loss =  1.28, learning_rate = 0.07812500


[batch = 3300] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3400] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3500] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3600] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3700] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3800] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3900] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4000] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4100] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch = 4200] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4300] train_perplexity =     3.40, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4400] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4500] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4600] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4700] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4800] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 4900] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5000] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 5100] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5200] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5300] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5400] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5500] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5600] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5700] train_perplexity =     3.30, train_loss =  1.19, learning_rate = 0.07812500


[batch = 5800] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5900] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 6000] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 6100] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6200] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6300] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6400] train_perplexity =     3.38, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6500] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch = 6600] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6700] train_perplexity =     3.38, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6800] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6900] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[epoch = 145] validation_perplexity =     3.55, validation_loss =  1.27

Saved model to 'e200/ptb_char_lstm.pt'...
[epoch = 145] min_validation_perplexity =     3.55, min_validation_loss =  1.27


[batch =  100] train_perplexity =     3.69, train_loss =  1.31, learning_rate = 0.07812500


[batch =  200] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch =  300] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch =  400] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch =  500] train_perplexity =     3.32, train_loss =  1.20, learning_rate = 0.07812500


[batch =  600] train_perplexity =     3.29, train_loss =  1.19, learning_rate = 0.07812500


[batch =  700] train_perplexity =     3.32, train_loss =  1.20, learning_rate = 0.07812500


[batch =  800] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch =  900] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 1000] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 1100] train_perplexity =     3.57, train_loss =  1.27, learning_rate = 0.07812500


[batch = 1200] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 1300] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 1400] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1500] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1600] train_perplexity =     3.38, train_loss =  1.22, learning_rate = 0.07812500


[batch = 1700] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1800] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1900] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2000] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2100] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2200] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2300] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2400] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2500] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2600] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2700] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2800] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2900] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 3000] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 3100] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3200] train_perplexity =     3.59, train_loss =  1.28, learning_rate = 0.07812500


[batch = 3300] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3400] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3500] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3600] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3700] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3800] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3900] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4000] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4100] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch = 4200] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4300] train_perplexity =     3.40, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4400] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4500] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4600] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4700] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4800] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 4900] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5000] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 5100] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5200] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5300] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5400] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5500] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5600] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5700] train_perplexity =     3.30, train_loss =  1.19, learning_rate = 0.07812500


[batch = 5800] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5900] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 6000] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 6100] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6200] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6300] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6400] train_perplexity =     3.38, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6500] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch = 6600] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6700] train_perplexity =     3.38, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6800] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6900] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[epoch = 146] validation_perplexity =     3.55, validation_loss =  1.27

Saved model to 'e200/ptb_char_lstm.pt'...
[epoch = 146] min_validation_perplexity =     3.55, min_validation_loss =  1.27


[batch =  100] train_perplexity =     3.69, train_loss =  1.31, learning_rate = 0.07812500


[batch =  200] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch =  300] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch =  400] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch =  500] train_perplexity =     3.32, train_loss =  1.20, learning_rate = 0.07812500


[batch =  600] train_perplexity =     3.29, train_loss =  1.19, learning_rate = 0.07812500


[batch =  700] train_perplexity =     3.32, train_loss =  1.20, learning_rate = 0.07812500


[batch =  800] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch =  900] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 1000] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 1100] train_perplexity =     3.57, train_loss =  1.27, learning_rate = 0.07812500


[batch = 1200] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 1300] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 1400] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1500] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1600] train_perplexity =     3.38, train_loss =  1.22, learning_rate = 0.07812500


[batch = 1700] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1800] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1900] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2000] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2100] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2200] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2300] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2400] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2500] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2600] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2700] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2800] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2900] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 3000] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 3100] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3200] train_perplexity =     3.59, train_loss =  1.28, learning_rate = 0.07812500


[batch = 3300] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3400] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3500] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3600] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3700] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3800] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3900] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4000] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4100] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch = 4200] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4300] train_perplexity =     3.40, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4400] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4500] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4600] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4700] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4800] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 4900] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5000] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 5100] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5200] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5300] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5400] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5500] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5600] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5700] train_perplexity =     3.30, train_loss =  1.19, learning_rate = 0.07812500


[batch = 5800] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5900] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 6000] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 6100] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6200] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6300] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6400] train_perplexity =     3.38, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6500] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch = 6600] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6700] train_perplexity =     3.38, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6800] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6900] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[epoch = 147] validation_perplexity =     3.55, validation_loss =  1.27

Saved model to 'e200/ptb_char_lstm.pt'...
[epoch = 147] min_validation_perplexity =     3.55, min_validation_loss =  1.27


[batch =  100] train_perplexity =     3.69, train_loss =  1.31, learning_rate = 0.07812500


[batch =  200] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch =  300] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch =  400] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch =  500] train_perplexity =     3.32, train_loss =  1.20, learning_rate = 0.07812500


[batch =  600] train_perplexity =     3.29, train_loss =  1.19, learning_rate = 0.07812500


[batch =  700] train_perplexity =     3.32, train_loss =  1.20, learning_rate = 0.07812500


[batch =  800] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch =  900] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 1000] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 1100] train_perplexity =     3.57, train_loss =  1.27, learning_rate = 0.07812500


[batch = 1200] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 1300] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 1400] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1500] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1600] train_perplexity =     3.38, train_loss =  1.22, learning_rate = 0.07812500


[batch = 1700] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1800] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1900] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2000] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2100] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2200] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2300] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2400] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2500] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2600] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2700] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2800] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2900] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 3000] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 3100] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3200] train_perplexity =     3.59, train_loss =  1.28, learning_rate = 0.07812500


[batch = 3300] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3400] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3500] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3600] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3700] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3800] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3900] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4000] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4100] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch = 4200] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4300] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 4400] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4500] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4600] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4700] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4800] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 4900] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5000] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 5100] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5200] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5300] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5400] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5500] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5600] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5700] train_perplexity =     3.30, train_loss =  1.19, learning_rate = 0.07812500


[batch = 5800] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5900] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 6000] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 6100] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6200] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6300] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6400] train_perplexity =     3.38, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6500] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch = 6600] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6700] train_perplexity =     3.38, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6800] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6900] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[epoch = 148] validation_perplexity =     3.55, validation_loss =  1.27

Saved model to 'e200/ptb_char_lstm.pt'...
[epoch = 148] min_validation_perplexity =     3.55, min_validation_loss =  1.27


[batch =  100] train_perplexity =     3.69, train_loss =  1.31, learning_rate = 0.07812500


[batch =  200] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch =  300] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch =  400] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch =  500] train_perplexity =     3.32, train_loss =  1.20, learning_rate = 0.07812500


[batch =  600] train_perplexity =     3.29, train_loss =  1.19, learning_rate = 0.07812500


[batch =  700] train_perplexity =     3.32, train_loss =  1.20, learning_rate = 0.07812500


[batch =  800] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch =  900] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 1000] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 1100] train_perplexity =     3.57, train_loss =  1.27, learning_rate = 0.07812500


[batch = 1200] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 1300] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 1400] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1500] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1600] train_perplexity =     3.38, train_loss =  1.22, learning_rate = 0.07812500


[batch = 1700] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1800] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1900] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2000] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2100] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2200] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2300] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2400] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2500] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2600] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2700] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2800] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2900] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 3000] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 3100] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3200] train_perplexity =     3.59, train_loss =  1.28, learning_rate = 0.07812500


[batch = 3300] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3400] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3500] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3600] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3700] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3800] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3900] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4000] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4100] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch = 4200] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4300] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 4400] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4500] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4600] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4700] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4800] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 4900] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5000] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 5100] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5200] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5300] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5400] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5500] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5600] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5700] train_perplexity =     3.30, train_loss =  1.19, learning_rate = 0.07812500


[batch = 5800] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5900] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 6000] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 6100] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6200] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6300] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6400] train_perplexity =     3.38, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6500] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch = 6600] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6700] train_perplexity =     3.38, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6800] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6900] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[epoch = 149] validation_perplexity =     3.55, validation_loss =  1.27

Saved model to 'e200/ptb_char_lstm.pt'...
[epoch = 149] min_validation_perplexity =     3.55, min_validation_loss =  1.27


[batch =  100] train_perplexity =     3.69, train_loss =  1.31, learning_rate = 0.07812500


[batch =  200] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch =  300] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch =  400] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch =  500] train_perplexity =     3.32, train_loss =  1.20, learning_rate = 0.07812500


[batch =  600] train_perplexity =     3.29, train_loss =  1.19, learning_rate = 0.07812500


[batch =  700] train_perplexity =     3.32, train_loss =  1.20, learning_rate = 0.07812500


[batch =  800] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch =  900] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 1000] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 1100] train_perplexity =     3.57, train_loss =  1.27, learning_rate = 0.07812500


[batch = 1200] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 1300] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 1400] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1500] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1600] train_perplexity =     3.38, train_loss =  1.22, learning_rate = 0.07812500


[batch = 1700] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1800] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1900] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2000] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2100] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2200] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2300] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2400] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2500] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2600] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2700] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2800] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2900] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 3000] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 3100] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3200] train_perplexity =     3.59, train_loss =  1.28, learning_rate = 0.07812500


[batch = 3300] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3400] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3500] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3600] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3700] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3800] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3900] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4000] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4100] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch = 4200] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4300] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 4400] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4500] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4600] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4700] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4800] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 4900] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5000] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 5100] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5200] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5300] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5400] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5500] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5600] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5700] train_perplexity =     3.30, train_loss =  1.19, learning_rate = 0.07812500


[batch = 5800] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5900] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 6000] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 6100] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6200] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6300] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6400] train_perplexity =     3.38, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6500] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch = 6600] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6700] train_perplexity =     3.38, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6800] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6900] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[epoch = 150] validation_perplexity =     3.55, validation_loss =  1.27

Saved model to 'e200/ptb_char_lstm.pt'...
[epoch = 150] min_validation_perplexity =     3.55, min_validation_loss =  1.27


[batch =  100] train_perplexity =     3.69, train_loss =  1.31, learning_rate = 0.07812500


[batch =  200] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch =  300] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch =  400] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch =  500] train_perplexity =     3.32, train_loss =  1.20, learning_rate = 0.07812500


[batch =  600] train_perplexity =     3.29, train_loss =  1.19, learning_rate = 0.07812500


[batch =  700] train_perplexity =     3.32, train_loss =  1.20, learning_rate = 0.07812500


[batch =  800] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch =  900] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 1000] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 1100] train_perplexity =     3.57, train_loss =  1.27, learning_rate = 0.07812500


[batch = 1200] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 1300] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 1400] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1500] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1600] train_perplexity =     3.38, train_loss =  1.22, learning_rate = 0.07812500


[batch = 1700] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1800] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1900] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2000] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2100] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2200] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2300] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2400] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2500] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2600] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2700] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2800] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2900] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 3000] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 3100] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3200] train_perplexity =     3.59, train_loss =  1.28, learning_rate = 0.07812500


[batch = 3300] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3400] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3500] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3600] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3700] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3800] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3900] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4000] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4100] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch = 4200] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4300] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 4400] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4500] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4600] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4700] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4800] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 4900] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5000] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 5100] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5200] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5300] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5400] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5500] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5600] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5700] train_perplexity =     3.30, train_loss =  1.19, learning_rate = 0.07812500


[batch = 5800] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5900] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 6000] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 6100] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6200] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6300] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6400] train_perplexity =     3.38, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6500] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch = 6600] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6700] train_perplexity =     3.38, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6800] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6900] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[epoch = 151] validation_perplexity =     3.55, validation_loss =  1.27

Saved model to 'e200/ptb_char_lstm.pt'...
[epoch = 151] min_validation_perplexity =     3.55, min_validation_loss =  1.27


[batch =  100] train_perplexity =     3.69, train_loss =  1.31, learning_rate = 0.07812500


[batch =  200] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch =  300] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch =  400] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch =  500] train_perplexity =     3.32, train_loss =  1.20, learning_rate = 0.07812500


[batch =  600] train_perplexity =     3.29, train_loss =  1.19, learning_rate = 0.07812500


[batch =  700] train_perplexity =     3.32, train_loss =  1.20, learning_rate = 0.07812500


[batch =  800] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch =  900] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 1000] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 1100] train_perplexity =     3.57, train_loss =  1.27, learning_rate = 0.07812500


[batch = 1200] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 1300] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 1400] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1500] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1600] train_perplexity =     3.38, train_loss =  1.22, learning_rate = 0.07812500


[batch = 1700] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1800] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1900] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2000] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2100] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2200] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2300] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2400] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2500] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2600] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2700] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2800] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2900] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 3000] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 3100] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3200] train_perplexity =     3.59, train_loss =  1.28, learning_rate = 0.07812500


[batch = 3300] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3400] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3500] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3600] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3700] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3800] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3900] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4000] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4100] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch = 4200] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4300] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 4400] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4500] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4600] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4700] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4800] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 4900] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5000] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 5100] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5200] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5300] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5400] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5500] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5600] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5700] train_perplexity =     3.30, train_loss =  1.19, learning_rate = 0.07812500


[batch = 5800] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5900] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 6000] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 6100] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6200] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6300] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6400] train_perplexity =     3.38, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6500] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch = 6600] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6700] train_perplexity =     3.38, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6800] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6900] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[epoch = 152] validation_perplexity =     3.55, validation_loss =  1.27

Saved model to 'e200/ptb_char_lstm.pt'...
[epoch = 152] min_validation_perplexity =     3.55, min_validation_loss =  1.27


[batch =  100] train_perplexity =     3.69, train_loss =  1.31, learning_rate = 0.07812500


[batch =  200] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch =  300] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch =  400] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch =  500] train_perplexity =     3.32, train_loss =  1.20, learning_rate = 0.07812500


[batch =  600] train_perplexity =     3.29, train_loss =  1.19, learning_rate = 0.07812500


[batch =  700] train_perplexity =     3.32, train_loss =  1.20, learning_rate = 0.07812500


[batch =  800] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch =  900] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 1000] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 1100] train_perplexity =     3.57, train_loss =  1.27, learning_rate = 0.07812500


[batch = 1200] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 1300] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 1400] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1500] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1600] train_perplexity =     3.38, train_loss =  1.22, learning_rate = 0.07812500


[batch = 1700] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1800] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1900] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2000] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2100] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2200] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2300] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2400] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2500] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2600] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2700] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2800] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2900] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 3000] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 3100] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3200] train_perplexity =     3.59, train_loss =  1.28, learning_rate = 0.07812500


[batch = 3300] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3400] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3500] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3600] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3700] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3800] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3900] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4000] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4100] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch = 4200] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4300] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 4400] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4500] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4600] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4700] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4800] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 4900] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5000] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 5100] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5200] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5300] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5400] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5500] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5600] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5700] train_perplexity =     3.30, train_loss =  1.19, learning_rate = 0.07812500


[batch = 5800] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5900] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 6000] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 6100] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6200] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6300] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6400] train_perplexity =     3.38, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6500] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch = 6600] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6700] train_perplexity =     3.38, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6800] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6900] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[epoch = 153] validation_perplexity =     3.55, validation_loss =  1.27

Saved model to 'e200/ptb_char_lstm.pt'...
[epoch = 153] min_validation_perplexity =     3.55, min_validation_loss =  1.27


[batch =  100] train_perplexity =     3.69, train_loss =  1.31, learning_rate = 0.07812500


[batch =  200] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch =  300] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch =  400] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch =  500] train_perplexity =     3.32, train_loss =  1.20, learning_rate = 0.07812500


[batch =  600] train_perplexity =     3.29, train_loss =  1.19, learning_rate = 0.07812500


[batch =  700] train_perplexity =     3.32, train_loss =  1.20, learning_rate = 0.07812500


[batch =  800] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch =  900] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 1000] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 1100] train_perplexity =     3.57, train_loss =  1.27, learning_rate = 0.07812500


[batch = 1200] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 1300] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 1400] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1500] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1600] train_perplexity =     3.38, train_loss =  1.22, learning_rate = 0.07812500


[batch = 1700] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1800] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1900] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2000] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2100] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2200] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2300] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2400] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2500] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2600] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2700] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2800] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2900] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 3000] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 3100] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3200] train_perplexity =     3.59, train_loss =  1.28, learning_rate = 0.07812500


[batch = 3300] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3400] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3500] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3600] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3700] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3800] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3900] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4000] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4100] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch = 4200] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4300] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 4400] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4500] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4600] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4700] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4800] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 4900] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5000] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 5100] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5200] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5300] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5400] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5500] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5600] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5700] train_perplexity =     3.30, train_loss =  1.19, learning_rate = 0.07812500


[batch = 5800] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5900] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 6000] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 6100] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6200] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6300] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6400] train_perplexity =     3.38, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6500] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch = 6600] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6700] train_perplexity =     3.38, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6800] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6900] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[epoch = 154] validation_perplexity =     3.55, validation_loss =  1.27

Saved model to 'e200/ptb_char_lstm.pt'...
[epoch = 154] min_validation_perplexity =     3.55, min_validation_loss =  1.27


[batch =  100] train_perplexity =     3.69, train_loss =  1.31, learning_rate = 0.07812500


[batch =  200] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch =  300] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch =  400] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch =  500] train_perplexity =     3.32, train_loss =  1.20, learning_rate = 0.07812500


[batch =  600] train_perplexity =     3.29, train_loss =  1.19, learning_rate = 0.07812500


[batch =  700] train_perplexity =     3.32, train_loss =  1.20, learning_rate = 0.07812500


[batch =  800] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch =  900] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 1000] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 1100] train_perplexity =     3.57, train_loss =  1.27, learning_rate = 0.07812500


[batch = 1200] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 1300] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 1400] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1500] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1600] train_perplexity =     3.38, train_loss =  1.22, learning_rate = 0.07812500


[batch = 1700] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1800] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1900] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2000] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2100] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2200] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2300] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2400] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2500] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2600] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2700] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2800] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2900] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 3000] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 3100] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3200] train_perplexity =     3.59, train_loss =  1.28, learning_rate = 0.07812500


[batch = 3300] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3400] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3500] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3600] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3700] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3800] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3900] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4000] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4100] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch = 4200] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4300] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 4400] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4500] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4600] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4700] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4800] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 4900] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5000] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 5100] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5200] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5300] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5400] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5500] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5600] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5700] train_perplexity =     3.30, train_loss =  1.19, learning_rate = 0.07812500


[batch = 5800] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5900] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 6000] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 6100] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6200] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6300] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6400] train_perplexity =     3.38, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6500] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch = 6600] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6700] train_perplexity =     3.38, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6800] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6900] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[epoch = 155] validation_perplexity =     3.55, validation_loss =  1.27

Saved model to 'e200/ptb_char_lstm.pt'...
[epoch = 155] min_validation_perplexity =     3.55, min_validation_loss =  1.27


[batch =  100] train_perplexity =     3.69, train_loss =  1.31, learning_rate = 0.07812500


[batch =  200] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch =  300] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch =  400] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch =  500] train_perplexity =     3.32, train_loss =  1.20, learning_rate = 0.07812500


[batch =  600] train_perplexity =     3.29, train_loss =  1.19, learning_rate = 0.07812500


[batch =  700] train_perplexity =     3.32, train_loss =  1.20, learning_rate = 0.07812500


[batch =  800] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch =  900] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 1000] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 1100] train_perplexity =     3.57, train_loss =  1.27, learning_rate = 0.07812500


[batch = 1200] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 1300] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 1400] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1500] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1600] train_perplexity =     3.38, train_loss =  1.22, learning_rate = 0.07812500


[batch = 1700] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1800] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1900] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2000] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2100] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2200] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2300] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2400] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2500] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2600] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2700] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2800] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2900] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 3000] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 3100] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3200] train_perplexity =     3.59, train_loss =  1.28, learning_rate = 0.07812500


[batch = 3300] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3400] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3500] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3600] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3700] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3800] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3900] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4000] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4100] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch = 4200] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4300] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 4400] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4500] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4600] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4700] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4800] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 4900] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5000] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 5100] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5200] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5300] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5400] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5500] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5600] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5700] train_perplexity =     3.30, train_loss =  1.19, learning_rate = 0.07812500


[batch = 5800] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5900] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 6000] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 6100] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6200] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6300] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6400] train_perplexity =     3.38, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6500] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch = 6600] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6700] train_perplexity =     3.38, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6800] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6900] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[epoch = 156] validation_perplexity =     3.55, validation_loss =  1.27

Saved model to 'e200/ptb_char_lstm.pt'...
[epoch = 156] min_validation_perplexity =     3.55, min_validation_loss =  1.27


[batch =  100] train_perplexity =     3.69, train_loss =  1.31, learning_rate = 0.07812500


[batch =  200] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch =  300] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch =  400] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch =  500] train_perplexity =     3.32, train_loss =  1.20, learning_rate = 0.07812500


[batch =  600] train_perplexity =     3.29, train_loss =  1.19, learning_rate = 0.07812500


[batch =  700] train_perplexity =     3.32, train_loss =  1.20, learning_rate = 0.07812500


[batch =  800] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch =  900] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 1000] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 1100] train_perplexity =     3.57, train_loss =  1.27, learning_rate = 0.07812500


[batch = 1200] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 1300] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 1400] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1500] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1600] train_perplexity =     3.38, train_loss =  1.22, learning_rate = 0.07812500


[batch = 1700] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1800] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1900] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2000] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2100] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2200] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2300] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2400] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2500] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2600] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2700] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2800] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2900] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 3000] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 3100] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3200] train_perplexity =     3.59, train_loss =  1.28, learning_rate = 0.07812500


[batch = 3300] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3400] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3500] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3600] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3700] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3800] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3900] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4000] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4100] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch = 4200] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4300] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 4400] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4500] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4600] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4700] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4800] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 4900] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5000] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 5100] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5200] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5300] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5400] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5500] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5600] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5700] train_perplexity =     3.30, train_loss =  1.19, learning_rate = 0.07812500


[batch = 5800] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5900] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 6000] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 6100] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6200] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6300] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6400] train_perplexity =     3.38, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6500] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch = 6600] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6700] train_perplexity =     3.38, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6800] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6900] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[epoch = 157] validation_perplexity =     3.55, validation_loss =  1.27

Saved model to 'e200/ptb_char_lstm.pt'...
[epoch = 157] min_validation_perplexity =     3.55, min_validation_loss =  1.27


[batch =  100] train_perplexity =     3.69, train_loss =  1.31, learning_rate = 0.07812500


[batch =  200] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch =  300] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch =  400] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch =  500] train_perplexity =     3.32, train_loss =  1.20, learning_rate = 0.07812500


[batch =  600] train_perplexity =     3.29, train_loss =  1.19, learning_rate = 0.07812500


[batch =  700] train_perplexity =     3.32, train_loss =  1.20, learning_rate = 0.07812500


[batch =  800] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch =  900] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 1000] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 1100] train_perplexity =     3.57, train_loss =  1.27, learning_rate = 0.07812500


[batch = 1200] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 1300] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 1400] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1500] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1600] train_perplexity =     3.38, train_loss =  1.22, learning_rate = 0.07812500


[batch = 1700] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1800] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1900] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2000] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2100] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2200] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2300] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2400] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2500] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2600] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2700] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2800] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2900] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 3000] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 3100] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3200] train_perplexity =     3.59, train_loss =  1.28, learning_rate = 0.07812500


[batch = 3300] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3400] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3500] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3600] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3700] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3800] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3900] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4000] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4100] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch = 4200] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4300] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 4400] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4500] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4600] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4700] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4800] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 4900] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5000] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 5100] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5200] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5300] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5400] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5500] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5600] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5700] train_perplexity =     3.30, train_loss =  1.19, learning_rate = 0.07812500


[batch = 5800] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5900] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 6000] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 6100] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6200] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6300] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6400] train_perplexity =     3.38, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6500] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch = 6600] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6700] train_perplexity =     3.38, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6800] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6900] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[epoch = 158] validation_perplexity =     3.55, validation_loss =  1.27

Saved model to 'e200/ptb_char_lstm.pt'...
[epoch = 158] min_validation_perplexity =     3.55, min_validation_loss =  1.27


[batch =  100] train_perplexity =     3.69, train_loss =  1.31, learning_rate = 0.07812500


[batch =  200] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch =  300] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch =  400] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch =  500] train_perplexity =     3.32, train_loss =  1.20, learning_rate = 0.07812500


[batch =  600] train_perplexity =     3.29, train_loss =  1.19, learning_rate = 0.07812500


[batch =  700] train_perplexity =     3.32, train_loss =  1.20, learning_rate = 0.07812500


[batch =  800] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch =  900] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 1000] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 1100] train_perplexity =     3.57, train_loss =  1.27, learning_rate = 0.07812500


[batch = 1200] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 1300] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 1400] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1500] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1600] train_perplexity =     3.38, train_loss =  1.22, learning_rate = 0.07812500


[batch = 1700] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1800] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1900] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2000] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2100] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2200] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2300] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2400] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2500] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2600] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2700] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2800] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2900] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 3000] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 3100] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3200] train_perplexity =     3.59, train_loss =  1.28, learning_rate = 0.07812500


[batch = 3300] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3400] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3500] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3600] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3700] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3800] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3900] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4000] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4100] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch = 4200] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4300] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 4400] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4500] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4600] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4700] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4800] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 4900] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5000] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 5100] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5200] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5300] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5400] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5500] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5600] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5700] train_perplexity =     3.30, train_loss =  1.19, learning_rate = 0.07812500


[batch = 5800] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5900] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 6000] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 6100] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6200] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6300] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6400] train_perplexity =     3.38, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6500] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch = 6600] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6700] train_perplexity =     3.38, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6800] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6900] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[epoch = 159] validation_perplexity =     3.55, validation_loss =  1.27

Saved model to 'e200/ptb_char_lstm.pt'...
[epoch = 159] min_validation_perplexity =     3.55, min_validation_loss =  1.27


[batch =  100] train_perplexity =     3.69, train_loss =  1.31, learning_rate = 0.07812500


[batch =  200] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch =  300] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch =  400] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch =  500] train_perplexity =     3.32, train_loss =  1.20, learning_rate = 0.07812500


[batch =  600] train_perplexity =     3.29, train_loss =  1.19, learning_rate = 0.07812500


[batch =  700] train_perplexity =     3.32, train_loss =  1.20, learning_rate = 0.07812500


[batch =  800] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch =  900] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 1000] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 1100] train_perplexity =     3.57, train_loss =  1.27, learning_rate = 0.07812500


[batch = 1200] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 1300] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 1400] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1500] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1600] train_perplexity =     3.38, train_loss =  1.22, learning_rate = 0.07812500


[batch = 1700] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1800] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1900] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2000] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2100] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2200] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2300] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2400] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2500] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2600] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2700] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2800] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2900] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 3000] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 3100] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3200] train_perplexity =     3.59, train_loss =  1.28, learning_rate = 0.07812500


[batch = 3300] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3400] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3500] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3600] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3700] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3800] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3900] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4000] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4100] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch = 4200] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4300] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 4400] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4500] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4600] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4700] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4800] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 4900] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5000] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 5100] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5200] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5300] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5400] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5500] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5600] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5700] train_perplexity =     3.30, train_loss =  1.19, learning_rate = 0.07812500


[batch = 5800] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5900] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 6000] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 6100] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6200] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6300] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6400] train_perplexity =     3.38, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6500] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch = 6600] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6700] train_perplexity =     3.38, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6800] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6900] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[epoch = 160] validation_perplexity =     3.55, validation_loss =  1.27

Saved model to 'e200/ptb_char_lstm.pt'...
[epoch = 160] min_validation_perplexity =     3.55, min_validation_loss =  1.27


[batch =  100] train_perplexity =     3.69, train_loss =  1.31, learning_rate = 0.07812500


[batch =  200] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch =  300] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch =  400] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch =  500] train_perplexity =     3.32, train_loss =  1.20, learning_rate = 0.07812500


[batch =  600] train_perplexity =     3.29, train_loss =  1.19, learning_rate = 0.07812500


[batch =  700] train_perplexity =     3.32, train_loss =  1.20, learning_rate = 0.07812500


[batch =  800] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch =  900] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 1000] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 1100] train_perplexity =     3.57, train_loss =  1.27, learning_rate = 0.07812500


[batch = 1200] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 1300] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 1400] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1500] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1600] train_perplexity =     3.38, train_loss =  1.22, learning_rate = 0.07812500


[batch = 1700] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1800] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1900] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2000] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2100] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2200] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2300] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2400] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2500] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2600] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2700] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2800] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2900] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 3000] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 3100] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3200] train_perplexity =     3.59, train_loss =  1.28, learning_rate = 0.07812500


[batch = 3300] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3400] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3500] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3600] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3700] train_perplexity =     3.47, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3800] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3900] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4000] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4100] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch = 4200] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4300] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 4400] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4500] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4600] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4700] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4800] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 4900] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5000] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 5100] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5200] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5300] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5400] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5500] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5600] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5700] train_perplexity =     3.30, train_loss =  1.19, learning_rate = 0.07812500


[batch = 5800] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5900] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 6000] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 6100] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6200] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6300] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6400] train_perplexity =     3.38, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6500] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch = 6600] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6700] train_perplexity =     3.38, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6800] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6900] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[epoch = 161] validation_perplexity =     3.55, validation_loss =  1.27

Saved model to 'e200/ptb_char_lstm.pt'...
[epoch = 161] min_validation_perplexity =     3.55, min_validation_loss =  1.27


[batch =  100] train_perplexity =     3.69, train_loss =  1.31, learning_rate = 0.07812500


[batch =  200] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch =  300] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch =  400] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch =  500] train_perplexity =     3.32, train_loss =  1.20, learning_rate = 0.07812500


[batch =  600] train_perplexity =     3.29, train_loss =  1.19, learning_rate = 0.07812500


[batch =  700] train_perplexity =     3.32, train_loss =  1.20, learning_rate = 0.07812500


[batch =  800] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch =  900] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 1000] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 1100] train_perplexity =     3.57, train_loss =  1.27, learning_rate = 0.07812500


[batch = 1200] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 1300] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 1400] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1500] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1600] train_perplexity =     3.38, train_loss =  1.22, learning_rate = 0.07812500


[batch = 1700] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1800] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1900] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2000] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2100] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2200] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2300] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2400] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2500] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2600] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2700] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2800] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2900] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 3000] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 3100] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3200] train_perplexity =     3.59, train_loss =  1.28, learning_rate = 0.07812500


[batch = 3300] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3400] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3500] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3600] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3700] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3800] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3900] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4000] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4100] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch = 4200] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4300] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 4400] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4500] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4600] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4700] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4800] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 4900] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5000] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 5100] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5200] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5300] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5400] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5500] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5600] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5700] train_perplexity =     3.30, train_loss =  1.19, learning_rate = 0.07812500


[batch = 5800] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5900] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 6000] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 6100] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6200] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6300] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6400] train_perplexity =     3.38, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6500] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch = 6600] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6700] train_perplexity =     3.38, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6800] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6900] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[epoch = 162] validation_perplexity =     3.55, validation_loss =  1.27

Saved model to 'e200/ptb_char_lstm.pt'...
[epoch = 162] min_validation_perplexity =     3.55, min_validation_loss =  1.27


[batch =  100] train_perplexity =     3.69, train_loss =  1.31, learning_rate = 0.07812500


[batch =  200] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch =  300] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch =  400] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch =  500] train_perplexity =     3.32, train_loss =  1.20, learning_rate = 0.07812500


[batch =  600] train_perplexity =     3.29, train_loss =  1.19, learning_rate = 0.07812500


[batch =  700] train_perplexity =     3.32, train_loss =  1.20, learning_rate = 0.07812500


[batch =  800] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch =  900] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 1000] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 1100] train_perplexity =     3.57, train_loss =  1.27, learning_rate = 0.07812500


[batch = 1200] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 1300] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 1400] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1500] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1600] train_perplexity =     3.38, train_loss =  1.22, learning_rate = 0.07812500


[batch = 1700] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1800] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1900] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2000] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2100] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2200] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2300] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2400] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2500] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2600] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2700] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2800] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2900] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 3000] train_perplexity =     3.38, train_loss =  1.22, learning_rate = 0.07812500


[batch = 3100] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3200] train_perplexity =     3.59, train_loss =  1.28, learning_rate = 0.07812500


[batch = 3300] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3400] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3500] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3600] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3700] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3800] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3900] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4000] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4100] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch = 4200] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4300] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 4400] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4500] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4600] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4700] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4800] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 4900] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5000] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 5100] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5200] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5300] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5400] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5500] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5600] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5700] train_perplexity =     3.30, train_loss =  1.19, learning_rate = 0.07812500


[batch = 5800] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5900] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 6000] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 6100] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6200] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6300] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6400] train_perplexity =     3.38, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6500] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch = 6600] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6700] train_perplexity =     3.38, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6800] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6900] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[epoch = 163] validation_perplexity =     3.55, validation_loss =  1.27

Saved model to 'e200/ptb_char_lstm.pt'...
[epoch = 163] min_validation_perplexity =     3.55, min_validation_loss =  1.27


[batch =  100] train_perplexity =     3.69, train_loss =  1.31, learning_rate = 0.07812500


[batch =  200] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch =  300] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch =  400] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch =  500] train_perplexity =     3.32, train_loss =  1.20, learning_rate = 0.07812500


[batch =  600] train_perplexity =     3.29, train_loss =  1.19, learning_rate = 0.07812500


[batch =  700] train_perplexity =     3.32, train_loss =  1.20, learning_rate = 0.07812500


[batch =  800] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch =  900] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 1000] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 1100] train_perplexity =     3.57, train_loss =  1.27, learning_rate = 0.07812500


[batch = 1200] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 1300] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 1400] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1500] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1600] train_perplexity =     3.38, train_loss =  1.22, learning_rate = 0.07812500


[batch = 1700] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1800] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1900] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2000] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2100] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2200] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2300] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2400] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2500] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2600] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2700] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2800] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2900] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 3000] train_perplexity =     3.38, train_loss =  1.22, learning_rate = 0.07812500


[batch = 3100] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3200] train_perplexity =     3.59, train_loss =  1.28, learning_rate = 0.07812500


[batch = 3300] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3400] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3500] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3600] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3700] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3800] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3900] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4000] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4100] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch = 4200] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4300] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 4400] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4500] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4600] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4700] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4800] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 4900] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5000] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 5100] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5200] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5300] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5400] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5500] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5600] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5700] train_perplexity =     3.30, train_loss =  1.19, learning_rate = 0.07812500


[batch = 5800] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5900] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 6000] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 6100] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6200] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6300] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6400] train_perplexity =     3.38, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6500] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch = 6600] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6700] train_perplexity =     3.38, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6800] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6900] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[epoch = 164] validation_perplexity =     3.55, validation_loss =  1.27

Saved model to 'e200/ptb_char_lstm.pt'...
[epoch = 164] min_validation_perplexity =     3.55, min_validation_loss =  1.27


[batch =  100] train_perplexity =     3.69, train_loss =  1.31, learning_rate = 0.07812500


[batch =  200] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch =  300] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch =  400] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch =  500] train_perplexity =     3.32, train_loss =  1.20, learning_rate = 0.07812500


[batch =  600] train_perplexity =     3.29, train_loss =  1.19, learning_rate = 0.07812500


[batch =  700] train_perplexity =     3.32, train_loss =  1.20, learning_rate = 0.07812500


[batch =  800] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch =  900] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 1000] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 1100] train_perplexity =     3.57, train_loss =  1.27, learning_rate = 0.07812500


[batch = 1200] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 1300] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 1400] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1500] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1600] train_perplexity =     3.38, train_loss =  1.22, learning_rate = 0.07812500


[batch = 1700] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1800] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1900] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2000] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2100] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2200] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2300] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2400] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2500] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2600] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2700] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2800] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2900] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 3000] train_perplexity =     3.38, train_loss =  1.22, learning_rate = 0.07812500


[batch = 3100] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3200] train_perplexity =     3.59, train_loss =  1.28, learning_rate = 0.07812500


[batch = 3300] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3400] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3500] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3600] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3700] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3800] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3900] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4000] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4100] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch = 4200] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4300] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 4400] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4500] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4600] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4700] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4800] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.07812500


[batch = 4900] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5000] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 5100] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5200] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5300] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5400] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5500] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5600] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5700] train_perplexity =     3.30, train_loss =  1.19, learning_rate = 0.07812500


[batch = 5800] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5900] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 6000] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 6100] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6200] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6300] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6400] train_perplexity =     3.38, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6500] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch = 6600] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6700] train_perplexity =     3.38, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6800] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6900] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[epoch = 165] validation_perplexity =     3.55, validation_loss =  1.27

Saved model to 'e200/ptb_char_lstm.pt'...
[epoch = 165] min_validation_perplexity =     3.55, min_validation_loss =  1.27


[batch =  100] train_perplexity =     3.69, train_loss =  1.31, learning_rate = 0.07812500


[batch =  200] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch =  300] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch =  400] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch =  500] train_perplexity =     3.32, train_loss =  1.20, learning_rate = 0.07812500


[batch =  600] train_perplexity =     3.29, train_loss =  1.19, learning_rate = 0.07812500


[batch =  700] train_perplexity =     3.32, train_loss =  1.20, learning_rate = 0.07812500


[batch =  800] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch =  900] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 1000] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 1100] train_perplexity =     3.57, train_loss =  1.27, learning_rate = 0.07812500


[batch = 1200] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 1300] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 1400] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1500] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1600] train_perplexity =     3.38, train_loss =  1.22, learning_rate = 0.07812500


[batch = 1700] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1800] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1900] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2000] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2100] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2200] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2300] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2400] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2500] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2600] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2700] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2800] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2900] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 3000] train_perplexity =     3.38, train_loss =  1.22, learning_rate = 0.07812500


[batch = 3100] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3200] train_perplexity =     3.59, train_loss =  1.28, learning_rate = 0.07812500


[batch = 3300] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3400] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3500] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3600] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3700] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3800] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3900] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4000] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4100] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch = 4200] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4300] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 4400] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4500] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4600] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4700] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4800] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.07812500


[batch = 4900] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5000] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 5100] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5200] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5300] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5400] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5500] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5600] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5700] train_perplexity =     3.30, train_loss =  1.19, learning_rate = 0.07812500


[batch = 5800] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5900] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 6000] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 6100] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6200] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6300] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6400] train_perplexity =     3.38, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6500] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch = 6600] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6700] train_perplexity =     3.38, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6800] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6900] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[epoch = 166] validation_perplexity =     3.55, validation_loss =  1.27

Saved model to 'e200/ptb_char_lstm.pt'...
[epoch = 166] min_validation_perplexity =     3.55, min_validation_loss =  1.27


[batch =  100] train_perplexity =     3.69, train_loss =  1.31, learning_rate = 0.07812500


[batch =  200] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch =  300] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch =  400] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch =  500] train_perplexity =     3.32, train_loss =  1.20, learning_rate = 0.07812500


[batch =  600] train_perplexity =     3.29, train_loss =  1.19, learning_rate = 0.07812500


[batch =  700] train_perplexity =     3.32, train_loss =  1.20, learning_rate = 0.07812500


[batch =  800] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch =  900] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 1000] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 1100] train_perplexity =     3.57, train_loss =  1.27, learning_rate = 0.07812500


[batch = 1200] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 1300] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 1400] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1500] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1600] train_perplexity =     3.38, train_loss =  1.22, learning_rate = 0.07812500


[batch = 1700] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1800] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1900] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2000] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2100] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2200] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2300] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2400] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2500] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2600] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2700] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2800] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2900] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 3000] train_perplexity =     3.38, train_loss =  1.22, learning_rate = 0.07812500


[batch = 3100] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3200] train_perplexity =     3.59, train_loss =  1.28, learning_rate = 0.07812500


[batch = 3300] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3400] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3500] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3600] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3700] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3800] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3900] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4000] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4100] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch = 4200] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4300] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 4400] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4500] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4600] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4700] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4800] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.07812500


[batch = 4900] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5000] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 5100] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5200] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5300] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5400] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5500] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5600] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5700] train_perplexity =     3.30, train_loss =  1.19, learning_rate = 0.07812500


[batch = 5800] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5900] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 6000] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 6100] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6200] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6300] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6400] train_perplexity =     3.38, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6500] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch = 6600] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6700] train_perplexity =     3.38, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6800] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6900] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[epoch = 167] validation_perplexity =     3.55, validation_loss =  1.27

Saved model to 'e200/ptb_char_lstm.pt'...
[epoch = 167] min_validation_perplexity =     3.55, min_validation_loss =  1.27


[batch =  100] train_perplexity =     3.69, train_loss =  1.31, learning_rate = 0.07812500


[batch =  200] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch =  300] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch =  400] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch =  500] train_perplexity =     3.32, train_loss =  1.20, learning_rate = 0.07812500


[batch =  600] train_perplexity =     3.29, train_loss =  1.19, learning_rate = 0.07812500


[batch =  700] train_perplexity =     3.32, train_loss =  1.20, learning_rate = 0.07812500


[batch =  800] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch =  900] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 1000] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 1100] train_perplexity =     3.57, train_loss =  1.27, learning_rate = 0.07812500


[batch = 1200] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 1300] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 1400] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1500] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1600] train_perplexity =     3.38, train_loss =  1.22, learning_rate = 0.07812500


[batch = 1700] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1800] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1900] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2000] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2100] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2200] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2300] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2400] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2500] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2600] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2700] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2800] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2900] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 3000] train_perplexity =     3.38, train_loss =  1.22, learning_rate = 0.07812500


[batch = 3100] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3200] train_perplexity =     3.59, train_loss =  1.28, learning_rate = 0.07812500


[batch = 3300] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3400] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3500] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3600] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3700] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3800] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3900] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4000] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4100] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch = 4200] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4300] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 4400] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4500] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4600] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4700] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4800] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.07812500


[batch = 4900] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5000] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 5100] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5200] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5300] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5400] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5500] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5600] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5700] train_perplexity =     3.30, train_loss =  1.19, learning_rate = 0.07812500


[batch = 5800] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5900] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 6000] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 6100] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6200] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6300] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6400] train_perplexity =     3.38, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6500] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch = 6600] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6700] train_perplexity =     3.38, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6800] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6900] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[epoch = 168] validation_perplexity =     3.55, validation_loss =  1.27

Saved model to 'e200/ptb_char_lstm.pt'...
[epoch = 168] min_validation_perplexity =     3.55, min_validation_loss =  1.27


[batch =  100] train_perplexity =     3.69, train_loss =  1.31, learning_rate = 0.07812500


[batch =  200] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch =  300] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch =  400] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch =  500] train_perplexity =     3.32, train_loss =  1.20, learning_rate = 0.07812500


[batch =  600] train_perplexity =     3.29, train_loss =  1.19, learning_rate = 0.07812500


[batch =  700] train_perplexity =     3.32, train_loss =  1.20, learning_rate = 0.07812500


[batch =  800] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch =  900] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 1000] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 1100] train_perplexity =     3.57, train_loss =  1.27, learning_rate = 0.07812500


[batch = 1200] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 1300] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 1400] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1500] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1600] train_perplexity =     3.38, train_loss =  1.22, learning_rate = 0.07812500


[batch = 1700] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1800] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1900] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2000] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2100] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2200] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2300] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2400] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2500] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2600] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2700] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2800] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2900] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 3000] train_perplexity =     3.38, train_loss =  1.22, learning_rate = 0.07812500


[batch = 3100] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3200] train_perplexity =     3.59, train_loss =  1.28, learning_rate = 0.07812500


[batch = 3300] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3400] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3500] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3600] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3700] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3800] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3900] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4000] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4100] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch = 4200] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4300] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 4400] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4500] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4600] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4700] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4800] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.07812500


[batch = 4900] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5000] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 5100] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5200] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5300] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5400] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5500] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5600] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5700] train_perplexity =     3.30, train_loss =  1.19, learning_rate = 0.07812500


[batch = 5800] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5900] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 6000] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 6100] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6200] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6300] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6400] train_perplexity =     3.38, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6500] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch = 6600] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6700] train_perplexity =     3.38, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6800] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6900] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[epoch = 169] validation_perplexity =     3.55, validation_loss =  1.27

Saved model to 'e200/ptb_char_lstm.pt'...
[epoch = 169] min_validation_perplexity =     3.55, min_validation_loss =  1.27


[batch =  100] train_perplexity =     3.69, train_loss =  1.31, learning_rate = 0.07812500


[batch =  200] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch =  300] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch =  400] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch =  500] train_perplexity =     3.32, train_loss =  1.20, learning_rate = 0.07812500


[batch =  600] train_perplexity =     3.29, train_loss =  1.19, learning_rate = 0.07812500


[batch =  700] train_perplexity =     3.32, train_loss =  1.20, learning_rate = 0.07812500


[batch =  800] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch =  900] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 1000] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 1100] train_perplexity =     3.57, train_loss =  1.27, learning_rate = 0.07812500


[batch = 1200] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 1300] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 1400] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1500] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1600] train_perplexity =     3.38, train_loss =  1.22, learning_rate = 0.07812500


[batch = 1700] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1800] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1900] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2000] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2100] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2200] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2300] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2400] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2500] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2600] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2700] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2800] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2900] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 3000] train_perplexity =     3.38, train_loss =  1.22, learning_rate = 0.07812500


[batch = 3100] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3200] train_perplexity =     3.59, train_loss =  1.28, learning_rate = 0.07812500


[batch = 3300] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3400] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3500] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3600] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3700] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3800] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3900] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4000] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4100] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch = 4200] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4300] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 4400] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4500] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4600] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4700] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4800] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.07812500


[batch = 4900] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5000] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 5100] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5200] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5300] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5400] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5500] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5600] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5700] train_perplexity =     3.30, train_loss =  1.19, learning_rate = 0.07812500


[batch = 5800] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5900] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 6000] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 6100] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6200] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6300] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6400] train_perplexity =     3.38, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6500] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch = 6600] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6700] train_perplexity =     3.38, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6800] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6900] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[epoch = 170] validation_perplexity =     3.55, validation_loss =  1.27

Saved model to 'e200/ptb_char_lstm.pt'...
[epoch = 170] min_validation_perplexity =     3.55, min_validation_loss =  1.27


[batch =  100] train_perplexity =     3.69, train_loss =  1.31, learning_rate = 0.07812500


[batch =  200] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch =  300] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch =  400] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch =  500] train_perplexity =     3.32, train_loss =  1.20, learning_rate = 0.07812500


[batch =  600] train_perplexity =     3.29, train_loss =  1.19, learning_rate = 0.07812500


[batch =  700] train_perplexity =     3.32, train_loss =  1.20, learning_rate = 0.07812500


[batch =  800] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch =  900] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 1000] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 1100] train_perplexity =     3.57, train_loss =  1.27, learning_rate = 0.07812500


[batch = 1200] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 1300] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 1400] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1500] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1600] train_perplexity =     3.38, train_loss =  1.22, learning_rate = 0.07812500


[batch = 1700] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1800] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1900] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2000] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2100] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2200] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2300] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2400] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2500] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2600] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2700] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2800] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2900] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 3000] train_perplexity =     3.38, train_loss =  1.22, learning_rate = 0.07812500


[batch = 3100] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3200] train_perplexity =     3.59, train_loss =  1.28, learning_rate = 0.07812500


[batch = 3300] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3400] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3500] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3600] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3700] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3800] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3900] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4000] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4100] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch = 4200] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4300] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 4400] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4500] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4600] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4700] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4800] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.07812500


[batch = 4900] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5000] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 5100] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5200] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5300] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5400] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5500] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5600] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5700] train_perplexity =     3.30, train_loss =  1.19, learning_rate = 0.07812500


[batch = 5800] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5900] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 6000] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 6100] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6200] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6300] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6400] train_perplexity =     3.38, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6500] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch = 6600] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6700] train_perplexity =     3.38, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6800] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6900] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[epoch = 171] validation_perplexity =     3.55, validation_loss =  1.27

Saved model to 'e200/ptb_char_lstm.pt'...
[epoch = 171] min_validation_perplexity =     3.55, min_validation_loss =  1.27


[batch =  100] train_perplexity =     3.69, train_loss =  1.31, learning_rate = 0.07812500


[batch =  200] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch =  300] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.07812500


[batch =  400] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch =  500] train_perplexity =     3.32, train_loss =  1.20, learning_rate = 0.07812500


[batch =  600] train_perplexity =     3.29, train_loss =  1.19, learning_rate = 0.07812500


[batch =  700] train_perplexity =     3.32, train_loss =  1.20, learning_rate = 0.07812500


[batch =  800] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch =  900] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 1000] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 1100] train_perplexity =     3.57, train_loss =  1.27, learning_rate = 0.07812500


[batch = 1200] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 1300] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 1400] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1500] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1600] train_perplexity =     3.38, train_loss =  1.22, learning_rate = 0.07812500


[batch = 1700] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1800] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1900] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2000] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2100] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2200] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2300] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2400] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2500] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2600] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2700] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2800] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2900] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 3000] train_perplexity =     3.38, train_loss =  1.22, learning_rate = 0.07812500


[batch = 3100] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3200] train_perplexity =     3.59, train_loss =  1.28, learning_rate = 0.07812500


[batch = 3300] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3400] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3500] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3600] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3700] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3800] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3900] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4000] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4100] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch = 4200] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4300] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 4400] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4500] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4600] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4700] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4800] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.07812500


[batch = 4900] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5000] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 5100] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5200] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5300] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5400] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5500] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5600] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5700] train_perplexity =     3.30, train_loss =  1.19, learning_rate = 0.07812500


[batch = 5800] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5900] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 6000] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 6100] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6200] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6300] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6400] train_perplexity =     3.38, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6500] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch = 6600] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6700] train_perplexity =     3.38, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6800] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6900] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[epoch = 172] validation_perplexity =     3.55, validation_loss =  1.27

Saved model to 'e200/ptb_char_lstm.pt'...
[epoch = 172] min_validation_perplexity =     3.55, min_validation_loss =  1.27


[batch =  100] train_perplexity =     3.69, train_loss =  1.31, learning_rate = 0.07812500


[batch =  200] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch =  300] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.07812500


[batch =  400] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch =  500] train_perplexity =     3.32, train_loss =  1.20, learning_rate = 0.07812500


[batch =  600] train_perplexity =     3.29, train_loss =  1.19, learning_rate = 0.07812500


[batch =  700] train_perplexity =     3.32, train_loss =  1.20, learning_rate = 0.07812500


[batch =  800] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch =  900] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 1000] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 1100] train_perplexity =     3.57, train_loss =  1.27, learning_rate = 0.07812500


[batch = 1200] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 1300] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 1400] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1500] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1600] train_perplexity =     3.38, train_loss =  1.22, learning_rate = 0.07812500


[batch = 1700] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1800] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1900] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2000] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2100] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2200] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2300] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2400] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2500] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2600] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2700] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2800] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2900] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 3000] train_perplexity =     3.38, train_loss =  1.22, learning_rate = 0.07812500


[batch = 3100] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3200] train_perplexity =     3.59, train_loss =  1.28, learning_rate = 0.07812500


[batch = 3300] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3400] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3500] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3600] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3700] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3800] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3900] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4000] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4100] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch = 4200] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4300] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 4400] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4500] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4600] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4700] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4800] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.07812500


[batch = 4900] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5000] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 5100] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5200] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5300] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5400] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5500] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5600] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5700] train_perplexity =     3.30, train_loss =  1.19, learning_rate = 0.07812500


[batch = 5800] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5900] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 6000] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 6100] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6200] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6300] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6400] train_perplexity =     3.38, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6500] train_perplexity =     3.32, train_loss =  1.20, learning_rate = 0.07812500


[batch = 6600] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6700] train_perplexity =     3.38, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6800] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6900] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[epoch = 173] validation_perplexity =     3.55, validation_loss =  1.27

Saved model to 'e200/ptb_char_lstm.pt'...
[epoch = 173] min_validation_perplexity =     3.55, min_validation_loss =  1.27


[batch =  100] train_perplexity =     3.69, train_loss =  1.31, learning_rate = 0.07812500


[batch =  200] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch =  300] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.07812500


[batch =  400] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch =  500] train_perplexity =     3.32, train_loss =  1.20, learning_rate = 0.07812500


[batch =  600] train_perplexity =     3.29, train_loss =  1.19, learning_rate = 0.07812500


[batch =  700] train_perplexity =     3.32, train_loss =  1.20, learning_rate = 0.07812500


[batch =  800] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch =  900] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 1000] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 1100] train_perplexity =     3.57, train_loss =  1.27, learning_rate = 0.07812500


[batch = 1200] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 1300] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 1400] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1500] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1600] train_perplexity =     3.38, train_loss =  1.22, learning_rate = 0.07812500


[batch = 1700] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1800] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1900] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2000] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2100] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2200] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2300] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2400] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2500] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2600] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2700] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2800] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2900] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 3000] train_perplexity =     3.38, train_loss =  1.22, learning_rate = 0.07812500


[batch = 3100] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3200] train_perplexity =     3.59, train_loss =  1.28, learning_rate = 0.07812500


[batch = 3300] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3400] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3500] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3600] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3700] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3800] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3900] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4000] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4100] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch = 4200] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4300] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 4400] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4500] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4600] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4700] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4800] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.07812500


[batch = 4900] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5000] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 5100] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5200] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5300] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5400] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5500] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5600] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5700] train_perplexity =     3.30, train_loss =  1.19, learning_rate = 0.07812500


[batch = 5800] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5900] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 6000] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 6100] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6200] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6300] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6400] train_perplexity =     3.38, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6500] train_perplexity =     3.32, train_loss =  1.20, learning_rate = 0.07812500


[batch = 6600] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6700] train_perplexity =     3.38, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6800] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6900] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[epoch = 174] validation_perplexity =     3.55, validation_loss =  1.27

Saved model to 'e200/ptb_char_lstm.pt'...
[epoch = 174] min_validation_perplexity =     3.55, min_validation_loss =  1.27


[batch =  100] train_perplexity =     3.69, train_loss =  1.31, learning_rate = 0.07812500


[batch =  200] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch =  300] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.07812500


[batch =  400] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch =  500] train_perplexity =     3.32, train_loss =  1.20, learning_rate = 0.07812500


[batch =  600] train_perplexity =     3.29, train_loss =  1.19, learning_rate = 0.07812500


[batch =  700] train_perplexity =     3.32, train_loss =  1.20, learning_rate = 0.07812500


[batch =  800] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch =  900] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 1000] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 1100] train_perplexity =     3.57, train_loss =  1.27, learning_rate = 0.07812500


[batch = 1200] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 1300] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 1400] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1500] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1600] train_perplexity =     3.38, train_loss =  1.22, learning_rate = 0.07812500


[batch = 1700] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1800] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1900] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2000] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2100] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2200] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2300] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2400] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2500] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2600] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2700] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2800] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2900] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 3000] train_perplexity =     3.38, train_loss =  1.22, learning_rate = 0.07812500


[batch = 3100] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3200] train_perplexity =     3.58, train_loss =  1.28, learning_rate = 0.07812500


[batch = 3300] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3400] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3500] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3600] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3700] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3800] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3900] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4000] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4100] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch = 4200] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4300] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 4400] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4500] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4600] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4700] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4800] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.07812500


[batch = 4900] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5000] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 5100] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5200] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5300] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5400] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5500] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5600] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5700] train_perplexity =     3.30, train_loss =  1.19, learning_rate = 0.07812500


[batch = 5800] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5900] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 6000] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 6100] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6200] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6300] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6400] train_perplexity =     3.38, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6500] train_perplexity =     3.32, train_loss =  1.20, learning_rate = 0.07812500


[batch = 6600] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6700] train_perplexity =     3.38, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6800] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6900] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[epoch = 175] validation_perplexity =     3.55, validation_loss =  1.27

Saved model to 'e200/ptb_char_lstm.pt'...
[epoch = 175] min_validation_perplexity =     3.55, min_validation_loss =  1.27


[batch =  100] train_perplexity =     3.69, train_loss =  1.31, learning_rate = 0.07812500


[batch =  200] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch =  300] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.07812500


[batch =  400] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch =  500] train_perplexity =     3.32, train_loss =  1.20, learning_rate = 0.07812500


[batch =  600] train_perplexity =     3.29, train_loss =  1.19, learning_rate = 0.07812500


[batch =  700] train_perplexity =     3.32, train_loss =  1.20, learning_rate = 0.07812500


[batch =  800] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch =  900] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 1000] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 1100] train_perplexity =     3.57, train_loss =  1.27, learning_rate = 0.07812500


[batch = 1200] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 1300] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 1400] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1500] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1600] train_perplexity =     3.38, train_loss =  1.22, learning_rate = 0.07812500


[batch = 1700] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1800] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1900] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2000] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2100] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2200] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2300] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2400] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2500] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2600] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2700] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2800] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2900] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 3000] train_perplexity =     3.38, train_loss =  1.22, learning_rate = 0.07812500


[batch = 3100] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3200] train_perplexity =     3.58, train_loss =  1.28, learning_rate = 0.07812500


[batch = 3300] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3400] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3500] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3600] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3700] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3800] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3900] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4000] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4100] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch = 4200] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4300] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 4400] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4500] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4600] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4700] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4800] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.07812500


[batch = 4900] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5000] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 5100] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5200] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5300] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5400] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5500] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5600] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5700] train_perplexity =     3.30, train_loss =  1.19, learning_rate = 0.07812500


[batch = 5800] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5900] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 6000] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 6100] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6200] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6300] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6400] train_perplexity =     3.38, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6500] train_perplexity =     3.32, train_loss =  1.20, learning_rate = 0.07812500


[batch = 6600] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6700] train_perplexity =     3.38, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6800] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6900] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[epoch = 176] validation_perplexity =     3.55, validation_loss =  1.27

Saved model to 'e200/ptb_char_lstm.pt'...
[epoch = 176] min_validation_perplexity =     3.55, min_validation_loss =  1.27


[batch =  100] train_perplexity =     3.69, train_loss =  1.31, learning_rate = 0.07812500


[batch =  200] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch =  300] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.07812500


[batch =  400] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch =  500] train_perplexity =     3.32, train_loss =  1.20, learning_rate = 0.07812500


[batch =  600] train_perplexity =     3.29, train_loss =  1.19, learning_rate = 0.07812500


[batch =  700] train_perplexity =     3.32, train_loss =  1.20, learning_rate = 0.07812500


[batch =  800] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch =  900] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 1000] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 1100] train_perplexity =     3.57, train_loss =  1.27, learning_rate = 0.07812500


[batch = 1200] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 1300] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 1400] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1500] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1600] train_perplexity =     3.38, train_loss =  1.22, learning_rate = 0.07812500


[batch = 1700] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1800] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1900] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2000] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2100] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2200] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2300] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2400] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2500] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2600] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2700] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2800] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2900] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 3000] train_perplexity =     3.38, train_loss =  1.22, learning_rate = 0.07812500


[batch = 3100] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3200] train_perplexity =     3.58, train_loss =  1.28, learning_rate = 0.07812500


[batch = 3300] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3400] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3500] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3600] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3700] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3800] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3900] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4000] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4100] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch = 4200] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4300] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 4400] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4500] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4600] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4700] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4800] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.07812500


[batch = 4900] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5000] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 5100] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5200] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5300] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5400] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5500] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5600] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5700] train_perplexity =     3.30, train_loss =  1.19, learning_rate = 0.07812500


[batch = 5800] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5900] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 6000] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 6100] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6200] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6300] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6400] train_perplexity =     3.38, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6500] train_perplexity =     3.32, train_loss =  1.20, learning_rate = 0.07812500


[batch = 6600] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6700] train_perplexity =     3.38, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6800] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6900] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[epoch = 177] validation_perplexity =     3.55, validation_loss =  1.27

Saved model to 'e200/ptb_char_lstm.pt'...
[epoch = 177] min_validation_perplexity =     3.55, min_validation_loss =  1.27


[batch =  100] train_perplexity =     3.69, train_loss =  1.31, learning_rate = 0.07812500


[batch =  200] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch =  300] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.07812500


[batch =  400] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch =  500] train_perplexity =     3.32, train_loss =  1.20, learning_rate = 0.07812500


[batch =  600] train_perplexity =     3.29, train_loss =  1.19, learning_rate = 0.07812500


[batch =  700] train_perplexity =     3.32, train_loss =  1.20, learning_rate = 0.07812500


[batch =  800] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch =  900] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 1000] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 1100] train_perplexity =     3.57, train_loss =  1.27, learning_rate = 0.07812500


[batch = 1200] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 1300] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 1400] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1500] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1600] train_perplexity =     3.38, train_loss =  1.22, learning_rate = 0.07812500


[batch = 1700] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1800] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1900] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2000] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2100] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2200] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2300] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2400] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2500] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2600] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2700] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2800] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2900] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 3000] train_perplexity =     3.38, train_loss =  1.22, learning_rate = 0.07812500


[batch = 3100] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3200] train_perplexity =     3.58, train_loss =  1.28, learning_rate = 0.07812500


[batch = 3300] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3400] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3500] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3600] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3700] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3800] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3900] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4000] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4100] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch = 4200] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4300] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 4400] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4500] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4600] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4700] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4800] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.07812500


[batch = 4900] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5000] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 5100] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5200] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5300] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5400] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5500] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5600] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5700] train_perplexity =     3.30, train_loss =  1.19, learning_rate = 0.07812500


[batch = 5800] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5900] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 6000] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 6100] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6200] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6300] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6400] train_perplexity =     3.38, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6500] train_perplexity =     3.32, train_loss =  1.20, learning_rate = 0.07812500


[batch = 6600] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6700] train_perplexity =     3.38, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6800] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6900] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[epoch = 178] validation_perplexity =     3.55, validation_loss =  1.27

Saved model to 'e200/ptb_char_lstm.pt'...
[epoch = 178] min_validation_perplexity =     3.55, min_validation_loss =  1.27


[batch =  100] train_perplexity =     3.69, train_loss =  1.31, learning_rate = 0.07812500


[batch =  200] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch =  300] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.07812500


[batch =  400] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch =  500] train_perplexity =     3.32, train_loss =  1.20, learning_rate = 0.07812500


[batch =  600] train_perplexity =     3.29, train_loss =  1.19, learning_rate = 0.07812500


[batch =  700] train_perplexity =     3.32, train_loss =  1.20, learning_rate = 0.07812500


[batch =  800] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch =  900] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 1000] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 1100] train_perplexity =     3.57, train_loss =  1.27, learning_rate = 0.07812500


[batch = 1200] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 1300] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 1400] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1500] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1600] train_perplexity =     3.38, train_loss =  1.22, learning_rate = 0.07812500


[batch = 1700] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1800] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1900] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2000] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2100] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2200] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2300] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2400] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2500] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2600] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2700] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2800] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2900] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 3000] train_perplexity =     3.38, train_loss =  1.22, learning_rate = 0.07812500


[batch = 3100] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3200] train_perplexity =     3.58, train_loss =  1.28, learning_rate = 0.07812500


[batch = 3300] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3400] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3500] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3600] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3700] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3800] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3900] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4000] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4100] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch = 4200] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4300] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 4400] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4500] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4600] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4700] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4800] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.07812500


[batch = 4900] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5000] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 5100] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5200] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5300] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5400] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5500] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5600] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5700] train_perplexity =     3.30, train_loss =  1.19, learning_rate = 0.07812500


[batch = 5800] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5900] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 6000] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 6100] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6200] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6300] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6400] train_perplexity =     3.38, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6500] train_perplexity =     3.32, train_loss =  1.20, learning_rate = 0.07812500


[batch = 6600] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6700] train_perplexity =     3.38, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6800] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6900] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[epoch = 179] validation_perplexity =     3.55, validation_loss =  1.27

Saved model to 'e200/ptb_char_lstm.pt'...
[epoch = 179] min_validation_perplexity =     3.55, min_validation_loss =  1.27


[batch =  100] train_perplexity =     3.69, train_loss =  1.31, learning_rate = 0.07812500


[batch =  200] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch =  300] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.07812500


[batch =  400] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch =  500] train_perplexity =     3.32, train_loss =  1.20, learning_rate = 0.07812500


[batch =  600] train_perplexity =     3.29, train_loss =  1.19, learning_rate = 0.07812500


[batch =  700] train_perplexity =     3.32, train_loss =  1.20, learning_rate = 0.07812500


[batch =  800] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch =  900] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 1000] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 1100] train_perplexity =     3.57, train_loss =  1.27, learning_rate = 0.07812500


[batch = 1200] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 1300] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 1400] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1500] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1600] train_perplexity =     3.38, train_loss =  1.22, learning_rate = 0.07812500


[batch = 1700] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1800] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1900] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2000] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2100] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2200] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2300] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2400] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2500] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2600] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2700] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2800] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2900] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 3000] train_perplexity =     3.38, train_loss =  1.22, learning_rate = 0.07812500


[batch = 3100] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3200] train_perplexity =     3.58, train_loss =  1.28, learning_rate = 0.07812500


[batch = 3300] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3400] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3500] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3600] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3700] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3800] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3900] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4000] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4100] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch = 4200] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4300] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 4400] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4500] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4600] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4700] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4800] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.07812500


[batch = 4900] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5000] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 5100] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5200] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5300] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5400] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5500] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5600] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5700] train_perplexity =     3.30, train_loss =  1.19, learning_rate = 0.07812500


[batch = 5800] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5900] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 6000] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 6100] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6200] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6300] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6400] train_perplexity =     3.38, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6500] train_perplexity =     3.32, train_loss =  1.20, learning_rate = 0.07812500


[batch = 6600] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6700] train_perplexity =     3.38, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6800] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6900] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[epoch = 180] validation_perplexity =     3.55, validation_loss =  1.27

Saved model to 'e200/ptb_char_lstm.pt'...
[epoch = 180] min_validation_perplexity =     3.55, min_validation_loss =  1.27


[batch =  100] train_perplexity =     3.69, train_loss =  1.31, learning_rate = 0.07812500


[batch =  200] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch =  300] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.07812500


[batch =  400] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch =  500] train_perplexity =     3.32, train_loss =  1.20, learning_rate = 0.07812500


[batch =  600] train_perplexity =     3.29, train_loss =  1.19, learning_rate = 0.07812500


[batch =  700] train_perplexity =     3.32, train_loss =  1.20, learning_rate = 0.07812500


[batch =  800] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch =  900] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 1000] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 1100] train_perplexity =     3.57, train_loss =  1.27, learning_rate = 0.07812500


[batch = 1200] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 1300] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 1400] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1500] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1600] train_perplexity =     3.38, train_loss =  1.22, learning_rate = 0.07812500


[batch = 1700] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1800] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1900] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2000] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2100] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2200] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2300] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2400] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2500] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2600] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2700] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2800] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2900] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 3000] train_perplexity =     3.38, train_loss =  1.22, learning_rate = 0.07812500


[batch = 3100] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3200] train_perplexity =     3.58, train_loss =  1.28, learning_rate = 0.07812500


[batch = 3300] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3400] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3500] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3600] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3700] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3800] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3900] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4000] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4100] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch = 4200] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4300] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 4400] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4500] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4600] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4700] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4800] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.07812500


[batch = 4900] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5000] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 5100] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5200] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5300] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5400] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5500] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5600] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5700] train_perplexity =     3.30, train_loss =  1.19, learning_rate = 0.07812500


[batch = 5800] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5900] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 6000] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 6100] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6200] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6300] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6400] train_perplexity =     3.38, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6500] train_perplexity =     3.32, train_loss =  1.20, learning_rate = 0.07812500


[batch = 6600] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6700] train_perplexity =     3.38, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6800] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6900] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[epoch = 181] validation_perplexity =     3.55, validation_loss =  1.27

Saved model to 'e200/ptb_char_lstm.pt'...
[epoch = 181] min_validation_perplexity =     3.55, min_validation_loss =  1.27


[batch =  100] train_perplexity =     3.69, train_loss =  1.31, learning_rate = 0.07812500


[batch =  200] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch =  300] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.07812500


[batch =  400] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch =  500] train_perplexity =     3.32, train_loss =  1.20, learning_rate = 0.07812500


[batch =  600] train_perplexity =     3.29, train_loss =  1.19, learning_rate = 0.07812500


[batch =  700] train_perplexity =     3.32, train_loss =  1.20, learning_rate = 0.07812500


[batch =  800] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch =  900] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 1000] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 1100] train_perplexity =     3.57, train_loss =  1.27, learning_rate = 0.07812500


[batch = 1200] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 1300] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 1400] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1500] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1600] train_perplexity =     3.38, train_loss =  1.22, learning_rate = 0.07812500


[batch = 1700] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1800] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1900] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2000] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2100] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2200] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2300] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2400] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2500] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2600] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2700] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2800] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2900] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 3000] train_perplexity =     3.38, train_loss =  1.22, learning_rate = 0.07812500


[batch = 3100] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3200] train_perplexity =     3.58, train_loss =  1.28, learning_rate = 0.07812500


[batch = 3300] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3400] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3500] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3600] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3700] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3800] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3900] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4000] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4100] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch = 4200] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4300] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 4400] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4500] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4600] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4700] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4800] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.07812500


[batch = 4900] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5000] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 5100] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5200] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5300] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5400] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5500] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5600] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5700] train_perplexity =     3.30, train_loss =  1.19, learning_rate = 0.07812500


[batch = 5800] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5900] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 6000] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 6100] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6200] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6300] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6400] train_perplexity =     3.38, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6500] train_perplexity =     3.32, train_loss =  1.20, learning_rate = 0.07812500


[batch = 6600] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6700] train_perplexity =     3.38, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6800] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6900] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[epoch = 182] validation_perplexity =     3.55, validation_loss =  1.27

Saved model to 'e200/ptb_char_lstm.pt'...
[epoch = 182] min_validation_perplexity =     3.55, min_validation_loss =  1.27


[batch =  100] train_perplexity =     3.69, train_loss =  1.31, learning_rate = 0.07812500


[batch =  200] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch =  300] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.07812500


[batch =  400] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch =  500] train_perplexity =     3.32, train_loss =  1.20, learning_rate = 0.07812500


[batch =  600] train_perplexity =     3.29, train_loss =  1.19, learning_rate = 0.07812500


[batch =  700] train_perplexity =     3.32, train_loss =  1.20, learning_rate = 0.07812500


[batch =  800] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch =  900] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 1000] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 1100] train_perplexity =     3.57, train_loss =  1.27, learning_rate = 0.07812500


[batch = 1200] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 1300] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 1400] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1500] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1600] train_perplexity =     3.38, train_loss =  1.22, learning_rate = 0.07812500


[batch = 1700] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1800] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1900] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2000] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2100] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2200] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2300] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2400] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2500] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2600] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2700] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2800] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2900] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 3000] train_perplexity =     3.38, train_loss =  1.22, learning_rate = 0.07812500


[batch = 3100] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3200] train_perplexity =     3.58, train_loss =  1.28, learning_rate = 0.07812500


[batch = 3300] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3400] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3500] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3600] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3700] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3800] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3900] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4000] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4100] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch = 4200] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4300] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 4400] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4500] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4600] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4700] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4800] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.07812500


[batch = 4900] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5000] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 5100] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5200] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5300] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5400] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5500] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5600] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5700] train_perplexity =     3.30, train_loss =  1.19, learning_rate = 0.07812500


[batch = 5800] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5900] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 6000] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 6100] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6200] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6300] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6400] train_perplexity =     3.38, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6500] train_perplexity =     3.32, train_loss =  1.20, learning_rate = 0.07812500


[batch = 6600] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6700] train_perplexity =     3.38, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6800] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6900] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[epoch = 183] validation_perplexity =     3.55, validation_loss =  1.27

Saved model to 'e200/ptb_char_lstm.pt'...
[epoch = 183] min_validation_perplexity =     3.55, min_validation_loss =  1.27


[batch =  100] train_perplexity =     3.69, train_loss =  1.31, learning_rate = 0.07812500


[batch =  200] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch =  300] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.07812500


[batch =  400] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch =  500] train_perplexity =     3.32, train_loss =  1.20, learning_rate = 0.07812500


[batch =  600] train_perplexity =     3.29, train_loss =  1.19, learning_rate = 0.07812500


[batch =  700] train_perplexity =     3.32, train_loss =  1.20, learning_rate = 0.07812500


[batch =  800] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch =  900] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 1000] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 1100] train_perplexity =     3.57, train_loss =  1.27, learning_rate = 0.07812500


[batch = 1200] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 1300] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 1400] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1500] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1600] train_perplexity =     3.38, train_loss =  1.22, learning_rate = 0.07812500


[batch = 1700] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1800] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1900] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2000] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2100] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2200] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2300] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2400] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2500] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2600] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2700] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2800] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2900] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 3000] train_perplexity =     3.38, train_loss =  1.22, learning_rate = 0.07812500


[batch = 3100] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3200] train_perplexity =     3.58, train_loss =  1.28, learning_rate = 0.07812500


[batch = 3300] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3400] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3500] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3600] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3700] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3800] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3900] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4000] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4100] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch = 4200] train_perplexity =     3.40, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4300] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 4400] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4500] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4600] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4700] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4800] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.07812500


[batch = 4900] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5000] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 5100] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5200] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5300] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5400] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5500] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5600] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5700] train_perplexity =     3.30, train_loss =  1.19, learning_rate = 0.07812500


[batch = 5800] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5900] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 6000] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 6100] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6200] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6300] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6400] train_perplexity =     3.38, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6500] train_perplexity =     3.32, train_loss =  1.20, learning_rate = 0.07812500


[batch = 6600] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6700] train_perplexity =     3.38, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6800] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6900] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[epoch = 184] validation_perplexity =     3.55, validation_loss =  1.27

Saved model to 'e200/ptb_char_lstm.pt'...
[epoch = 184] min_validation_perplexity =     3.55, min_validation_loss =  1.27


[batch =  100] train_perplexity =     3.69, train_loss =  1.31, learning_rate = 0.07812500


[batch =  200] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch =  300] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.07812500


[batch =  400] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch =  500] train_perplexity =     3.32, train_loss =  1.20, learning_rate = 0.07812500


[batch =  600] train_perplexity =     3.29, train_loss =  1.19, learning_rate = 0.07812500


[batch =  700] train_perplexity =     3.32, train_loss =  1.20, learning_rate = 0.07812500


[batch =  800] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch =  900] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 1000] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 1100] train_perplexity =     3.57, train_loss =  1.27, learning_rate = 0.07812500


[batch = 1200] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 1300] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 1400] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1500] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1600] train_perplexity =     3.38, train_loss =  1.22, learning_rate = 0.07812500


[batch = 1700] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1800] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1900] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2000] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2100] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2200] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2300] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2400] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2500] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2600] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2700] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2800] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2900] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 3000] train_perplexity =     3.38, train_loss =  1.22, learning_rate = 0.07812500


[batch = 3100] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3200] train_perplexity =     3.58, train_loss =  1.28, learning_rate = 0.07812500


[batch = 3300] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3400] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3500] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3600] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3700] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3800] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3900] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4000] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4100] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch = 4200] train_perplexity =     3.40, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4300] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 4400] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4500] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4600] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4700] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4800] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.07812500


[batch = 4900] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5000] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 5100] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5200] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5300] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5400] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5500] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5600] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5700] train_perplexity =     3.30, train_loss =  1.19, learning_rate = 0.07812500


[batch = 5800] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5900] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 6000] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 6100] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6200] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6300] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6400] train_perplexity =     3.38, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6500] train_perplexity =     3.32, train_loss =  1.20, learning_rate = 0.07812500


[batch = 6600] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6700] train_perplexity =     3.38, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6800] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6900] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[epoch = 185] validation_perplexity =     3.55, validation_loss =  1.27

Saved model to 'e200/ptb_char_lstm.pt'...
[epoch = 185] min_validation_perplexity =     3.55, min_validation_loss =  1.27


[batch =  100] train_perplexity =     3.69, train_loss =  1.31, learning_rate = 0.07812500


[batch =  200] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch =  300] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.07812500


[batch =  400] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch =  500] train_perplexity =     3.32, train_loss =  1.20, learning_rate = 0.07812500


[batch =  600] train_perplexity =     3.29, train_loss =  1.19, learning_rate = 0.07812500


[batch =  700] train_perplexity =     3.32, train_loss =  1.20, learning_rate = 0.07812500


[batch =  800] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch =  900] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 1000] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 1100] train_perplexity =     3.57, train_loss =  1.27, learning_rate = 0.07812500


[batch = 1200] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 1300] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 1400] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1500] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1600] train_perplexity =     3.38, train_loss =  1.22, learning_rate = 0.07812500


[batch = 1700] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1800] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1900] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2000] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2100] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2200] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2300] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2400] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2500] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2600] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2700] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2800] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2900] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 3000] train_perplexity =     3.38, train_loss =  1.22, learning_rate = 0.07812500


[batch = 3100] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3200] train_perplexity =     3.58, train_loss =  1.28, learning_rate = 0.07812500


[batch = 3300] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3400] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3500] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3600] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3700] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3800] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3900] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4000] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4100] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch = 4200] train_perplexity =     3.40, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4300] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 4400] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4500] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4600] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4700] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4800] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.07812500


[batch = 4900] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5000] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 5100] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5200] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5300] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5400] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5500] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5600] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5700] train_perplexity =     3.30, train_loss =  1.19, learning_rate = 0.07812500


[batch = 5800] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5900] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 6000] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 6100] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6200] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6300] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6400] train_perplexity =     3.38, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6500] train_perplexity =     3.32, train_loss =  1.20, learning_rate = 0.07812500


[batch = 6600] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6700] train_perplexity =     3.38, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6800] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6900] train_perplexity =     3.51, train_loss =  1.25, learning_rate = 0.07812500


[epoch = 186] validation_perplexity =     3.55, validation_loss =  1.27

Saved model to 'e200/ptb_char_lstm.pt'...
[epoch = 186] min_validation_perplexity =     3.55, min_validation_loss =  1.27


[batch =  100] train_perplexity =     3.69, train_loss =  1.31, learning_rate = 0.07812500


[batch =  200] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch =  300] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.07812500


[batch =  400] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch =  500] train_perplexity =     3.32, train_loss =  1.20, learning_rate = 0.07812500


[batch =  600] train_perplexity =     3.29, train_loss =  1.19, learning_rate = 0.07812500


[batch =  700] train_perplexity =     3.32, train_loss =  1.20, learning_rate = 0.07812500


[batch =  800] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch =  900] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 1000] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 1100] train_perplexity =     3.57, train_loss =  1.27, learning_rate = 0.07812500


[batch = 1200] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 1300] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 1400] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1500] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1600] train_perplexity =     3.38, train_loss =  1.22, learning_rate = 0.07812500


[batch = 1700] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1800] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1900] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2000] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2100] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2200] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2300] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2400] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2500] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2600] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2700] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2800] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2900] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 3000] train_perplexity =     3.38, train_loss =  1.22, learning_rate = 0.07812500


[batch = 3100] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3200] train_perplexity =     3.58, train_loss =  1.28, learning_rate = 0.07812500


[batch = 3300] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3400] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3500] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3600] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3700] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3800] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3900] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4000] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4100] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch = 4200] train_perplexity =     3.40, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4300] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 4400] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4500] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4600] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4700] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4800] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.07812500


[batch = 4900] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5000] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 5100] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5200] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5300] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5400] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5500] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5600] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5700] train_perplexity =     3.30, train_loss =  1.19, learning_rate = 0.07812500


[batch = 5800] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5900] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 6000] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 6100] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6200] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6300] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6400] train_perplexity =     3.38, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6500] train_perplexity =     3.32, train_loss =  1.20, learning_rate = 0.07812500


[batch = 6600] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6700] train_perplexity =     3.38, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6800] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6900] train_perplexity =     3.51, train_loss =  1.25, learning_rate = 0.07812500


[epoch = 187] validation_perplexity =     3.55, validation_loss =  1.27

Saved model to 'e200/ptb_char_lstm.pt'...
[epoch = 187] min_validation_perplexity =     3.55, min_validation_loss =  1.27


[batch =  100] train_perplexity =     3.69, train_loss =  1.30, learning_rate = 0.07812500


[batch =  200] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch =  300] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.07812500


[batch =  400] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch =  500] train_perplexity =     3.32, train_loss =  1.20, learning_rate = 0.07812500


[batch =  600] train_perplexity =     3.29, train_loss =  1.19, learning_rate = 0.07812500


[batch =  700] train_perplexity =     3.32, train_loss =  1.20, learning_rate = 0.07812500


[batch =  800] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch =  900] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 1000] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 1100] train_perplexity =     3.57, train_loss =  1.27, learning_rate = 0.07812500


[batch = 1200] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 1300] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 1400] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1500] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1600] train_perplexity =     3.38, train_loss =  1.22, learning_rate = 0.07812500


[batch = 1700] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1800] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1900] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2000] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2100] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2200] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2300] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2400] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2500] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2600] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2700] train_perplexity =     3.54, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2800] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2900] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 3000] train_perplexity =     3.38, train_loss =  1.22, learning_rate = 0.07812500


[batch = 3100] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3200] train_perplexity =     3.58, train_loss =  1.28, learning_rate = 0.07812500


[batch = 3300] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3400] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3500] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3600] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3700] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3800] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3900] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4000] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4100] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch = 4200] train_perplexity =     3.40, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4300] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 4400] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4500] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4600] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4700] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4800] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.07812500


[batch = 4900] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5000] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 5100] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5200] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5300] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5400] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5500] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5600] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5700] train_perplexity =     3.30, train_loss =  1.19, learning_rate = 0.07812500


[batch = 5800] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5900] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 6000] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 6100] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6200] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6300] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6400] train_perplexity =     3.38, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6500] train_perplexity =     3.32, train_loss =  1.20, learning_rate = 0.07812500


[batch = 6600] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6700] train_perplexity =     3.38, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6800] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6900] train_perplexity =     3.51, train_loss =  1.25, learning_rate = 0.07812500


[epoch = 188] validation_perplexity =     3.55, validation_loss =  1.27

Saved model to 'e200/ptb_char_lstm.pt'...
[epoch = 188] min_validation_perplexity =     3.55, min_validation_loss =  1.27


[batch =  100] train_perplexity =     3.69, train_loss =  1.30, learning_rate = 0.07812500


[batch =  200] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch =  300] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.07812500


[batch =  400] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch =  500] train_perplexity =     3.32, train_loss =  1.20, learning_rate = 0.07812500


[batch =  600] train_perplexity =     3.29, train_loss =  1.19, learning_rate = 0.07812500


[batch =  700] train_perplexity =     3.32, train_loss =  1.20, learning_rate = 0.07812500


[batch =  800] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch =  900] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 1000] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 1100] train_perplexity =     3.57, train_loss =  1.27, learning_rate = 0.07812500


[batch = 1200] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 1300] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 1400] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1500] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1600] train_perplexity =     3.38, train_loss =  1.22, learning_rate = 0.07812500


[batch = 1700] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1800] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1900] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2000] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2100] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2200] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2300] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2400] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2500] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2600] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2700] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2800] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2900] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 3000] train_perplexity =     3.38, train_loss =  1.22, learning_rate = 0.07812500


[batch = 3100] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3200] train_perplexity =     3.58, train_loss =  1.28, learning_rate = 0.07812500


[batch = 3300] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3400] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3500] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3600] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3700] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3800] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3900] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4000] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4100] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch = 4200] train_perplexity =     3.40, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4300] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 4400] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4500] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4600] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4700] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4800] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.07812500


[batch = 4900] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5000] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 5100] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5200] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5300] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5400] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5500] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5600] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5700] train_perplexity =     3.30, train_loss =  1.19, learning_rate = 0.07812500


[batch = 5800] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5900] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 6000] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 6100] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6200] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6300] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6400] train_perplexity =     3.38, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6500] train_perplexity =     3.32, train_loss =  1.20, learning_rate = 0.07812500


[batch = 6600] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6700] train_perplexity =     3.38, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6800] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6900] train_perplexity =     3.51, train_loss =  1.25, learning_rate = 0.07812500


[epoch = 189] validation_perplexity =     3.55, validation_loss =  1.27

Saved model to 'e200/ptb_char_lstm.pt'...
[epoch = 189] min_validation_perplexity =     3.55, min_validation_loss =  1.27


[batch =  100] train_perplexity =     3.69, train_loss =  1.30, learning_rate = 0.07812500


[batch =  200] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch =  300] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.07812500


[batch =  400] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch =  500] train_perplexity =     3.32, train_loss =  1.20, learning_rate = 0.07812500


[batch =  600] train_perplexity =     3.29, train_loss =  1.19, learning_rate = 0.07812500


[batch =  700] train_perplexity =     3.32, train_loss =  1.20, learning_rate = 0.07812500


[batch =  800] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch =  900] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 1000] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 1100] train_perplexity =     3.57, train_loss =  1.27, learning_rate = 0.07812500


[batch = 1200] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 1300] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 1400] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1500] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1600] train_perplexity =     3.38, train_loss =  1.22, learning_rate = 0.07812500


[batch = 1700] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1800] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1900] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2000] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2100] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2200] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2300] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2400] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2500] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2600] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2700] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2800] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2900] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 3000] train_perplexity =     3.38, train_loss =  1.22, learning_rate = 0.07812500


[batch = 3100] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3200] train_perplexity =     3.58, train_loss =  1.28, learning_rate = 0.07812500


[batch = 3300] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3400] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3500] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3600] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3700] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3800] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3900] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4000] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4100] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch = 4200] train_perplexity =     3.40, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4300] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 4400] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4500] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4600] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4700] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4800] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.07812500


[batch = 4900] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5000] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 5100] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5200] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5300] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5400] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5500] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5600] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5700] train_perplexity =     3.30, train_loss =  1.19, learning_rate = 0.07812500


[batch = 5800] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5900] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.07812500


[batch = 6000] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 6100] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6200] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6300] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6400] train_perplexity =     3.38, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6500] train_perplexity =     3.32, train_loss =  1.20, learning_rate = 0.07812500


[batch = 6600] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6700] train_perplexity =     3.38, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6800] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6900] train_perplexity =     3.51, train_loss =  1.25, learning_rate = 0.07812500


[epoch = 190] validation_perplexity =     3.55, validation_loss =  1.27

Saved model to 'e200/ptb_char_lstm.pt'...
[epoch = 190] min_validation_perplexity =     3.55, min_validation_loss =  1.27


[batch =  100] train_perplexity =     3.69, train_loss =  1.30, learning_rate = 0.07812500


[batch =  200] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch =  300] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.07812500


[batch =  400] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch =  500] train_perplexity =     3.32, train_loss =  1.20, learning_rate = 0.07812500


[batch =  600] train_perplexity =     3.29, train_loss =  1.19, learning_rate = 0.07812500


[batch =  700] train_perplexity =     3.32, train_loss =  1.20, learning_rate = 0.07812500


[batch =  800] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch =  900] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 1000] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 1100] train_perplexity =     3.57, train_loss =  1.27, learning_rate = 0.07812500


[batch = 1200] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 1300] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 1400] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1500] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1600] train_perplexity =     3.38, train_loss =  1.22, learning_rate = 0.07812500


[batch = 1700] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1800] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1900] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2000] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2100] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2200] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2300] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2400] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2500] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2600] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2700] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2800] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2900] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 3000] train_perplexity =     3.38, train_loss =  1.22, learning_rate = 0.07812500


[batch = 3100] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3200] train_perplexity =     3.58, train_loss =  1.28, learning_rate = 0.07812500


[batch = 3300] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3400] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3500] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3600] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3700] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3800] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3900] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4000] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4100] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch = 4200] train_perplexity =     3.40, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4300] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 4400] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4500] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4600] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4700] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4800] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.07812500


[batch = 4900] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5000] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 5100] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5200] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5300] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5400] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5500] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5600] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5700] train_perplexity =     3.30, train_loss =  1.19, learning_rate = 0.07812500


[batch = 5800] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5900] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.07812500


[batch = 6000] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 6100] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6200] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6300] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6400] train_perplexity =     3.38, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6500] train_perplexity =     3.32, train_loss =  1.20, learning_rate = 0.07812500


[batch = 6600] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6700] train_perplexity =     3.38, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6800] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6900] train_perplexity =     3.51, train_loss =  1.25, learning_rate = 0.07812500


[epoch = 191] validation_perplexity =     3.55, validation_loss =  1.27

Saved model to 'e200/ptb_char_lstm.pt'...
[epoch = 191] min_validation_perplexity =     3.55, min_validation_loss =  1.27


[batch =  100] train_perplexity =     3.69, train_loss =  1.30, learning_rate = 0.07812500


[batch =  200] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch =  300] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.07812500


[batch =  400] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch =  500] train_perplexity =     3.32, train_loss =  1.20, learning_rate = 0.07812500


[batch =  600] train_perplexity =     3.29, train_loss =  1.19, learning_rate = 0.07812500


[batch =  700] train_perplexity =     3.31, train_loss =  1.20, learning_rate = 0.07812500


[batch =  800] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch =  900] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 1000] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 1100] train_perplexity =     3.57, train_loss =  1.27, learning_rate = 0.07812500


[batch = 1200] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 1300] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 1400] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1500] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1600] train_perplexity =     3.38, train_loss =  1.22, learning_rate = 0.07812500


[batch = 1700] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1800] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1900] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2000] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2100] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2200] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2300] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2400] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2500] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2600] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2700] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2800] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2900] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 3000] train_perplexity =     3.38, train_loss =  1.22, learning_rate = 0.07812500


[batch = 3100] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3200] train_perplexity =     3.58, train_loss =  1.28, learning_rate = 0.07812500


[batch = 3300] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3400] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3500] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3600] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3700] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3800] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3900] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4000] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4100] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch = 4200] train_perplexity =     3.40, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4300] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 4400] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4500] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4600] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4700] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4800] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.07812500


[batch = 4900] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5000] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 5100] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5200] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5300] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5400] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5500] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5600] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5700] train_perplexity =     3.30, train_loss =  1.19, learning_rate = 0.07812500


[batch = 5800] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5900] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.07812500


[batch = 6000] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 6100] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6200] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6300] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6400] train_perplexity =     3.38, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6500] train_perplexity =     3.32, train_loss =  1.20, learning_rate = 0.07812500


[batch = 6600] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6700] train_perplexity =     3.38, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6800] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6900] train_perplexity =     3.51, train_loss =  1.25, learning_rate = 0.07812500


[epoch = 192] validation_perplexity =     3.55, validation_loss =  1.27

Saved model to 'e200/ptb_char_lstm.pt'...
[epoch = 192] min_validation_perplexity =     3.55, min_validation_loss =  1.27


[batch =  100] train_perplexity =     3.69, train_loss =  1.30, learning_rate = 0.07812500


[batch =  200] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch =  300] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.07812500


[batch =  400] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch =  500] train_perplexity =     3.32, train_loss =  1.20, learning_rate = 0.07812500


[batch =  600] train_perplexity =     3.29, train_loss =  1.19, learning_rate = 0.07812500


[batch =  700] train_perplexity =     3.31, train_loss =  1.20, learning_rate = 0.07812500


[batch =  800] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch =  900] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 1000] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 1100] train_perplexity =     3.57, train_loss =  1.27, learning_rate = 0.07812500


[batch = 1200] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 1300] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 1400] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1500] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1600] train_perplexity =     3.38, train_loss =  1.22, learning_rate = 0.07812500


[batch = 1700] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1800] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1900] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2000] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2100] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2200] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2300] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2400] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2500] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2600] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2700] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2800] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2900] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 3000] train_perplexity =     3.38, train_loss =  1.22, learning_rate = 0.07812500


[batch = 3100] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3200] train_perplexity =     3.58, train_loss =  1.28, learning_rate = 0.07812500


[batch = 3300] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3400] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3500] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3600] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3700] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3800] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3900] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4000] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4100] train_perplexity =     3.32, train_loss =  1.20, learning_rate = 0.07812500


[batch = 4200] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 4300] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 4400] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4500] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4600] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4700] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4800] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.07812500


[batch = 4900] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5000] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 5100] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5200] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5300] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5400] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5500] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5600] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5700] train_perplexity =     3.30, train_loss =  1.19, learning_rate = 0.07812500


[batch = 5800] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5900] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.07812500


[batch = 6000] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 6100] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6200] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6300] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6400] train_perplexity =     3.38, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6500] train_perplexity =     3.32, train_loss =  1.20, learning_rate = 0.07812500


[batch = 6600] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6700] train_perplexity =     3.38, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6800] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6900] train_perplexity =     3.51, train_loss =  1.25, learning_rate = 0.07812500


[epoch = 193] validation_perplexity =     3.55, validation_loss =  1.27

Saved model to 'e200/ptb_char_lstm.pt'...
[epoch = 193] min_validation_perplexity =     3.55, min_validation_loss =  1.27


[batch =  100] train_perplexity =     3.69, train_loss =  1.30, learning_rate = 0.07812500


[batch =  200] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch =  300] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.07812500


[batch =  400] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch =  500] train_perplexity =     3.32, train_loss =  1.20, learning_rate = 0.07812500


[batch =  600] train_perplexity =     3.29, train_loss =  1.19, learning_rate = 0.07812500


[batch =  700] train_perplexity =     3.31, train_loss =  1.20, learning_rate = 0.07812500


[batch =  800] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch =  900] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 1000] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 1100] train_perplexity =     3.57, train_loss =  1.27, learning_rate = 0.07812500


[batch = 1200] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 1300] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 1400] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1500] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1600] train_perplexity =     3.38, train_loss =  1.22, learning_rate = 0.07812500


[batch = 1700] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1800] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1900] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2000] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2100] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2200] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2300] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2400] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2500] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2600] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2700] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2800] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2900] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 3000] train_perplexity =     3.38, train_loss =  1.22, learning_rate = 0.07812500


[batch = 3100] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3200] train_perplexity =     3.58, train_loss =  1.28, learning_rate = 0.07812500


[batch = 3300] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3400] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.07812500


[batch = 3500] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3600] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3700] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3800] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3900] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4000] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4100] train_perplexity =     3.32, train_loss =  1.20, learning_rate = 0.07812500


[batch = 4200] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 4300] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 4400] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4500] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4600] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4700] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4800] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.07812500


[batch = 4900] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5000] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 5100] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5200] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5300] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5400] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5500] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5600] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5700] train_perplexity =     3.30, train_loss =  1.19, learning_rate = 0.07812500


[batch = 5800] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5900] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.07812500


[batch = 6000] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 6100] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6200] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6300] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6400] train_perplexity =     3.38, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6500] train_perplexity =     3.32, train_loss =  1.20, learning_rate = 0.07812500


[batch = 6600] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6700] train_perplexity =     3.38, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6800] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6900] train_perplexity =     3.51, train_loss =  1.25, learning_rate = 0.07812500


[epoch = 194] validation_perplexity =     3.55, validation_loss =  1.27

Saved model to 'e200/ptb_char_lstm.pt'...
[epoch = 194] min_validation_perplexity =     3.55, min_validation_loss =  1.27


[batch =  100] train_perplexity =     3.69, train_loss =  1.30, learning_rate = 0.07812500


[batch =  200] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch =  300] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.07812500


[batch =  400] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch =  500] train_perplexity =     3.32, train_loss =  1.20, learning_rate = 0.07812500


[batch =  600] train_perplexity =     3.29, train_loss =  1.19, learning_rate = 0.07812500


[batch =  700] train_perplexity =     3.31, train_loss =  1.20, learning_rate = 0.07812500


[batch =  800] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch =  900] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 1000] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 1100] train_perplexity =     3.57, train_loss =  1.27, learning_rate = 0.07812500


[batch = 1200] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 1300] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 1400] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1500] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1600] train_perplexity =     3.38, train_loss =  1.22, learning_rate = 0.07812500


[batch = 1700] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1800] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1900] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2000] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2100] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2200] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2300] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2400] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2500] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2600] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2700] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2800] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2900] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 3000] train_perplexity =     3.38, train_loss =  1.22, learning_rate = 0.07812500


[batch = 3100] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3200] train_perplexity =     3.58, train_loss =  1.28, learning_rate = 0.07812500


[batch = 3300] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3400] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.07812500


[batch = 3500] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3600] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3700] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3800] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3900] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4000] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4100] train_perplexity =     3.32, train_loss =  1.20, learning_rate = 0.07812500


[batch = 4200] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 4300] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 4400] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4500] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4600] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4700] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4800] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.07812500


[batch = 4900] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5000] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 5100] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5200] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5300] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5400] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5500] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5600] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5700] train_perplexity =     3.30, train_loss =  1.19, learning_rate = 0.07812500


[batch = 5800] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5900] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.07812500


[batch = 6000] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 6100] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6200] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6300] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6400] train_perplexity =     3.38, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6500] train_perplexity =     3.32, train_loss =  1.20, learning_rate = 0.07812500


[batch = 6600] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6700] train_perplexity =     3.38, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6800] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6900] train_perplexity =     3.51, train_loss =  1.25, learning_rate = 0.07812500


[epoch = 195] validation_perplexity =     3.55, validation_loss =  1.27

Saved model to 'e200/ptb_char_lstm.pt'...
[epoch = 195] min_validation_perplexity =     3.55, min_validation_loss =  1.27


[batch =  100] train_perplexity =     3.69, train_loss =  1.30, learning_rate = 0.07812500


[batch =  200] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch =  300] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.07812500


[batch =  400] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch =  500] train_perplexity =     3.32, train_loss =  1.20, learning_rate = 0.07812500


[batch =  600] train_perplexity =     3.29, train_loss =  1.19, learning_rate = 0.07812500


[batch =  700] train_perplexity =     3.31, train_loss =  1.20, learning_rate = 0.07812500


[batch =  800] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch =  900] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 1000] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 1100] train_perplexity =     3.57, train_loss =  1.27, learning_rate = 0.07812500


[batch = 1200] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 1300] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 1400] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1500] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1600] train_perplexity =     3.38, train_loss =  1.22, learning_rate = 0.07812500


[batch = 1700] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1800] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1900] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2000] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2100] train_perplexity =     3.40, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2200] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2300] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2400] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2500] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2600] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2700] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2800] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2900] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 3000] train_perplexity =     3.38, train_loss =  1.22, learning_rate = 0.07812500


[batch = 3100] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3200] train_perplexity =     3.58, train_loss =  1.28, learning_rate = 0.07812500


[batch = 3300] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3400] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.07812500


[batch = 3500] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3600] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3700] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3800] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3900] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4000] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4100] train_perplexity =     3.32, train_loss =  1.20, learning_rate = 0.07812500


[batch = 4200] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 4300] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 4400] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4500] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4600] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4700] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4800] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.07812500


[batch = 4900] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5000] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 5100] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5200] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5300] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5400] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5500] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5600] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5700] train_perplexity =     3.30, train_loss =  1.19, learning_rate = 0.07812500


[batch = 5800] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5900] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.07812500


[batch = 6000] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 6100] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6200] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6300] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6400] train_perplexity =     3.38, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6500] train_perplexity =     3.32, train_loss =  1.20, learning_rate = 0.07812500


[batch = 6600] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6700] train_perplexity =     3.38, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6800] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6900] train_perplexity =     3.51, train_loss =  1.25, learning_rate = 0.07812500


[epoch = 196] validation_perplexity =     3.55, validation_loss =  1.27

Saved model to 'e200/ptb_char_lstm.pt'...
[epoch = 196] min_validation_perplexity =     3.55, min_validation_loss =  1.27


[batch =  100] train_perplexity =     3.69, train_loss =  1.30, learning_rate = 0.07812500


[batch =  200] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch =  300] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.07812500


[batch =  400] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch =  500] train_perplexity =     3.32, train_loss =  1.20, learning_rate = 0.07812500


[batch =  600] train_perplexity =     3.29, train_loss =  1.19, learning_rate = 0.07812500


[batch =  700] train_perplexity =     3.31, train_loss =  1.20, learning_rate = 0.07812500


[batch =  800] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch =  900] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 1000] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 1100] train_perplexity =     3.57, train_loss =  1.27, learning_rate = 0.07812500


[batch = 1200] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 1300] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 1400] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1500] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1600] train_perplexity =     3.38, train_loss =  1.22, learning_rate = 0.07812500


[batch = 1700] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1800] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1900] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2000] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2100] train_perplexity =     3.40, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2200] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2300] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2400] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2500] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2600] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2700] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2800] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2900] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 3000] train_perplexity =     3.38, train_loss =  1.22, learning_rate = 0.07812500


[batch = 3100] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3200] train_perplexity =     3.58, train_loss =  1.28, learning_rate = 0.07812500


[batch = 3300] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3400] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.07812500


[batch = 3500] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3600] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3700] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3800] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3900] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4000] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4100] train_perplexity =     3.32, train_loss =  1.20, learning_rate = 0.07812500


[batch = 4200] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 4300] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 4400] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4500] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4600] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4700] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4800] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.07812500


[batch = 4900] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5000] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 5100] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5200] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5300] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5400] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5500] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5600] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5700] train_perplexity =     3.30, train_loss =  1.19, learning_rate = 0.07812500


[batch = 5800] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5900] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.07812500


[batch = 6000] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 6100] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6200] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6300] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6400] train_perplexity =     3.38, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6500] train_perplexity =     3.32, train_loss =  1.20, learning_rate = 0.07812500


[batch = 6600] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6700] train_perplexity =     3.38, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6800] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6900] train_perplexity =     3.51, train_loss =  1.25, learning_rate = 0.07812500


[epoch = 197] validation_perplexity =     3.55, validation_loss =  1.27

Saved model to 'e200/ptb_char_lstm.pt'...
[epoch = 197] min_validation_perplexity =     3.55, min_validation_loss =  1.27


[batch =  100] train_perplexity =     3.69, train_loss =  1.30, learning_rate = 0.07812500


[batch =  200] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch =  300] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.07812500


[batch =  400] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch =  500] train_perplexity =     3.32, train_loss =  1.20, learning_rate = 0.07812500


[batch =  600] train_perplexity =     3.28, train_loss =  1.19, learning_rate = 0.07812500


[batch =  700] train_perplexity =     3.31, train_loss =  1.20, learning_rate = 0.07812500


[batch =  800] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch =  900] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 1000] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 1100] train_perplexity =     3.57, train_loss =  1.27, learning_rate = 0.07812500


[batch = 1200] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 1300] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 1400] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1500] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1600] train_perplexity =     3.38, train_loss =  1.22, learning_rate = 0.07812500


[batch = 1700] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1800] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1900] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2000] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2100] train_perplexity =     3.40, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2200] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2300] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2400] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2500] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2600] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2700] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2800] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2900] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 3000] train_perplexity =     3.38, train_loss =  1.22, learning_rate = 0.07812500


[batch = 3100] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3200] train_perplexity =     3.58, train_loss =  1.28, learning_rate = 0.07812500


[batch = 3300] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3400] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.07812500


[batch = 3500] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3600] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3700] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3800] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3900] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4000] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4100] train_perplexity =     3.32, train_loss =  1.20, learning_rate = 0.07812500


[batch = 4200] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 4300] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 4400] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4500] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4600] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4700] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4800] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.07812500


[batch = 4900] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5000] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 5100] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5200] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5300] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5400] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5500] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5600] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5700] train_perplexity =     3.30, train_loss =  1.19, learning_rate = 0.07812500


[batch = 5800] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5900] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.07812500


[batch = 6000] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 6100] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6200] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6300] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6400] train_perplexity =     3.38, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6500] train_perplexity =     3.32, train_loss =  1.20, learning_rate = 0.07812500


[batch = 6600] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6700] train_perplexity =     3.38, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6800] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6900] train_perplexity =     3.51, train_loss =  1.25, learning_rate = 0.07812500


[epoch = 198] validation_perplexity =     3.55, validation_loss =  1.27

Saved model to 'e200/ptb_char_lstm.pt'...
[epoch = 198] min_validation_perplexity =     3.55, min_validation_loss =  1.27


[batch =  100] train_perplexity =     3.69, train_loss =  1.30, learning_rate = 0.07812500


[batch =  200] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch =  300] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.07812500


[batch =  400] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch =  500] train_perplexity =     3.32, train_loss =  1.20, learning_rate = 0.07812500


[batch =  600] train_perplexity =     3.28, train_loss =  1.19, learning_rate = 0.07812500


[batch =  700] train_perplexity =     3.31, train_loss =  1.20, learning_rate = 0.07812500


[batch =  800] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch =  900] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 1000] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 1100] train_perplexity =     3.57, train_loss =  1.27, learning_rate = 0.07812500


[batch = 1200] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 1300] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 1400] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1500] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1600] train_perplexity =     3.38, train_loss =  1.22, learning_rate = 0.07812500


[batch = 1700] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1800] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1900] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2000] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2100] train_perplexity =     3.40, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2200] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2300] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2400] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2500] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2600] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2700] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2800] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2900] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 3000] train_perplexity =     3.38, train_loss =  1.22, learning_rate = 0.07812500


[batch = 3100] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3200] train_perplexity =     3.58, train_loss =  1.28, learning_rate = 0.07812500


[batch = 3300] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3400] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.07812500


[batch = 3500] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3600] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3700] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3800] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3900] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4000] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4100] train_perplexity =     3.32, train_loss =  1.20, learning_rate = 0.07812500


[batch = 4200] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 4300] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 4400] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4500] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4600] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4700] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4800] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.07812500


[batch = 4900] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5000] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 5100] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5200] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5300] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5400] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5500] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5600] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5700] train_perplexity =     3.30, train_loss =  1.19, learning_rate = 0.07812500


[batch = 5800] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5900] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.07812500


[batch = 6000] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 6100] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6200] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6300] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6400] train_perplexity =     3.38, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6500] train_perplexity =     3.32, train_loss =  1.20, learning_rate = 0.07812500


[batch = 6600] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6700] train_perplexity =     3.38, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6800] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6900] train_perplexity =     3.51, train_loss =  1.25, learning_rate = 0.07812500


[epoch = 199] validation_perplexity =     3.55, validation_loss =  1.27

Saved model to 'e200/ptb_char_lstm.pt'...
[epoch = 199] min_validation_perplexity =     3.55, min_validation_loss =  1.27


[batch =  100] train_perplexity =     3.69, train_loss =  1.30, learning_rate = 0.07812500


[batch =  200] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch =  300] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.07812500


[batch =  400] train_perplexity =     3.33, train_loss =  1.20, learning_rate = 0.07812500


[batch =  500] train_perplexity =     3.32, train_loss =  1.20, learning_rate = 0.07812500


[batch =  600] train_perplexity =     3.28, train_loss =  1.19, learning_rate = 0.07812500


[batch =  700] train_perplexity =     3.31, train_loss =  1.20, learning_rate = 0.07812500


[batch =  800] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch =  900] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 1000] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 1100] train_perplexity =     3.57, train_loss =  1.27, learning_rate = 0.07812500


[batch = 1200] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 1300] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 1400] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1500] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1600] train_perplexity =     3.38, train_loss =  1.22, learning_rate = 0.07812500


[batch = 1700] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1800] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 1900] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2000] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2100] train_perplexity =     3.40, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2200] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2300] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2400] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 2500] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2600] train_perplexity =     3.52, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2700] train_perplexity =     3.53, train_loss =  1.26, learning_rate = 0.07812500


[batch = 2800] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 2900] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 3000] train_perplexity =     3.38, train_loss =  1.22, learning_rate = 0.07812500


[batch = 3100] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3200] train_perplexity =     3.58, train_loss =  1.28, learning_rate = 0.07812500


[batch = 3300] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3400] train_perplexity =     3.44, train_loss =  1.23, learning_rate = 0.07812500


[batch = 3500] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3600] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 3700] train_perplexity =     3.46, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3800] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 3900] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4000] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4100] train_perplexity =     3.32, train_loss =  1.20, learning_rate = 0.07812500


[batch = 4200] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 4300] train_perplexity =     3.40, train_loss =  1.22, learning_rate = 0.07812500


[batch = 4400] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4500] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4600] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 4700] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 4800] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.07812500


[batch = 4900] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5000] train_perplexity =     3.45, train_loss =  1.24, learning_rate = 0.07812500


[batch = 5100] train_perplexity =     3.51, train_loss =  1.26, learning_rate = 0.07812500


[batch = 5200] train_perplexity =     3.50, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5300] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5400] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5500] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5600] train_perplexity =     3.49, train_loss =  1.25, learning_rate = 0.07812500


[batch = 5700] train_perplexity =     3.30, train_loss =  1.19, learning_rate = 0.07812500


[batch = 5800] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 5900] train_perplexity =     3.48, train_loss =  1.25, learning_rate = 0.07812500


[batch = 6000] train_perplexity =     3.44, train_loss =  1.24, learning_rate = 0.07812500


[batch = 6100] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6200] train_perplexity =     3.39, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6300] train_perplexity =     3.41, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6400] train_perplexity =     3.38, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6500] train_perplexity =     3.32, train_loss =  1.20, learning_rate = 0.07812500


[batch = 6600] train_perplexity =     3.42, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6700] train_perplexity =     3.38, train_loss =  1.22, learning_rate = 0.07812500


[batch = 6800] train_perplexity =     3.43, train_loss =  1.23, learning_rate = 0.07812500


[batch = 6900] train_perplexity =     3.51, train_loss =  1.25, learning_rate = 0.07812500


[epoch = 200] validation_perplexity =     3.55, validation_loss =  1.27

Saved model to 'e200/ptb_char_lstm.pt'...
[epoch = 200] min_validation_perplexity =     3.55, min_validation_loss =  1.27

Test validation:

test_perplexity =     3.44, test_loss =  1.23
