Loaded corpus from 'ptb_word_corpus.pkl'...
['<eos>', 'N', '<unk>', 'aer', 'banknote', 'berlitz', 'calloway', 'centrust', 'cluett', 'fromstein', 'gitano', 'guterman', 'hydro-quebec', 'ipo', 'kia', 'memotec', 'mlx', 'nahb', 'punts', 'rake', 'regatta', 'rubens', 'sim', 'snack-food', 'ssangyong', 'swapo', 'wachter', 'pierre', 'years', 'old', 'will', 'join', 'the', 'board', 'as', 'a', 'nonexecutive', 'director', 'nov.', 'mr.', 'is', 'chairman', 'of', 'n.v.', 'dutch', 'publishing', 'group', 'rudolph', 'and', 'former', 'consolidated', 'gold', 'fields', 'plc', 'was', 'named', 'this', 'british', 'industrial', 'conglomerate', 'form', 'asbestos', 'once', 'used', 'to', 'make', 'kent', 'cigarette', 'filters', 'has', 'caused', 'high', 'percentage', 'cancer', 'deaths', 'among', 'workers', 'exposed', 'it', 'more', 'than', 'ago', 'researchers', 'reported', 'fiber', 'unusually', 'enters', 'with', 'even', 'brief', 'exposures', 'causing', 'symptoms', 'that', 'show', 'up', 'decades', 'later', 'said', 'inc.', 'unit', 'new', 'york-based', 'corp.', 'makes', 'cigarettes', 'stopped', 'using', 'in', 'its', 'although', 'preliminary', 'findings', 'were', 'year', 'latest', 'results', 'appear', 'today', "'s", 'england', 'journal', 'medicine', 'forum', 'likely', 'bring', 'attention', 'problem', 'an', 'story', 'we', "'re", 'talking', 'about', 'before', 'anyone', 'heard', 'having', 'any', 'questionable', 'properties', 'there', 'no', 'our', 'products', 'now', 'neither', 'nor', 'who', 'studied', 'aware', 'research', 'on', 'smokers', 'have', 'useful', 'information', 'whether', 'users', 'are', 'at', 'risk', 'james', 'a.', 'boston', 'institute', 'dr.', 'led', 'team', 'from', 'national', 'medical', 'schools', 'harvard', 'university', 'spokeswoman', 'very', 'modest', 'amounts', 'making', 'paper', 'for', 'early', '1950s', 'replaced', 'different', 'type', 'billion', 'sold', 'company', 'men', 'worked', 'closely', 'substance', 'died', 'three', 'times', 'expected', 'number', 'four', 'five', 'surviving', 'diseases', 'including', 'recently', 'total', 'malignant', 'lung', 'far', 'higher', 'rate', 'striking', 'finding', 'those', 'us', 'study', 'west', 'mass.', 'factory', 'appears', 'be', 'highest', 'western', 'industrialized', 'countries', 'he', 'plant', 'which', 'owned', 'by', '&', 'co.', 'under', 'contract', 'probably', 'support', 'argue', 'u.s.', 'should', 'regulate', 'class', 'common', 'kind', 'found', 'most', 'other', 'buildings', 'one', 'few', 'nations', 'does', "n't", 'standard', 'regulation', 'smooth', 'fibers', 'such', 'classified', 'according', 't.', 'professor', 'vermont', 'college', 'easily', 'rejected', 'body', 'explained', 'july', 'environmental', 'protection', 'agency', 'imposed', 'gradual', 'ban', 'virtually', 'all', 'uses', 'almost', 'remaining', 'outlawed', 'made', 'areas', 'particularly', 'dusty', 'where', 'dumped', 'large', 'imported', 'material', 'into', 'huge', 'poured', 'cotton', 'mixed', 'dry', 'process', 'described', 'clouds', 'blue', 'dust', 'hung', 'over', 'parts', 'though', 'fans', 'area', 'question', 'some', 'managers', 'contracted', 'phillips', 'vice', 'president', 'human', 'resources', 'but', 'you', 'recognize', 'these', 'events', 'took', 'place', 'bearing', 'work', 'force', 'yields', 'money-market', 'mutual', 'funds', 'continued', 'slide', 'amid', 'signs', 'portfolio', 'expect', 'further', 'declines', 'interest', 'rates', 'average', 'seven-day', 'compound', 'yield', 'taxable', 'tracked', 'money', 'fund', 'report', 'eased', 'fraction', 'point', 'week', 'ended', 'tuesday', 'assume', 'reinvestment', 'dividends', 'current', 'continues', 'maturity', "'", 'investments', 'day', 'days', 'longest', 'since', 'august', 'donoghue', 'longer', 'maturities', 'thought', 'indicate', 'declining', 'because', 'they', 'permit', 'retain', 'relatively', 'period', 'shorter', 'considered', 'sign', 'rising', 'can', 'capture', 'sooner', 'open', 'only', 'institutions', 'stronger', 'indicator', 'watch', 'market', 'reached', 'nevertheless', 'editor', 'may', 'again', 'down', 'recent', 'rises', 'short-term', 'six-month', 'treasury', 'bills', 'monday', 'auction', 'example', 'rose', 'despite', 'investors', 'continue', 'pour', 'cash', 'assets', 'grew', '$', 'during', 'typically', 'money-fund', 'beat', 'comparable', 'vary', 'go', 'after', 'top', 'currently', 'yielding', 'well', 'dreyfus', 'world-wide', 'dollar', 'had', 'earlier', 'invests', 'heavily', 'dollar-denominated', 'securities', 'overseas', 'management', 'fees', 'boosts', 'simple', '30-day', 'fell', 'slid', 'j.p.', 'grace', 'holds', 'elected', 'succeeds', 'd.', 'formerly', 'resigned', 'energy', 'seven', 'seats', 'pacific', 'first', 'financial', 'shareholders', 'approved', 'acquisition', 'royal', 'ltd.', 'toronto', 'share', 'or', 'million', 'thrift', 'holding', 'expects', 'obtain', 'regulatory', 'approval', 'complete', 'transaction', 'year-end', 'international', 'completed', 'sale', 'controls', 'operations', 's.p', 'italian', 'state-owned', 'interests', 'mechanical', 'engineering', 'industry', 'based', 'ohio', 'computerized', 'systems', 'employs', 'people', 'annual', 'revenue', 'federal', 'government', 'suspended', 'sales', 'savings', 'bonds', 'congress', 'lifted', 'ceiling', 'debt', 'until', 'acts', 'authority', 'issue', 'obligations', 'borrowing', 'dropped', 'midnight', 'trillion', 'legislation', 'lift', 'fight', 'cutting', 'capital-gains', 'taxes', 'house', 'voted', 'raise', 'senate', 'act', 'next', 'earliest', 'default', 'if', 'then', 'clark', 'j.', 'senior', 'general', 'manager', 'marketing', 'arm', 'japanese', 'auto', 'maker', 'mazda', 'motor', 'corp', 'position', 'oversee', 'service', 'previously', 'chrysler', 'division', 'been', 'executive', 'when', 'time', 'their', 'nation', 'manufacturing', 'jet', 'off', 'resort', 'towns', 'like', 'hot', 'springs', 'not', 'association', 'manufacturers', 'settled', 'capital', 'indianapolis', 'fall', 'meeting', 'city', 'decided', 'treat', 'guests', 'royalty', 'rock', 'stars', 'owners', 'idea', 'course', 'prove', 'corporate', 'decision', 'makers', 'buckle', 'belt', 'so', 'good', 'expand', 'receiving', 'end', 'message', 'officials', 'giants', 'du', 'pont', 'along', 'lesser', 'steel', 'valley', 'queen', 'executives', 'joined', 'mayor', 'william', 'h.', 'iii', 'evening', 'guest', 'victor', 'champagne', 'followed', 'morning', 'police', 'wives', 'traffic', 'red', 'lights', 'governor', 'could', 'welcomed', 'special', 'buffet', 'breakfast', 'held', 'museum', 'food', 'drinks', 'banned', 'everyday', 'visitors', 'honor', 'out', 'drivers', 'crews', 'official', 'announcer', 'exhibition', 'race', 'fortune', 'cars', 'pointed', 'still', 'space', 'machines', 'another', 'sponsor', 'name', 'two', 'back', 'downtown', 'squeezed', 'meetings', 'hotel', 'buses', 'dinner', 'block', 'away', 'indiana', 'nine', 'hottest', 'chefs', 'town', 'fed', 'them', 'knowing', 'free', 'eat', 'gave', 'standing', 'say', 'treatment', 'return', 'future', 'looking', 'forward', 'winter', 'february', 'south', 'korea', 'registered', 'trade', 'deficit', 'october', 'reflecting', 'country', 'economic', 'figures', 'released', 'wednesday', 'ministry', 'showed', 'fifth', 'monthly', 'setback', 'casting', 'cloud', 'economy', 'exports', 'stood', 'mere', 'increase', 'while', 'imports', 'increased', 'sharply', 'last', 'boom', 'began', 'prolonged', 'labor', 'disputes', 'conflicts', 'sluggish', 'would', 'remain', 'target', 'gloomy', 'forecast', 'recorded', 'surplus', 'january', 'accumulated', 'same', 'newsweek', 'trying', 'keep', 'pace', 'rival', 'magazine', 'announced', 'advertising', 'introduce', 'incentive', 'plan', 'advertisers', 'ad', 'washington', 'post', 'second', 'offered', 'plans', 'give', 'discounts', 'maintaining', 'increasing', 'spending', 'become', 'permanent', 'news', 'underscore', 'fierce', 'competition', 'between', 'warner', 'b.', 'world', 'alan', 'full', 'page', 'cost', 'mid-october', 'lowered', 'guaranteed', 'circulation', 'base', 'lower', 'effectively', 'per', 'subscriber', 'costs', 'yet', 'announce', 'credit', 'credits', 'renewal', 'reward', 'bonuses', 'meet', 'exceed', 'long', 'spent', 'attempt', 'shore', 'decline', 'pages', 'months', 'totaled', 'drop', 'publishers', 'bureau', 'what', 'matters', 'paying', 'department', 'doing', 'fine', 'both', 'gaining', 'without', 'heavy', 'use', 'electronic', 'subscribers', 'telephones', 'watches', 'however', 'none', 'big', 'gains', 'audit', 'largest', 'decrease', 'six', 'flat', 'electric', 'system', 'bowed', 'bidding', 'public', 'hampshire', 'saying', 'risks', 'too', 'potential', 'justify', 'offer', 'move', 'leaves', 'united', 'illuminating', 'northeast', 'utilities', 'outside', 'bidders', 'ps', 'also', 'proposed', 'internal', 'reorganization', 'chapter', 'bankruptcy', 'proceedings', 'independent', 'acquire', 'below', 'value', 'places', 'bid', 'says', 'worth', 'haven', 'conn.', 'hartford', 'conn', 'n.h.', 'values', 'john', 'rowe', 'chief', 'officer', 'equity', 'suffer', 'forecasts', 'related', 'growth', 'electricity', 'demand', 'improved', 'operating', 'did', 'come', 'true', 'raising', 'seemed', 'substantial', 'persistent', 'rewards', 'way', 'got', 'hard', 'take', 'added', 'noted', 'political', 'concerns', 'worried', 'matter', 'owns', 'emerges', 'attracts', 'just', 'factors', 'withdraw', 'wilbur', 'ross', 'jr.', 'rothschild', 'adviser', 'troubled', 'holders', 'withdrawal', 'might', 'speed', 'fact', 'increases', 'against', 'around', 'complicated', 'negotiations', 'state', 'asserted', 'field', 'less', 'separately', 'commission', 'turned', 'request', 'seeking', 'possible', 'purchase', 'hopes', 'review', 'ferc', 'summer', 'court', 'shares', 'closed', 'yesterday', 'cents', 'york', 'stock', 'exchange', 'composite', 'trading', 'norman', 'toys', 'r', 'frederick', 'banking', 'directors', 'consumer', 'electronics', 'appliances', 'retailing', 'chain', 'succeed', 'daniel', 'm.', 'retired', 'circuit', 'robert', 'r.', 'undersecretary', 'commonwealth', 'edison', 'ordered', 'refund', 'illegal', 'collected', 'overruns', 'nuclear', 'power', 'illinois', 'commerce', 'groups', 'ever', 'required', 'local', 'utility', 'judge', 'richard', 'curry', 'refunds', 'each', 'customers', 'received', 'april', 'moved', 'begin', 'feb.', 'appeals', 'attempts', 'his', 'order', 'pool', 'through', 'round', 'already', 'appealing', 'underlying', 'considering', 'exact', 'amount', 'determined', 'actual', 'dec.', 'ruling', 'slash', 'earnings', 'spokesman', 'tracking', 'whose', 'addresses', 'changed', 'past', 'administrative', 'nightmare', 'near', 'ill.', 'disputed', 'pay', 'courts', 'upheld', 'challenge', 'supreme', 'construction', 'expenses', 'collecting', 'subject', 'ruled', 'plus', 'additional', 'calculations', 'month', 'set', 'faces', 'appellate', 'estimated', 'hope', 'precedent', 'case', 'involving', 'rule', 'poor', 'performance', 'i', 'japan', 'domestic', 'trucks', 'units', 'record', 'automobile', 'dealers', 'strong', 'year-to-year', 'september', 'setting', 'records', 'every', 'march', 'compared', 'previous', 'inched', 'passenger', 'medium-sized', 'benefited', 'price', 'reductions', 'arising', 'introduction', 'consumption', 'tax', 'doubled', 'texas', 'instruments', 'opened', 'manufacture', 'control', 'devices', 'located', 'miles', 'seoul', 'help', 'diversifying', 'produce', 'vehicles', 'household', 'survival', 'spinoff', 'cray', 'computer', 'fledgling', 'supercomputer', 'business', 'depend', 'creativity', 'designer', 'seymour', 'development', 'initial', 'machine', 'tied', 'directly', 'balance', 'sheet', 'documents', 'filed', 'pending', 'disclosed', 'financing', 'providing', 'firm', 'project', 'heads', 'scrapped', 'working', 'cray-3', 'least', 'fully', 'operational', 'prototype', 'moreover', 'orders', 'several', 'prospects', 'many', 'anticipated', 'attached', 'much', 'choice', 'gregory', 'interview', 'theory', 'him', 'want', 'include', 'anticipates', 'perhaps', 'beginning', 'called', 'scenario', 'filing', 'details', 'jump', 'close', 'analysts', 'link', 'note', 'presence', 'complicate', 'valuation', 'investor', 'gary', 'p.', 'minneapolis', 'concept', 'either', 'believe', 'do', 'besides', 'age', 'tricky', 'chip', 'technology', 'sec', 'describe', 'chips', 'being', 'fragile', 'minute', 'require', 'handling', 'equipment', 'addition', 'contain', 'processors', 'twice', 'face', 'intense', 'roll', 'direct', 'competitor', 'compete', 'hitachi', 'nec', 'fujitsu', 'ltd', 'believes', 'fewer', 'priced', 'presumably', 'range', 'terms', 'stockholders', 'receive', 'own', 'distribution', 'occur', 'weeks', 'instead', 'companies', 'leave', 'marketplace', 'decide', 'applied', 'nasdaq', 'calculate', 'book', 'primarily', 'drain', 'sheets', 'clearly', 'why', 'favored', 'able', 'profit', 'half', 'rather', 'posted', 'hand', 'existed', 'incurred', 'loss', 'comment', 'colorado', 'colo.', 'contractor', 'arrangement', 'regarded', 'father', 'paid', 'messrs.', 'includes', 'neil', 'joseph', 'malcolm', 'software', 'douglas', 'hardware', 'came', 'stevens', 'positions', 'donald', 'arthur', 'hatch', 'eastern', 'claiming', 'success', 'removed', 'taiwan', 'saudi', 'arabia', 'list', 'watching', 'allegedly', 'failing', 'patents', 'rights', 'china', 'thailand', 'india', 'brazil', 'mexico', 'so-called', 'priority', 'result', 'interim', 'representative', 'carla', 'hills', 'law', 'accelerated', 'investigations', 'stiff', 'sanctions', 'improve', 'intellectual', 'property', 'spring', 'mrs.', 'she', 'placed', 'degrees', 'scrutiny', 'genuine', 'progress', 'touchy', 'growing', 'citizens', 'negotiators', 'inadequate', 'hurting', 'themselves', 'discouraging', 'scientists', 'authors', 'high-technology', 'firms', 'investing', 'best', 'creating', 'task', 'enforcement', 'teams', 'officers', 'prosecutors', 'trained', 'pursue', 'movie', 'instituted', 'effective', 'procedures', 'aid', 'copyright', 'agreement', 'trademark', 'introducing', 'protect', 'foreign', 'producers', 'unauthorized', 'films', 'measure', 'taipei', 'small', 'showing', 'part', 'vowed', 'enact', 'compatible', 'standards', 'apply', 'literary', 'works', 'completely', 'hook', 'italy', 'canada', 'greece', 'spain', 'concern', 'deemed', 'pose', 'problems', 'american', 'patent', 'hoffman', 'lawyer', 'specializing', 'cases', 'threat', 'combined', 'recognition', 'protecting', 'prompted', 'improvements', 'tells', 'efforts', 'craft', 'concerned', 'disturbing', 'developments', 'turkey', 'continuing', 'slow', 'malaysia', 'elaborate', 'reports', 'complained', 'videocassette', 'pharmaceutical', 'requires', 'bad', 'enough', 'merit', 'investigation', 'provision', 'argentina', 'ask', 'creditor', 'banks', 'developing', 'declaration', 'minister', 'believed', 'action', 'latin', 'little', 'reach', 'reduction', 'external', 'met', 'assistant', 'secretary', 'david', 'mulford', 'negotiator', 'carlos', 'office', 'feels', 'significant', 'principal', 'solved', 'wants', 'centennial', 'wall', 'street', 'century', 'stand', 'milestones', 'history', 'computers', 'personal', 'computing', 'launched', 'apple', 'ii', 'commodore', 'pet', 'tandy', 'crude', 'television', 'sets', 'screens', 'data', 'major', 'advance', 'built', 'garage', 'stephen', 'steven', 'jobs', 'club', 'affordable', 'pcs', 'triggered', 'explosive', 'product', 'desktop', 'models', 'home', 'mainframe', 'unlike', 'types', 'store', 'memories', 'faster', 'memory', 'capacity', 'greater', 'counterparts', 'pioneer', 'pc', 'gates', 'paul', 'allen', 'developed', 'became', 'billionaire', 'ibm', 'adapted', 'versions', 'f.', 'seagate', 'disk', 'drives', 'dennis', 'dale', 'atlanta', 'engineers', 'allow', 'via', 'telephone', 'leader', 'entered', 'shipments', 'annually', 'australian', 'pharmaceuticals', 'affiliate', 'acquired', 'sells', 'label', 'voting', 'stake', 'together', 'convertible', 'preferred', 'holdings', 'gives', 'right', 'oil', 'production', 'australia', 'bass', 'raised', 'barrels', 'launch', 'scheduled', 'brought', 'esso', 'exxon', 'broken', 'hill', 'operate', 'joint', 'venture', 'started', 'output', 'gradually', 'reaches', 'reserves', 'start', 'producing', 'southern', 'optical', 'subsidiary', 'thomas', 'sloan', 'following', 'buy-out', 'shearson', 'lehman', 'hutton', 'itself', 'certain', 'businesses', 'program', 'white', 'bush', 'duty-free', 'produced', 'quantities', 'virgin', 'islands', 'response', 'petition', 'changes', 'preferences', 'denied', 'requested', 'covered', 'tariff', 'grant', 'status', 'categories', 'injury', 'producer', 'seller', 'assembled', 'philippines', 'main', 'beneficiaries', 'eligible', 'aide', 'magna', 'mcalpine', 'frank', 'stepping', 'turn', 'manufacturer', 'effort', 'reduce', 'overhead', 'curb', 'satisfactory', 'level', 'achieved', 'maintained', 'finance', 'ambitious', 'expansion', 'left', 'excess', 'load', 'automotive', 'downturn', 'steady', 'cut', 'quarterly', 'dividend', '52-week', 'canadian', 'dollars', 'us$', 'c$', 'founder', 'controlling', 'shareholder', 'seek', 'unsuccessfully', 'seat', 'parliament', 'resume', 'influential', 'role', 'running', 'throughout', 'organization', 'personally', 'restructuring', 'consulting', 'career', 'clients', 'lord', 'english', 'chemical', 'nearly', 'bought', 'mortgage', 'totaling', 'purchases', 'fannie', 'mae', 'o.', 'maxwell', 'conference', 'institutional', 'rest', 'went', 'france', 'hong', 'kong', 'snapped', 'similar', 'mortgage-backed', 'put', 'blackstone', 'investment', 'bank', 'jointly', 'goldman', 'sachs', 'daiwa', 'co', 'seven-year', 'offering', 'fixed-rate', 'floating-rate', 'linked', 'london', 'interbank', 'key', 'advantages', 'designed', 'eliminate', 'prepayment', 'forces', 'channel', 'payments', 'semiannual', 'reducing', 'burden', 'addressing', 'extremely', 'attractive', 'boosted', 'outstanding', 'rapid', 'purchasers', 'buying', 'issued', 'l.', 'expanding', 'members', 'ltv', 'agreed', 'extend', 'aerospace', 'exclusive', 'file', 'code', 'giving', 'creditors', 'lawsuits', 'debts', 'giant', 'montedison', 'indirect', 'tender', 'erbamont', 'incorporated', 'netherlands', 'advertised', 'editions', 'expire', 'november', 'currencies', 'drawing', 'hefty', 'marks', 'sixth', 'consecutive', 'reflects', 'intensity', 'intervention', 'june', 'currency', 'temporarily', 'surged', 'above', 'yen', 'announcement', 'follows', 'sharper', 'pick', 'craze', 'sweeping', 'rash', 'closed-end', 'publicly', 'traded', 'portfolios', 'invest', 'stocks', 'single', 'regulators', 'triple', 'charles', 'e.', 'simon', 'washington-based', 'turf', 'ranged', 'chile', 'portugal', 'philippine', 'capped', 'visit', 'aquino', 'head', 'kicked', 'here', 'province', 'anything', 'how', 'guinea', 'george', 'foot', 'managing', 'partner', 'associates', 'mass', 'explosion', 'mania', '1920s', 'narrowly', 'focused', 'wildly', 'popular', 'crash', 'traditional', 'issuing', 'fixed', 'surge', 'brings', 'soon', 'listed', 'account', 'billions', 'claims', 'available', 'runs', 'michael', 'porter', 'analyst', 'smith', 'barney', 'harris', 'upham', 'behind', 'individual', 'scrambled', 'find', 'brokers', 'easy', 'sell', 'stretching', 'wide', 'planners', 'often', 'urge', 'diversify', 'hold', 'emerging', 'markets', 'mature', 'get', 'taste', 'burned', 'gyrations', 'prices', 'tend', 'swing', 'broader', 'oct.', 'instance', 'plunged', 'clobbered', 'wild', 'frenzy', 'historically', 'fat', 'premiums', 'discount', 'late', 'premium', 'reason', 'climbed', 'partially', 'specialists', 'european', 'skyrocketed', 'startling', 'targeted', 'long-term', 'play', 'integration', 'invested', 'jumped', "'m", 'alarmed', 'see', 'rich', 'newly', 'reflect', 'increasingly', 'global', 'suggests', 'asia', 'europe', 'exposure', 'viewpoint', 'cast', 'nonetheless', 'plenty', 'levels', 'buy', 'depositary', 'receipts', 'adrs', 'represent', 'essentially', 'insist', 'jumping', 'advice', 'folks', 'party', 'better', 'ready', 'quickly', 'soviet', 'union', 'technical', 'talks', 'repayment', 'moscow', 'russian', 'owed', 'repaid', 'clear', 'soviets', 'happen', 'permitted', 'branch', 'affairs', 'unless', 'settle', 'lent', 'short-lived', 'democratic', 'communists', 'seized', 'johnson', 'amended', 'americans', 'monetary', 'u.s.s.r.', 'belongs', 'stressed', 'satisfying', 'legal', 'obstacles', 'hundreds', 'millions', 'war', 'reflection', 'manufactured', 'goods', 'failed', 'rise', 'meanwhile', 'purchasing', 'survey', 'indicated', 'index', 'reading', 'sector', 'generally', 'factories', 'booked', 'defense', 'contractors', 'fallen', 'separate', 'ran', 'significantly', 'private', 'building', 'activity', 'adjusted', 'remove', 'effects', 'usual', 'seasonal', 'patterns', 'inflation', 'kenneth', 'economist', 'society', 'cleveland', 'revive', 'residential', 'slack', 'asked', 'draw', 'blank', 'measures', 'slowly', 'reserve', 'economists', 'predicting', 'slip', 'recession', 'cite', 'lack', 'imbalances', 'provide', 'warning', 'signals', 'inventories', 'watched', 'clues', 'buildup', 'cutbacks', 'lead', 'front', 'soft', 'landing', 'elliott', 'donaldson', 'lufkin', 'jenrette', 'excessive', 'slowdown', 'leading', 'intended', 'climbing', 'durable', 'backlogs', 'unfilled', 'helped', 'strength', 'excluding', 'declined', 'accounts', 'bankers', 'predicted', 'eventually', 'boost', 'single-family', 'homes', 'adjusting', 'change', 'contrast', 'mcgraw-hill', 'dodge', 'contracts', 'awarded', 'counts', 'slowing', 'imminent', 'committee', 'materials', 'pitney', 'bowes', 'stamford', 'low', 'evidence', 'export', 'purchased', 'vendors', 'delivering', 'consider', 'pressures', 'suppliers', 'handle', 'delivery', 'provided', 'indicators', 'difference', 'reporting', 'improvement', 'particular', 'worsening', 'polled', 'import', 'acknowledging', 'trend', 'suspect', 'going', 'items', 'short', 'supply', 'numbered', 'dozen', 'included', 'milk', 'odd', 'thing', 'row', 'shortage', 'blamed', 'dairy', 'exceptionally', 'coupled', 'quotas', 'contributed', 'article', 'seasonally', 'judging', 'sheep', 'chase', 'baby', 'boomers', 'sides', 'lot', 'novel', 'entirely', 'especially', 'characters', 'drink', 'dogs', 'bugs', 'bunny', 'reruns', 'read', 'talk', 'worry', 'careers', 'reader', 'engaging', 'recognizing', 'contemporary', 'author', 'belong', 'school', 'writers', 'notion', 'unique', 'outsiders', 'carries', 'implicit', 'relations', 'think', 'plot', 'rooted', 'reality', 'funny', 'hero', 'snow', 'search', 'star', 'stanford', 'degree', 'girlfriend', 'sassy', 'mark', 'her', 'meets', 'christian', 'offers', 'god', 'phone', 'man', 'sweet', 'figure', 'wears', 'else', '40-year-old', 'norwegian', 'wood', 'seems', 'copies', 'published', 'tokyo', 'pack', 'charts', 'books', 'written', 'language', 'usually', 'carry', 'macmillan', 'baseball', 'version', 'call', 'game', 'describes', 'mirror', 'harmony', 'spirit', 'player', 'commitment', 'practice', 'image', 'important', 'polls', 'soul', 'male', 'symbol', 'played', 'ball', 'bat', 'balls', 'stadium', 'strike', 'zone', 'depending', 'size', 'ties', 'shame', 'defeat', 'players', 'must', 'strict', 'rules', 'conduct', 'lives', 'always', 'wear', 'road', 'amusing', 'fare', 'enormous', 'sums', 'plate', 'run', 'complaint', 'whom', 'sony', 'regret', 'parties', 'sometimes', 'vicious', 'mundane', 'aspects', 'employees', 'assigned', 'lunch', 'partners', 'style', 'overtime', 'solidarity', 'letting', 'young', 'employee', 'responsibility', 'frustrating', 'science', 'ultimately', 'lesson', 'fired', 'committed', 'social', 'crime', 'appointment', 'venerable', 'never', 'certainly', 'learned', 'something', 'ms.', 'deputy', 'editorial', 'features', 'corners', 'globe', 'becoming', 'tobacco', 'smoke', 'singapore', 'entering', 'restaurants', 'stores', 'sports', 'centers', 'clubs', 'exempt', 'smoking', 'bars', 'except', 'hours', 'bans', 'theaters', 'elevators', 'hospitals', 'fast-food', 'prime', 'kuala', 'lumpur', 'urged', 'visiting', 'restricts', 'designated', 'indicates', 'consumers', 'carried', 'backer', 'spielvogel', 'bates', 'colony', 'feel', 'pressured', 'surveyed', 'identified', 'one-third', 'great', 'deal', 'stress', 'cabinet', 'endorsed', 'proposal', 'build', 'center', 'central', 'plaza', 'balked', 'conditions', 'undertaking', 'necessary', 'arafat', 'olympic', 'asking', 'palestinian', 'liberation', 'membership', 'renewed', 'application', 'plo', 'organizations', 'win', 'health', 'tourism', 'beijing', 'chinese', 'aids', 'sex', 'daily', 'disease', 'hospital', 'tests', 'confirmed', 'family', 'tested', 'newspaper', 'chaotic', 'life', 'polish', 'charges', 'gas', 'line', 'compensate', 'coal', 'establishing', 'diplomatic', 'poland', 'loans', 'financially', 'strapped', 'warsaw', 'victory', 'environmentalists', 'hungary', 'terminated', 'multibillion-dollar', 'river', 'dam', 'czechoslovakia', 'ending', 'authorized', 'modify', 'damage', 'planned', 'ca', 'operated', 'solely', 'peak', 'periods', 'painting', 'stockholm', 'painted', 'oils', 'playwright', 'couples', 'statistics', 'debentures', 'due', 'bond', 'warrants', 'toronto-based', 'real', 'estate', 'warrant', 'holder', 'par', 'accrued', 'date', 'underwriters', 'dominion', 'actor', 'lane', 'charlie', 'steve', 'martin', 'laid', 'claim', 'writer', 'tramp', 'film', 'student', 'campus', 'shot', 'black-and-white', 'artist', 'streets', 'revived', 'sidewalk', 'stories', 'piece', 'contained', 'dialogue', 'homeless', 'person', 'really', 'silent', 'marc', 'friend', 'earns', 'living', 'playing', 'double', 'classical', 'music', 'prepared', 'exciting', 'score', 'thinking', 'feeling', 'precisely', 'words', 'takes', 'highly', 'view', 'lovely', 'black', 'bill', 'seem', 'benign', 'women', 'walk', 'purpose', 'hangs', 'greenwich', 'village', 'strip', 'avenue', 'crack', 'selling', 'four-year-old', 'box', 'routine', 'spends', 'night', 'returns', 'calls', 'competing', 'given', 'blind', 'girl', 'cure', 'charge', 'returning', 'murdered', 'mother', 'child', 'turns', 'blessing', 'sense', 'serious', 'mission', 'stakes', 'woman', 'children', 'shop', 'expensive', 'apartment', 'strongly', 'camera', 'final', 'existence', 'ends', 'sound', 'rough', 'beaten', 'voices', 'french', 'managed', 'weird', 'achievement', 'harsh', 'brilliant', 'picture', 'character', 'viewed', 'sympathetic', 'historical', 'significance', 'executed', 'germans', 'thousands', 'resistance', 'needed', 'enabled', 'cocoa', 'job', 'killed', 'client', 'husband', 'openly', 'presented', 'nice', 'deserve', 'recommend', 'confused', 'mexican', 'revolution', 'taken', 'endless', 'scenes', 'fighting', 'eating', 'drinking', 'mention', 'movies', 'bright', 'spot', 'peck', 'loose', 'energetic', 'portrayal', 'die', 'video', 'tip', 'seeing', 'look', 'boeing', 'discussing', 'regular', 'possibly', 'larger', 'discussions', 'stages', 'specific', 'seattle', 'industries', 'mitsubishi', 'fuji', 'sections', 'accounting', 'aircraft', 'press', 'speculated', 'contribution', 'goes', 'ahead', 'plane', 'hit', 'mid-1990s', 'negative', 'secondary', 'campaigns', 'event', 'irony', 'attack', 'commercial', 'getting', 'presidential', 'campaign', 'election', 'scattered', 'across', 'ads', 'fears', 'empty', 'issues', 'era', 'content', 'politics', 'art', 'stage', 'consultant', 'your', 'tv', 'needs', 'bold', 'entertaining', 'means', 'fights', 'commercials', 'tone', 'elections', 'jersey', 'virginia', 'screen', 'tight', 'dinkins', 'candidate', 'income', 'straight', 'voice', 'republican', 'giuliani', 'roger', 'master', 'gets', 'contributions', 'accurately', 'links', 'insurance', 'convicted', 'phony', 'wait', 'caught', 'nasty', 'corruption', 'simply', 'exist', 'consultants', 'finally', 'shows', 'distorted', 'photos', 'politicians', 'compare', 'candidates', 'banning', 'bullets', 'opposed', 'pro-choice', 'choose', 'telling', 'truth', 'everybody', 'nobody', 'classic', 'situation', 'accurate', 'fail', 'insists', 'voluntarily', 'admitted', 'oversight', 'secret', 'refused', 'matching', 'incomplete', 'errors', 'know', 'peter', 'powers', 'deceptive', 'side', 'argues', 'knows', 'reservations', 'gov.', 'wilder', 'gubernatorial', 'battle', 'marshall', 'coleman', 'abortion', 'agree', 'tour', 'de', 'flag', 'tradition', 'freedom', 'liberty', 'generations', 'jefferson', 'wanted', 'rape', 'incest', 'denies', 'dynamics', 'attorney', 'series', 'advertisements', 'created', 'bob', 'goodman', 'shake', 'attracted', 'featured', 'suggested', 'courtroom', 'introduced', 'victims', 'younger', 'lawyers', 'accused', 'tried', 'pass', 'legislative', 'technique', 'lawmakers', 'grown', 'tired', 'gotten', 'scientist', 'mary', 'devote', 'remainder', 'season', 'positive', 'truce', 'lasted', 'stations', 'carrying', 'featuring', 'himself', 'questions', 'attacks', 'rep.', 'florio', 'aired', 'photograph', 'jim', 'courter', 'remember', 'female', 'mean', 'pictures', 'focus', 'hazardous', 'waste', 'neighbors', 'suing', 'fraud', 'nose', 'grows', 'involved', 'cry', 'responded', 'photographs', 'lying', 'asks', 'barrel', 'land', 'heating', 'pollution', 'environment', 'devastating', 'credibility', 'route', 're-election', 'rout', 'democrat', 'gop', 'nelson', 'rockefeller', 'appeared', 'chicago', 'friday', 'quoted', 'aug.', 'southeast', 'region', 'toward', 'interviews', 'suggest', 'cooperation', 'asian', 'pursued', 'fits', 'starts', 'domination', 'fresh', 'policy', 'flow', 'motion', 'economies', 'tripled', 'commitments', 'steep', 'assistance', 'pumping', 'wages', 'forcing', 'sites', 'known', 'contributing', 'influence', 'subordinate', 'spur', 'military', 'fearful', 'falling', 'budget', 'constraints', 'encourages', 'burdens', 'ground', 'coming', 'decade', 'comes', 'swelling', 'evolution', 'concentrated', '1990s', 'spurred', 'plants', 'accommodate', 'decisions', 'lender', 'california', 'graduate', 'parallel', 'approach', 'sectors', 'specialist', 'east', 'wilson', 'princeton', 'indonesia', 'effect', 'pull', 'integrated', 'closer', 'ministers', 'discuss', 'regional', 'transportation', 'telecommunications', 'participants', 'zealand', 'speech', 'baker', 'intention', 'taking', 'shape', 'friends', 'crucial', 'designing', 'architecture', 'difficult', 'dominance', 'flows', 'remains', 'optimism', 'convey', 'caution', 'understanding', 'functions', 'attitude', 'net', 'gain', 'everyone', 'step', 'ease', 'security', 'uncertainty', 'leases', 'bases', 'troop', 'regard', 'desirable', 'try', 'lee', 'pennsylvania', 'rice', 'hardly', 'eyes', 'comprehensive', 'test', 'basic', 'skills', 'ninth', 'greenville', 'spotted', 'seen', 'cheating', 'notes', 'profession', 'germany', 'surrendered', 'passed', 'forms', 'word', 'matched', 'answers', 'section', 'protest', 'my', 'teacher', 'ok', 'me', 'nancy', 'yeargin', 'students', 'parents', 'confronted', 'examination', 'geography', 'classes', 'gone', 'display', 'prosecuted', 'unusual', 'carolina', 'breach', 'pleaded', 'guilty', 'alternative', 'jail', 'partly', 'won', 'inspired', 'teach', 'wake', 'anger', 'colleagues', 'defended', 'treated', 'stunned', 'nature', 'actions', 'light', 'dark', 'reform', 'teachers', 'testing', 'enhanced', 'temptation', 'statute', 'violated', 'enforce', 'provisions', 'laws', 'alleged', 'bolster', 'scores', 'bonus', 'education', 'depended', 'ability', 'incredible', 'pressure', 'walt', 'educators', 'wrongdoing', 'n.m.', 'concluded', 'outright', 'standardized', 'greatly', 'inflated', 'widespread', 'surfaced', 'states', 'suspects', 'adult', 'wrong', 'ones', 'statewide', 'numerous', 'practices', 'booklets', 'classroom', 'instruction', 'booming', 'scoring', 'learning', 'nothing', 'sophisticated', 'academic', 'telegraph', 'precise', 'concentrate', 'county', 'district', 'experts', 'promotions', 'entire', 'districts', 'extra', 'lab', 'grants', 'projects', 'reforms', 'posts', 'lowest', 'sat', 'primary', 'critics', 'stressing', 'worst', 'administrators', 'seriously', 'allegations', 'foundation', 'unfair', 'worries', 'abuse', 'keeping', 'track', 'preparation', 'textile', 'educated', 'governors', 'nobel', 'prize', 'winning', 'actress', '1980s', 'glory', 'faded', 'yellow', 'bricks', 'broad', 'violence', 'gangs', 'kids', 'linda', 'ward', 'awful', 'programs', 'seventh', 'immediate', 'predecessor', 'suffered', 'nervous', 'breakdown', 'prior', 'term', 'death', 'halls', 'trouble', 'serving', 'evenly', 'split', 'elite', 'neighborhoods', 'blacks', 'inner', 'resolved', 'clean', 'faculty', 'restore', 'safety', 'behalf', 'overall', 'educational', 'funding', 'ambitions', 'reformers', 'dismissal', 'loved', 'things', 'restructured', 'staff', 'struggled', 'fast', 'favorite', 'encouraged', 'taught', 'creation', 'encourage', 'teaching', 'advised', 'cultural', 'chosen', 'lady', 'freshman', 'studying', 'beth', "'ve", 'green', 'distinguished', 'herself', 'approaches', 'pair', 'prepare', 'furniture', "'d", 'stands', 'football', 'fellow', 'told', 'pushing', 'deteriorating', 'hearing', 'feared', 'helping', 'earn', 'points', 'source', 'meant', 'meaningful', 'salary', 'third', 'others', 'eager', 'pride', 'elizabeth', 'indeed', 'interested', 'attending', 'returned', 'adequately', 'studies', 'admits', 'mistake', 'correct', 'subjects', 'bottom', 'newspapers', 'mostly', 'prevent', 'badly', 'broke', 'heart', 'whole', 'alive', 'desperately', 'somebody', 'blow', 'advanced', 'pretty', 'joe', 'watson', 'prosecutor', 'concedes', 'adding', 'hands', 'discovered', 'avoid', 'disclosure', 'trial', 'quiet', 'resignation', 'save', 'certificate', 'recalls', 'someone', 'authorities', 'dismissed', 'crowded', 'testify', 'supportive', 'callers', 'radio', 'policies', 'host', 'allowed', 'first-time', 'offenders', 'conviction', 'worthy', 'murder', 'witnesses', 'interviewed', 'crushed', 'explain', 'familiar', 'shirts', 'angry', 'harm', 'damn', 'incident', 'doubt', 'wisdom', 'evaluating', 'realized', 'afraid', 'relieved', 'sentiment', 'favor', 'jury', 'touched', 'slate', 'answer', 'widely', 'roman', 'add', 'parent', 'britain', 'communication', 'parallels', 'mich.', 'gauge', 'eighth', 'grade', 'cat', 'iowa', 'metropolitan', 'arizona', 'florida', 'louisiana', 'maryland', 'tools', 'kean', 'publishes', 'aimed', 'improving', 'referred', 'michigan', 'similarity', 'devised', 'scale', 'measured', 'metric', 'measurement', 'volume', 'pie', 'bar', 'intent', 'knowledge', 'sounds', 'exclusion', 'contains', 'examples', 'matches', 'rick', 'ignoring', 'need', 'format', 'deny', 'publication', 'random', 'advisory', 'represented', 'sacramento', 'loan', 'c.', 's&l', 'offices', 'north', 'services', 'fetal-tissue', 'transplants', 'tissue', 'humans', 'anti-abortionists', 'oppose', 'abortions', 'mason', 'transplant', 'indefinitely', 'wo', 'stop', 'privately', 'funded', 'federally', 'involve', 'hhs', 'louis', 'sullivan', 'letter', 'acting', 'institutes', 'threatened', 'controversy', 'implications', 'brain', 'patient', 'suffering', 'scientific', 'ethical', 'majority', 'panel', 'recommended', 'carefully', 'controlled', 'anti-abortion', 'dispute', 'hampered', 'administration', 'recruit', 'prominent', 'doctors', 'fill', 'prestigious', 'helm', 'nih', 'withdrawn', 'names', 'consideration', 'views', 'antonio', 'novello', 'serve', 'surgeon', 'reportedly', 'assured', 'opposes', 'charged', 'imposing', 'ideological', 'moves', 'judgments', 'applications', 'associate', 'dean', 'yale', 'debate', 'mechanism', 'exists', 'middle', 'conducting', 'warns', 'discourage', 'climate', 'visible', 'foundations', 'fronts', 'damaged', 'genes', 'cause', 'syndrome', 'mental', 'summary', 'interstate', 'over-the-counter', 'slightly', 'lackluster', 'turnover', 'busiest', 'averaged', 'roughly', 'biggest', 'gained', 'modestly', 'tracks', 'connecticut', 'trust', 'regions', 'merge', 'massachusetts', 'merged', 'traders', 'real-estate', 'takeover', 'targets', 'speculators', 'anticipating', 'approve', 'permitting', 'immediately', 'water', 'saw', 'uptick', 'trader', 'otc', 'bancorp', 'expires', 'ed', 'fared', 'constitution', 'signed', 'merger', 'l.p.', 'london-based', 'third-quarter', 'pretax', 'mobile', 'dan', 'one-time', 'substantially', 'losses', 'associated', 'core', 'assuming', 'dramatic', 'achieve', 'weisfield', 'ratners', 'definitive', 'jaguar', 'heritage', 'media', 'acquiring', 'pop', 'swap', 'tumbled', 'rally', 'lost', 'restaurant', 'operator', 'redeemed', 'resolve', 'burt', 'sugarman', 'trotter', 'sci', 'slipped', 'ala.', 'fiscal', 'quarter', 'sept.', 'year-earlier', 'criminal', 'detailed', 'warnings', 'attorneys', 'cities', 'outcry', 'organized', 'protected', 'privilege', 'irs', 'stem', 'receives', 'customer', 'transactions', 'payment', 'document', 'failure', 'punishable', 'felony', 'prison', 'argued', 'wish', 'citing', 'rarely', 'acted', 'witness', 'neal', 'miami', 'dozens', 'letters', 'sent', 'certified', 'mail', 'drug', 'individuals', 'necessarily', 'hire', 'circumstances', 'retained', 'filling', 'spark', 'formed', 'gerald', 'delegates', 'resolution', 'requirement', 'aba', 'grand', 'prohibited', 'ethics', 'disclosing', 'committing', 'justice', 'taxpayers', 'notice', 'moderate', 'stance', 'christopher', 'detroit', 'submit', 'initiated', 'dating', 'permission', 'obtained', 'develop', 'sending', 'relating', 'confidential', 'judicial', 'salaries', 'victim', 'judges', 'lucrative', 'fanfare', 'ramirez', 'calif', 'san', 'francisco', 'refusal', 'couple', 'professional', 'accountants', 'eye', 'sudden', 'sum', 'litigation', 'detail', 'high-priced', 'troubles', 'laughing', 'trudeau', 'guild', 'america', 'alleging', 'mounted', 'punish', 'crossing', 'picket', 'involves', 'productions', 'member', 'employed', 'lawsuit', 'illegally', 'reviewing', 'suit', 'disciplinary', 'k.', 'consists', 'mainly', 'threats', 'punishment', 'damages', 'seeks', 'preventing', 'adopted', 'prohibits', 'title', 'x', 'assist', 'obtaining', 'counseling', 'activities', 'promote', 'advocate', 'opinion', 'restrictions', 'violate', 'care', 'limits', 'pregnant', 'inquiry', 'clears', 'bias', 'comments', 'homosexual', 'dallas', 'jack', 'sparked', 'remarks', 'december', 'sentencing', 'defendant', 'killing', 'park', 'referring', 'picking', 'boys', 'murray', 'appointed', 'fairness', 'commenting', 'judiciary', 'impose', 'gaf', 'opening', 'arguments', 'manhattan', 'lowe', 'indictment', 'wayne', 'n.j.', 'specialty', 'sherwin', 'attempting', 'manipulate', 'carbide', 'trials', 'switching', 'prosecution', 'affair', 'mayer', 'brown', 'oliver', 'd.c.', 'specialize', 'white-collar', 'served', 'narcotics', 'tire', 'rubber', 'principle', 'albany', 'ga.', 'inc', 'consist', 'square', 'feet', 'acres', 'bids', 'leaders', 'statement', 'pricing', 'equivalent', 'penny', 'municipal', 'no.', 'alone', 'competitors', 'auctions', 'summoned', 'sure', 'understood', 'fair', 'investigating', 'violations', 'packed', 'headquarters', 'contrary', 'sorry', 'embarrassing', 'situations', 'reputation', 'profits', 'benefits', 'accepted', 'normal', 'accepting', 'gifts', 'businessmen', 'fire', 'fueled', 'behavior', 'complaints', 'touch', 'foreigners', 'complain', 'limited', 'access', 'procurement', 'unfairly', 'undercut', 'slashing', 'semiconductors', 'hurt', 'minus', 'factor', 'hiroshima', 'design', 'waterworks', 'library', 'municipalities', 'emerge', 'insisted', 'contacted', 'federation', 'papers', 'atlantic', 'publications', 'community', 'cbs', 'communications', 'retaining', 'operates', 'retail', 'furukawa', 'machinery', 'locally', 'german', 'structural', 'productivity', 'thrifts', 'compiled', 'monitor', 'b', 'minimum', 'lsi', 'logic', 'surprise', 'semiconductor', 'lagging', 'billings', 'cover', 'extended', 'weakness', 'switch', 'techniques', 'wilfred', 'midyear', 'phase', 'oldest', 'appropriate', 'counting', 'extraordinary', 'older', 'silicon', 'converting', 'santa', 'clara', 'calif.', 'facility', 'speculate', 'additions', 'aggressively', 'suddenly', 'poorly', 'positioned', 'stark', 'robertson', 'dead', 'facing', 'traditionally', 'buyers', 'jitters', 'lake', 'n.j', 'g.', 'buoyed', 'cautiously', 'bearish', 'kept', 'plunging', 'locked', 'narrow', 'offset', 'resulting', 'jay', 'insight', 'reasons', 'lure', 'driving', 'tom', 'banque', 'paribas', 'convinced', 'erode', 'sterling', 'thursday', 'madison', 'los', 'angeles', 'solid', 'wave', 'waiting', 'wings', 'contends', 'perception', 'columbia', 'entertainment', 'cites', 'knight', 'undisclosed', 'georgia-pacific', 'northern', 'nekoosa', '10-year', 'valued', 'speculation', 'unclear', 'rolled', 'prospect', 'lock', 'high-yield', 'merely', 'drifted', 'release', 'expectations', 'minimal', 'commodity', 'ounce', 'ounces', 'cosby', 'ratings', 'nbc', 'debut', 'keeps', 'viewers', 'network', 'affiliates', 'air', 'considerably', 'episodes', 'distributor', 'viacom', 'losing', 'persuade', 'tactics', 'willing', 'negotiate', 'midwest', 'tell', "'ll", 'flooded', 'networks', 'station', 'a.c.', 'nielsen', 'r.i.', 'n.c.', 'louisville', 'ky.', 'renew', 'dick', 'disappointment', 'pleased', 'adds', 'disappointed', 'uncomfortable', 'frankly', 'quite', 'georgia', 'officially', 'atlanta-based', 'conventional', 'life-insurance', 'magazines', 'articles', 'survive', 'downright', 'garbage', 'billed', 'practical', 'founded', 'brooklyn', 'n.y.', 'entrepreneur', 'promise', 'crisis', 'combines', 'pieces', 'topics', 'happens', 'flush', 'toilet', 'editors', 'considerable', 'supermarket', 'identify', 'guys', 'feature', 'standpoint', 'would-be', 'campbell', 'soup', 'microwave', 'premiere', 'column', 'pointing', 'packaging', 'foam', 'plastic', 'landfill', 'monster', 'wrote', 'practicing', 'opportunity', 'modifications', 'printed', 'portrayed', 'surprisingly', 'turning', 'mike', 'ddb', 'needham', 'economics', 'big-time', 'handful', 'coors', 'spend', 'relied', 'haul', 'print', 'publisher', 'initially', 'subscription', 'revenues', 'yearly', 'recycled', 'scared', 'replies', 'sleep', 'interpublic', 'programming', 'expanded', 'original', 'largely', 'supplier', 'philadelphia', 'palmer', 'handled', 'baltimore', 'at&t', 'fax', 'ogilvy', 'mather', 'wpp', 'rubicam', 'enterprise', 'breaks', 'st.', 'specializes', 'pitches', 'pittsburgh', 'sea', 'containers', 'buy-back', 'pressed', 'temple', 'hamilton', 'shipping', 'proceeds', 'apiece', 'hostile', 'ag', 'sweetened', 'approximately', 'allocated', 'leaving', 'flexibility', 'raises', 'criticized', 'conditional', 'device', 'superior', 'asset', 'converted', 'possibility', 'subsidiaries', 'mired', 'barred', 'reaction', 'responding', 'advocates', 'steps', 'vans', 'requirements', 'automobiles', 'requiring', 'minivans', 'belts', 'rear', 'model', 'samuel', 'skinner', 'represents', 'ongoing', 'vehicle', 'extension', 'car', 'equipped', 'surprising', 'urging', 'actually', 'cargo', 'therefore', 'luck', 'reagan', 'fairly', 'address', 'chuck', 'highway', 'sen.', 'praised', 'noting', 'crashes', 'bags', 'automatic', 'weighing', 'pounds', 'withstand', 'weight', 'depressed', 'inches', 'shoulder', 'installed', 'deadline', 'engineer', 'ford', 'crew', 'rail', 'autos', 'heights', 'ill', 'railroad', 'sir', 'walters', 'petroleum', 'joins', 'cement', 'circle', 'formal', 'longstanding', 'opposition', 'within', 'dow', 'jones', 'options', 'conversations', 'interesting', 'prevailing', 'passage', 'slump', 'lay', 'altogether', 'employment', 'exclusively', 'leasing', 'reduced', 'payable', 'jan.', 'hair', 'accessories', 'cosmetic', 'year-ago', 'anti-takeover', 'henderson', 'u.k.', 'metals', 'succeeding', 'ian', 'butler', 'retiring', 'cent', 'redemption', 'upon', 'warren', 'leap', 'wine', 'tag', 'cabernet', 'shops', 'experience', 'declared', 'diamond', 'creek', 'weighed', 'bottle', 'fastest', 'segments', 'category', 'wines', 'quality', 'perceived', 'stable', 'la', 'magnitude', 'flashy', 'releases', 'lion', 'bottles', 'smallest', 'lighter', 'vintage', 'prestige', 'barrier', 'salon', 'soared', 'steal', 'precious', 'anywhere', 'angelo', 'command', 'larry', 'shapiro', 'happening', 'scarce', 'exhausted', 'newer', 'bargain', 'perfectly', 'yielded', 'owner', 'al', 'originally', 'retailer', 'wholesale', 'merchants', 'check', 'responses', 'yes', 'shipped', 'spread', 'thin', 'retailers', 'suburban', 'opinions', 'thinks', 'dramatically', 'wins', 'talked', 'excited', 'collection', 'sort', 'dunn', 'appeal', 'explains', 'rapidly', 'knowledgeable', 'orleans', 'equally', 'movement', 'forced', 'push', 'lowering', 'beautiful', 'ideas', 'walking', 'door', 'moment', 'looming', 'holidays', 'free-lance', 'upward', 'richmond', 'posting', 'fourth', 'agrees', 'kansas', 'stated', 'objective', 'hopefully', 'downward', 'expecting', 'easier', 'plunge', 'shown', 'weak', 'slower', 'discretionary', 'halt', 'divided', 'presidents', 'excesses', 'tilt', 'chance', 'mailing', 'exercised', 'hotels', 'exercise', 'merchant', 'steelmaker', 'steelworkers', 'tube', 'expired', 'vote', 'reject', 'pact', 'postponed', 'congressional', 'fails', 'disruption', 'schedule', 'taxpayer', 'nicholas', 'brady', 'redeem', 'maturing', 'refunding', 'three-year', 'auctioned', '30-year', 'when-issued', 'approves', 'clearing', 'two-year', 'five-year', 'estimates', 'decides', 'aim', 'lancaster', 'foods', 'marketer', 'frozen', 'pasta', 'advantage', 'fractionally', 'finished', 'advancing', 'enthusiasm', 'rushed', 'sight', 'pattern', 'benchmark', 'afternoon', 'mitsui', 'mining', 'eaton', 'sierra', 'supplies', 'harold', 'simmons', 'nl', 'gulf', 'chemicals', 'restructure', 'rebuffed', 'alternatives', 'proposals', 'reviewed', 'regarding', 'eliminated', 'various', 'proof', 'aggressive', 'pit', 'bull', 'morgan', 'stanley', 'valhi', 'two-thirds', 'surprised', 'cited', 'becomes', 'unchanged', 'proposing', 'subordinated', 'junk-bond', 'collapsed', 'likelihood', 'hundred', 'agreeing', 'consent', 'jerry', 'respond', 'solicitation', 'replacing', 'friendly', 'apparently', 'nominal', 'earned', 'licensing', 'representing', 'challenging', 'punitive', 'license', 'elsewhere', 'albert', 'researcher', '1960s', 'combat', 'licensed', 'criticism', 'acquisitions', 'color', 'disagree', 'removal', 'barriers', 'focusing', 'differences', 'initiative', 'rhetoric', 'signal', 'impending', 'nervousness', 'devoted', 'half-hour', 'briefing', 'journalists', 'notably', 'coca-cola', 'fuel', 'fires', 'publicized', 'boone', 'pickens', 'jr', 'lloyd', 'highlight', 'disproportionate', 'table', 'structures', 'recommendations', 'anxious', 'successful', 'concessions', 'clarify', 'exactly', 'quick', 'crossed', 'smaller', 'controversial', 'improves', 'provides', 'candela', 'laser', 'high-tech', 'sights', 'tiny', 'joint-venture', 'bureaucratic', 'secured', 'kidney', 'stones', 'skin', 'count', 'promising', 'frequently', 'putting', 'financed', 'corporations', 'minority', 'heightened', 'acceleration', 'deals', 'feed', 'glass', 'catch', 'fancy', 'strategic', 'fit', 'va.', 'architectural', 'ronald', 'window', 'merchandise', 'inside', 'counterpart', 'commodities', 'ships', 'u.s.a.', 'houses', 'ventures', 'goal', 'objectives', 'drive', 'basis', 'performed', 'businessman', 'alliance', 'generate', 'extent', 'ai', 'hudson', 'departure', 'maintenance', 'fueling', 'airlines', 'airports', 'omitted', 'duties', 'assumed', 'neighborhood', 'convenient', 'crown', 'package', 'checking', 'safe', 'deposit', 'card', 'installment', 'qualify', 'loyalty', 'competitive', 'deposits', 'packages', 'targeting', 'population', 'anne', 'moore', 'ncnb', 'charlotte', 'connections', 'adults', 'starting', 'throws', 'saving', 'memphis', 'tenn.', 'edge', 'crowd', 'borrowed', 'aiming', 'elderly', 'segment', 'stepped', 'macdonald', 'barnett', 'fla.', 'seniors', 'styles', 'create', 'branches', 'athletic', 'travel', 'active', 'games', 'panama', 'wells', 'fargo', 'travelers', 'checks', 'fee', 'slew', 'promoting', 'margins', 'profitable', 'mid-1970s', 'emphasis', 'switched', 'ways', 'enabling', 'analyze', 'deregulation', '1970s', 'cds', 'certificates', 'staggering', 'bigger', 'market-share', 'banker', 'worrying', 'unions', 'scrambling', 'define', 'n.c', 'attracting', 'costly', 'rewarding', 'bulk', 'audience', 'loyal', 'borrowers', 'personnel', 'training', 'promotional', 'jacob', 'demanding', 'tailored', 'moving', 'direction', 'alvin', 'realize', 'iras', 'registration', 'chandler', 'ariz.', 'exercisable', 'develops', 'low-cost', 'peripheral', 'chevrolet', 'vacation', 'send', 'lasting', 'savings-and-loan', 'foster', 'ailing', 'demise', 'healthy', 'performing', 'institution', 'gift', 'doors', 'felt', 'lewis', 'builds', 'confidence', 'critical', 'delay', 'worse', 'sagging', 'morale', 'mercantile', 'breaker', 'stemming', 'phelan', 'subcommittee', 'halts', 'collar', 'arguing', 'limit', 'merc', 'one-hour', 'stock-index', 'futures', 's&p', 'p.m', 'execute', 'trades', 'tumultuous', 'industrials', 'skidded', 'hitting', 'remained', 'session', 'existing', 'equal', 'leo', 'panic', 'reopened', 'subsequent', 'flood', 'knocked', 'intermediate', 'floor', 'shortly', 'maximum', 'one-day', 'minutes', 'aides', 'congressmen', 'executing', 'expressed', 'volatility', 'program-trading', 'comfortable', 'capitol', 'legislators', 'shut', 'volatile', 'breeden', 'breakers', 'trigger', 'vague', 'angered', 'edward', 'markey', 'beyond', 'done', 'sensitive', 'happy', 'writing', 'discussed', 'congressman', 'attended', 'dingell', 'let', 'sources', 'regulated', 'agriculture', 'committees', 'rural', 'ancient', 'stone', 'church', 'bells', 'tower', 'calling', 'angels', 'ropes', 'sounded', 'modern', 'enjoying', 'cool', 'autumn', 'band', 'ringers', 'herald', 'sunday', 'ringer', 'hammond', 'live', 'youngsters', 'ring', 'worker', 'train', 'youth', 'ranks', 'nationwide', 'continental', 'invented', 'physical', 'weigh', 'ton', 'concentration', 'proper', 'bell', 'methods', 'ten', 'pulling', 'disappears', 'hole', 'ringing', 'chamber', 'speaks', 'totally', 'absorbed', 'vision', 'thus', 'pulls', 'wheels', 'skilled', 'well-known', 'novelist', 'passion', 'finds', 'satisfaction', 'filled', 'bit', 'stays', 'stuck', 'sweat', 'sit', 'steadily', 'pressing', 'attend', 'rev.', 'vicar', 'promptly', 'premises', 'eight', 'attendance', 'refuses', 'reopen', 'wound', 'nearby', 'fault', 'crunch', 'bang', 'obvious', 'exit', 'feelings', 'drawn', 'council', 'aims', 'baldwin', 'speak', 'colleges', 'publish', 'entitled', 'attacking', 'inner-city', 'attract', 'lucky', 'everywhere', 'sole', 'weekly', 'signing', 'balanced', 'frequency', 'tea', 'observed', 'dressed', 'unsettled', 'comfort', 'predictable', 'payouts', 'warn', 'strongest', 'stock-market', 'trap', 'bernstein', 'merrill', 'lynch', 'widow', 'reference', 'males', 'kill', 'boesel', 'invariably', 'environments', 'risen', 'cyclical', '500-stock', 'sliding', 'watchers', 'double-digit', 'trends', 'advances', 'relative', 'sustained', 'rebound', 'exception', 'w.', 'hint', 'escaped', 'debacle', 'sharp', 'troublesome', 'manner', 'cigna', 'insurer', 'einhorn', 'weakening', 'painewebber', 'forecasting', 'argument', 'declare', 'weaken', 'bulls', 'expenditures', 'tally', 'basically', 'trailed', 'deliver', 'appreciation', 'vs.', 'element', 'outlook', 'upside', 'page-one', 'plight', 'alexander', 'graham', 'supposedly', 'replace', 'hubbard', 'wealthy', 'sued', 'infringed', 'established', 'enter', 'beverly', 'nbi', 'quarters', 'kingdom', 'commit', 'lacked', 'hired', 'nixon', 'historic', 'spoke', 'length', 'nowhere', 'easing', 'relationship', 'bloody', 'pro-democracy', 'demonstrators', 'massacre', 'normally', 'peaceful', 'phrase', 'socialist', 'capitalist', 'tension', 'evident', 'friendship', 'reminded', 'assault', 'crackdown', 'ordering', 'respect', 'fashion', 'killings', 'demonstrations', 'deng', 'speaking', 'deeply', 'turmoil', 'occurred', 'exchanges', 'deteriorated', 'greatest', 'dissident', 'wife', 'refuge', 'embassy', 'suspension', 'traveling', 'citizen', 'scowcroft', 'saturday', 'participation', 'pulled', 'acknowledge', 'welcome', 'contacts', 'borders', 'sdi', 'weapon', 'shoot', 'minor', 'premier', 'hoped', 'guards', 'guns', 'arrived', 'blocks', 'ambassador', 'residence', 'discarded', 'arms', 'contingent', 'soldiers', 'protests', 'diplomats', 'loaded', 'graduates', 'tree', 'passing', 'buck', 'everything', 'volunteer', 'unfortunately', 'compensation', 'impression', 'harder', 'visited', 'images', 'changing', 'perceptions', 'accept', 'barbara', 'continually', 'liberals', 'klein', 'puts', 'professionals', 'favorable', 'voters', 'spouse', 'assumption', 'perspective', 'defined', 'birmingham', 'disciplined', 'expelled', 'nasd', 'fined', 'fernando', 'marina', 'del', 'rey', 'listing', 'henry', 'i.', 'improper', 'unpublished', 'settlement', 'admitting', 'denying', 'consented', 'connection', 'crane', 'mount', 'glenn', 'escrow', 'inaccurate', 'principals', 'dell', 'eugene', 'island', 'wash.', 'implication', 'inappropriate', 'timing', 'del.', 'n.', 'differ', 'meaning', 'express', 'timely', 'requests', 'davis', 'jeffrey', 'gerard', 'stewart', 'amounted', 'parking', 'ticket', 'complaining', 'sizable', 'anybody', 'rolling', 'stuart', 'breaking', 'agents', 'compliance', 'stupid', 'mistakes', 'actively', 'engaged', 'blocked', 'computer-driven', 'plays', 'hurdles', 'resist', 'furor', 'abroad', 'arbitrage', 'exploit', 'imagine', 'racing', 'cope', 'utsumi', 'osaka', 'wary', 'forgotten', 'drove', 'operators', 'suspend', 'maybe', 'tightened', 'salomon', 'chunk', 's.', 'futures-related', 'strategies', 'liquid', 'impact', 'index-arbitrage', 'serves', 'counter', 'tapes', 'begins', 'pegged', 'privacy', 'brokerage', 'assurance', 'manages', '#', 'times-stock', '100-share', 'option', 'ft-se', 'tenfold', 'compares', 'wage', 'compromise', 'backed', 'proponents', 'hour', 'opponents', 'reflected', 'republicans', 'lifting', 'four-year', 'enacted', 'pat', 'williams', 'acceptance', 'sought', 'adopting', 'nickel', 'lately', 'touted', 'democrats', 'fought', 'insistence', 'employers', 'restriction', 'covers', 'impossible', 'elimination', 'zenith', 'navy', 'rockwell', 'replacement', 'navigation', 'intelligence', 'hahn', 'mode', 'teddy', 'unsolicited', 'cold', 'faced', 'negotiated', 'negotiating', 'surrender', 'tough', 'completion', 'opens', 'implies', 'dilemma', 'observers', 'pulp', 'consensus', 'insiders', 'onto', 'knew', 'picked', 'careful', 'universities', 'willingness', 'arrest', 'protesters', 'impressed', 'joining', 'administrator', 'transition', 'swiftly', 'raw', 'son', 'beta', 'kentucky', 'physics', 'photographic', 'engineered', 'turnaround', 'inherited', 'repair', 'remodeling', 'new-home', 'cycles', 'formula', 'reins', 'attributes', 'philosophy', 'concentrating', 'diversified', 'impressive', 'diversification', 'match', 'high-quality', 'forest-products', 'waters', 'house-senate', 'portions', 'relies', 'guarantees', 'initiatives', 'contrasts', 'provoked', 'veto', 'confined', 'shaping', 'appropriations', 'decisive', 'pentagon', 'planning', 'portion', 'warming', 'african', 'elephant', 'draws', 'variety', 'amendments', 'exemption', 'intriguing', 'struggle', 'sugar', 'fate', 'quota', 'stripped', 'noriega', 'regime', 'tons', 'growers', 'pound', 'whip', 'gray', 'pa', 'caribbean', 'jamaica', 'conservative', 'basin', 'powerful', 'hawaii', 'inouye', 'allies', 'instructed', 'lobbyist', 'abandon', 'drafted', 'supplemental', 'anti-drug', 'airline', 'flights', 'leadership', 'bounce', 'departments', 'beauty', 'bridges', 'repaired', 'disagreement', 'lose', 'planner', 'prefer', 'install', 'f', 'concrete', 'ind.', 'bridge', 'teaches', 'structure', 'charter', 'oak', 'peninsula', 'upset', 'similarly', 'carrier', 'competes', 'beverage', 'outlets', 'resembles', 'beer', 'hang', 'inventor', 'marvin', 'carriers', 'acknowledges', 'driver', 'perestroika', 'designers', 'promises', 'chairs', 'citicorp', 'sees', 'learn', 'seed', 'solution', 'root', 'birds', 'architects', 'propose', 'grain', 'preserving', 'solutions', 'cell', 'walls', 'cells', 'visits', 'architect', 'egypt', 'houston', 'vacant', 'altered', 'efficient', 'usx', 'fines', 'penalty', 'workplace', 'employer', 'pa.', 'mill', 'penalties', 'osha', 'electrical', 'covering', 'coke', 'dole', 'hazards', 'resulted', 'severe', 'deficiencies', 'failures', 'properly', 'injuries', 'spite', 'discrepancies', 'evaluation', 'corrected', 'promised', 'proved', 'unwilling', 'manpower', 'removing', 'contest', 'brands', 'cincinnati', 'error', 'edition', 'mistakenly', 'natural', 'implied', 'homelessness', 'numbers', 'causes', 'complex', 'emphasized', 'illness', 'examined', 'disorders', 'psychiatric', 'cardiovascular', 'shelter', 'consequence', 'composed', 'lacks', 'adequate', 'johns', 'hopkins', 'prof.', 'wright', 'array', 'poverty', 'combination', 'alcohol', 'simultaneously', 'housing', 'welfare', 'connected', 'persons', 'quote', 'fear', 'robbed', 'unable', 'fend', 'yourself', 'obviously', 'cracks', 'grim', 'escape', 'drugs', 'n.y', 'dismiss', 'sleeping', 'bothered', 'namely', 'consequences', 'cambridge', 'sponsors', 'chose', 'builders', 'allied', 'crusade', 'greed', 'motive', 'desire', 'subscribe', 'catholic', 'charities', 'usa', 'nonprofit', 'participated', 'deprived', 'families', 'predict', 'substitute', 'chivas', 'ruth', 'rogers', 'placement', 'perpetual', 'conversion', 'cable', 'coupon', 'leveraged', 'buy-outs', 'supporters', 'override', 'ark', 'diminished', 'benefit', 'amendment', 'ore.', 'criteria', 'occasionally', 'dubbed', 'lorenzo', 'text', 'broadcasting', 'tune', 'extensive', 'reporter', 'scholar', 'ordinary', 'democracy', 'censorship', 'propaganda', 'neat', 'tours', 'facilities', 'sorts', 'enables', 'hear', 'mind', 'absurd', 'inform', 'columns', 'facts', 'scholars', 'reporters', 'copying', 'copy', 'des', 'happened', 'usia', 'mccormick', 'memo', 'intend', 'preclude', 'plaintiffs', 'domestically', 'mentioned', 'notwithstanding', 'statutory', 'credentials', 'appearing', 'examine', 'determine', 'helpful', 'disagreed', "o'brien", 'duty', 'assure', 'scripts', 'xerox', 'public-relations', 'sends', 'stuff', 'conclude', 'shall', 'thanks', 'ought', 'answered', 'gartner', 'tribune', 'gordon', 'mcgovern', 'dorrance', 'herbert', 'baum', 'edwin', 'harper', 'responsibilities', 'successor', 'searching', 'reacted', 'prudential-bache', 'distant', 'operation', 'farm', 'veteran', 'dominated', 'pushed', 'profitability', 'closing', 'eliminating', 'disappointing', 'ceo', 'succession', 'retirement', 'agenda', 'repeatedly', 'stay', 'pension', 'convince', 'tremendous', 'duo', 'naming', 'sitting', 'guide', 'overnight', 'fulton', 'prebon', 'u.s.a', 'depository', 'collateral', 'motors', 'high-grade', 'unsecured', 'multiples', 'negotiable', 'c.d.s', 'typical', 'acceptances', 'bank-backed', 'eurodollars', 'libor', 'quotations', 'switzerland', 'indications', 'lending', 'location', 'freddie', 'mac', 'mortgages', 'one-year', 'adjustable', 'telerate', 'annualized', 'newhouse', 'abrupt', 'departures', 'empire', 'gut', 'unhappy', 'tenure', 'enjoyed', 'spectacular', 'smoothly', 'succeeded', 'bennett', 'susan', 'anthony', 'hutchinson', 'powerhouse', 'alfred', 'yorker', 'evans', 'recruited', 'mehta', 'divisions', 'full-time', 'rumored', 'tall', 'respected', 'prerogatives', 'specified', 'interpretation', 'clause', 'unconstitutional', 'iran-contra', 'understand', 'broadly', 'presidency', 'separation', 'supported', 'constitutional', 'convention', 'ensure', 'accountability', 'independence', 'perform', 'confederation', 'technically', 'placing', 'rewrite', 'containing', 'alternatively', 'declaring', 'violation', 'appointments', 'granting', 'commissions', 'nomination', 'imposes', 'choosing', 'rider', 'voluntary', 'recommendation', 'recommending', 'discretion', 'select', 'regulations', 'prevents', 'agricultural', 'wasted', 'illustrates', 'implement', 'applicable', 'execution', 'presentation', 'signature', 'v.', 'kinds', 'purposes', 'custom', 'undo', 'mikhail', 'gorbachev', 'ratified', 'salt', 'restricting', 'riders', 'trespass', 'line-item', 'context', 'characterized', 'conflict', 'item', 'downside', 'asserts', 'establish', 'sue', 'loses', 'morrison', 'valuable', 'legislature', 'extending', 'analysis', 'duke', 'farmers', 'pockets', 'gross', 'drought', 'crop', 'midwestern', 'corn', 'plains', 'saved', 'reclaim', 'crops', 'soaring', 'nebraska', 'attributed', 'confirms', 'depression', 'helps', 'reluctance', 'curtailed', 'surpluses', 'wheat', 'strengthened', 'keith', 'collins', 'livestock', 'cattle', 'inventory', 'minnesota', 'disaster', 'relief', 'farms', 'soybeans', 'struggling', 'examiner', 'hearst', 'unsuccessful', 'exceeding', 'second-largest', 'dominates', 'orange', 'register', 'pasadena', 'beach', 'attempted', 'materialized', 'stream', 'prospective', 'sun', 'freeway', 'inevitable', 'limbo', 'torn', 'blue-collar', 'upscale', 'tabloid', 'abandoned', 'marginally', 'castle', 'spanish', 'condition', 'noble', 'deterioration', 'looks', 'bitter', 'recovered', 'moments', 'restored', 'notable', 'consistently', 'disclosures', 'bradley', 'dealings', 'coverage', 'arts', 'representatives', 'recruiting', 'emotional', 'l.a.', 'headline', 'beers', 'drunk', 'andy', 'preference', 'closes', 'forget', 'handed', 'olympia', 'lenders', 'buick', 'beleaguered', 'promotion', 'boosting', 'encouraging', 'four-day', 'begun', 'explaining', 'disclose', 'approached', 'doctor', 'broaden', 'gasoline', 'gm', 'gmac', 'billing', 'considers', 'direct-mail', 'marketers', 'maximize', 'capability', 'incomes', 'missed', 'visa', 'cards', 'establishment', 'responsible', 'tickets', 'meals', 'las', 'vegas', 'nev.', 'buyer', 'chooses', 'fly', 'companion', 'prizes', 'stereo', 'recorder', 'trans', 'luxury', 'interest-rate', 'borough', 'cancellation', 'recovering', 'accounted', 'engage', 'betting', 'obligated', 'swaps', 'vested', 'barclays', 'midland', 'citibank', 'recover', 'arrangements', 'aftermath', '190-point', 'kidder', 'peabody', 'stockbrokers', 'shaken', 'baskets', 'reeling', 'reap', 'labeled', 'evil', 'guard', 'consisting', 'pools', 'fast-growing', 'defending', 'tens', 'successfully', 'layer', 'refuse', 'practitioners', 'broad-based', 'neuberger', 'palace', 'wohlstetter', 'contel', 'consistent', 'waited', 'headed', 'casino', 'civil', 'floors', 'entrenched', 'basket', 'facilitate', 'theme', 'greedy', 'gambling', 'odds', 'old-fashioned', 'pickers', 'unknown', 'divergence', 'occurs', 'constitute', 'whichever', 'cheaper', 'seconds', 'seize', 'movements', 'frightened', 'stoll', 'ual', 'unload', 'takeover-stock', 'arbitragers', 'blue-chip', 'halted', 'apart', 'manage', 'pennies', 'automated', 'threatens', 'virtue', 'monopoly', 'dislike', 'printers', 'spooked', 'swings', 'absolutely', 'deck', 'scaring', 'raymond', 'broker', 'legg', 'transformed', 'complains', 'andrew', 'champion', 'oh', 'brothers', 'publicity', 'strategy', 'psychology', 'proven', 'fastest-growing', 'jeopardy', 'unlikely', 'anytime', 'enjoy', 'margin', 'potentially', 'speculative', 'protects', 'sellers', 'destroy', 'efficiency', 'picks', 'impetus', 'practiced', 'locations', 'widget', 'constantly', 'owning', 'cheapest', 'fundamental', 'vast', 'contribute', 'surrounding', 'opportunities', 'arise', 'implemented', 'harmful', 'sufficient', 'transfers', 'fundamentally', 'ownership', 'hypothetical', 'sacrifice', 'liquidity', 'cleaner', 'cautious', 'please', 'ultimate', 'sufficiently', 'deviation', 'advent', 'boy', 'wooing', 'bringing', 'damaging', 'merits', 'cater', 'advise', 'somehow', 'expert', 'guy', 'spreads', 'surely', 'mutual-fund', 'sad', 'performers', 'profitably', 'temporary', 'taxation', 'etc.', 'loser', 'inviting', 'transfer', 'antitrust', 'agencies', 'affect', 'filings', 'mergers', 'brooks', 'cuts', 'ftc', 'appropriated', 'offsetting', 'hart-scott-rodino', 'notify', 'completing', 'don', 'edwards', 'staffs', 'drastically', 'dismal', 'inhibit', 'noticed', 'ballot', 'patrick', 'maine', 'cruise', 'missiles', 'oklahoma', 'backing', 'christie', 'prints', 'equivalents', 'sotheby', 'stability', 'masters', 'ray', 'photography', 'dialing', 'joel', 'caller', 'celebrity', 'lines', 'break', 'merchandising', 'predicts', 'recovery', 'patients', 'milwaukee', 'blood', 'appetite', 'therapy', 'ann', 'coordinator', 'trimming', 'christmas', 'string', 'israel', 'marine', 'follow', 'diaper', 'comeback', 'shrinking', 'awareness', 'inquiries', 'shortages', 'spurring', 'fasteners', 'briefs', 'columbus', 'marketed', 'tool', 'doubts', 'quantity', 'heading', 'blames', 'junk', 'interior', 'arnold', 'creates', 'inherent', 'curbing', 'schwab', 'skepticism', 'miller', 'anderson', 'fluctuations', 'disappear', 'dealing', 'limiting', 'strictly', 'capitalism', 'immune', 'leery', 'scott', 'weather', 'storm', 'bargains', 'sky', 'spirits', 'corporation', 'laboratories', 'current-carrying', 'superconductor', 'crystals', 'superconductors', 'moderately', 'magnetic', 'cooled', 'wires', 'ceramic', 'technologies', 'generation', 'aspect', 'overcome', 'cautioned', 'van', 'dover', 'samples', 'crystal', 'neutrons', 'large-scale', 'collective', 'sigh', 'demonstrates', 'determining', 'enable', 'combine', 'processes', 'fidelity', 'taylor', 'kane', 'write-downs', 'after-tax', 'offerings', 'pricings', 'non-u.s.', 'syndicate', 'bellwether', 'rated', 'triple-a', 'moody', 'obligation', 'tentatively', 'serial', 'insured', 'ana', 'redevelopment', 'allocation', 'single-a', 'underwriter', 'double-a', 'rating', 'eurobonds', 'equity-purchase', 'indicating', 'sweden', 'swiss', 'francs', 'yamaichi', 'franc', 'pencil', 'dai-ichi', 'kangyo', 'guarantee', 'fan', 'proposition', 'candlestick', 'wonder', 'sink', 'digs', 'afford', 'stadiums', 'ranging', 'looked', 'wealth', 'phoenix', 'backers', 'concede', 'claimed', 'bowl', 'moon', 'money-losing', 'faith', 'egyptian', 'justified', 'schemes', 'reed', 'pence', 'discontinued', 'disposal', 'downgrading', 'cs', 'downgrade', 'maintain', 'issuers', 'warned', 'borrowings', 'drexel', 'burnham', 'lambert', 'difficulty', 'financings', 'structured', 'stretch', 'disarray', 'longer-term', 'delays', 'downgraded', 'riskier', 'collapse', 'acquires', 'expectation', 'generated', 'overcapacity', 'reliance', 'squeeze', 'campeau', 'proving', 'arranged', 'high-risk', 'suisse', 'participant', 'rebounding', 'goodwill', 'disadvantage', 'rivals', 'toll', 'settlements', 'plea', 'insider-trading', 'scandal', 'circulated', 'confident', 'uncertainties', 'mercury', 'experiencing', 'midsized', 'instrumental', 'involvement', 'cycle', 'bureaucracy', 'responsive', 'demands', 'stretched', 'defendants', 'innovation', 'compelling', 'synthetic', 'generic', 'labels', 'daughters', 'mothers', 'thousand', 'recall', 'brand', 'liable', 'pills', 'assessed', 'proportion', 'duck', 'liability', 'identical', 'alike', 'differently', 'lilly', 'pill', 'apples', 'doctrine', 'reversed', 'justices', 'prescription', 'hidden', 'favors', 'beneficial', 'pain', 'anyway', 'predictably', 'understands', 'utterly', 'billion-dollar', 'trash', 'partnership', 'n.v', 'guilders', 'lens', 'surgery', 'inserted', 'refer', 'ec', 'ackerman', 'el', 'salvador', 'habit', 'incentives', 'estimate', 'truck', 'wis.', 'start-up', 'softer', 'deere', 'anticipation', 'edged', 'nikkei', 'selected', 'outnumbered', 'rumors', 'ample', 'sidelines', 'nomura', 'participate', 'tokyu', 'profit-taking', 'dominant', 'directed', 'defensive', 'shell', 'metal', 'nippon', 'winners', 'bolstered', 'affecting', 'intraday', 'reversal', '30-share', 'indication', 'shed', 'waive', 'golden', 'waiver', 'b.a.t', 'goldsmith', 'highs', 'market-makers', 'shopping', 'searched', 'wellcome', 'amsterdam', 'frankfurt', 'zurich', 'paris', 'brussels', 'milan', 'holiday', 'wellington', 'sydney', 'manila', 'calculated', 'geneva', 'equaling', 'walker', 'howard', 'weil', 'occidental', 'shelf', 'zero', 'montgomery', 'mitchell', 'garrison', 'blunt', 'ellis', 'tackle', 'charging', 'irvine', 'calif.-based', 'acid', 'matthews', 'hearings', 'inspection', 'stayed', 'barely', 'revised', 'present', 'worsen', 'day-to-day', 'unemployment', 'reasonably', 'households', 'conducted', 'toledo', 'respondents', 'fixed-income', 'investment-grade', 'rare', 'ranked', 'hanover', 'diverse', 'importance', 'tomorrow', 'nine-month', 'three-month', 'debenture', 'topped', 'wanting', 'ginnie', '12-year', 'derivative', 'drew', 'clothing', 'expense', 'exceeds', 'hurricane', 'hugo', 'earthquake', 'fourth-quarter', 'property\\/casualty', 'michelin', 'monopolies', 'tires', 's.a', 'clean-air', 'unveil', 'centerpiece', 'rain', 'lobbyists', 'weaker', 'emissions', 'chaos', 'cap', 'hailed', 'innovative', 'cost-sharing', 'subsidize', 'sticking', 'avoiding', 'cleanup', 'fuels', 'quietly', 'devise', 'russell', 'resign', 'shaw', 'coordinate', 'deliberately', 'routes', 'falls', 'processing', 'zip', 'handles', 'instrument', 'opposite', 'indexing', 'mix', 'barometer', 'simultaneous', 'newest', 'breed', 'rocket', 'backgrounds', 'hedging', 'fleeting', 'indexes', 'expression', 'financier', 'saul', 'steinberg', 'labor-management', 'clearance', 'definitely', 'wolf', 'airways', '300-a-share', 'formally', 'twist', 'coniston', 'notified', 'skeptical', 'tiger', 'acquirer', 'accord', 'pilots', 'machinists', 'primerica', 'cleared', 'financial-services', 'liabilities', 'intelogic', 'edelman', 'oust', 'datapoint', 'explore', 'specify', 'p.m.', 'est', 'tendered', 'barron', 'first-half', 'trail', 'adjustment', 'bomber', 'emerged', 'graphics', 'singled', 'sagged', 'shuttle', 'engines', 'hits', 'fixed-price', 'austin', 'intel', 'microprocessor', 'microprocessors', 'grower', 'exporter', 'ship', 'rumor', 'brazilian', 'restraints', 'permissible', 'stevenson', 'regardless', 'third-largest', 'shift', 'drastic', 'granted', 'licenses', 'trees', 'cane', 'am', 'curtail', 'confirm', 'atmosphere', 'fleet', 'importer', 'grains', 'buys', 'verge', 'generous', 'copper', 'ignored', 'mine', 'mines', 'reuter', 'emergency', 'bougainville', 'native', 'younkers', 'equitable', 'cos.', 'fred', 'tony', 'lama', 'liquidation', 'paso', 'leather', 'reuters', 'reupke', 'unspecified', 'termed', 'amicable', 'stemmed', 'irrelevant', 'nigel', 'dd', 'unicorp', 'cara', 'donuts', 'diluted', 'poison', 'delaware', 'dunkin', 'deter', 'magnified', 'nonrecurring', 'adjustments', 'softening', 'natural-gas', 'write', 'exploration', 'amortization', 'brunt', 'decliners', 'subdued', 'somewhat', 'arrive', 'firmed', 'faltered', 'philip', 'reluctant', 'stick', 'revival', 'woolworth', 'avon', 'paramount', 'upjohn', 'amr', 'developer', 'trump', 'withdrew', 'mead', 'texaco', 'ex-dividend', 'fe', 'warehouse', 'freeport-mcmoran', 'convert', 'adverse', 'otherwise', 'entity', 'liquidated', 'distributed', 'nissan', 'backdrop', 'experienced', 'remarkable', 'difficulties', 'firmly', 'accident', 'unexpected', 'full-year', 'harry', 'mcdonald', 'unanticipated', 'government-owned', 'assembly', 'distributes', 'produces', 'literature', 'displays', 'meridian', 'multinational', 'derived', 'carnival', 'finland', 'miami-based', 'waertsilae', 'finnish', 'shipyard', 'shipbuilding', 'fantasy', 'slated', 'delivered', 'write-off', 'capitalized', 'adopt', 'severance', 'staying', 'toy', 'peaked', 'unveiled', 'plagued', 'fortunes', 'bounced', 'patch', 'winner', 'bankruptcy-law', 'ortega', 'contras', 'nicaraguan', 'rebels', 'cease-fire', 'thwart', 'balloting', 'brushed', 'renewing', 'contra', 'honduras', 'sandinista', 'troops', 'offensive', 'rebel', 'krenz', 'freedoms', 'socialism', 'communist', 'fled', 'berlin', 'cross-border', 'massive', 'exodus', 'discussion', 'conferees', 'africa', 'armed', 'guerrillas', 'neighboring', 'violating', 'peace', 'territory', 'pretoria', 'alert', 'guerrilla', 'sabotage', 'namibia', 'lebanon', 'salinas', 'pledged', 'modernization', 'pakistan', 'defeated', 'votes', 'trafficking', 'coffee', 'pan', 'cia', 'fbi', 'bomb', 'planted', 'aboard', 'exploded', 'scotland', 'studio', 'jon', 'peters', 'guber', 'laying', 'cost-cutting', 'comparison', 'disasters', 'unconsolidated', 'per-share', 'brisk', 'climb', 'outlays', 'shrank', 'apparent', 'drops', 'crises', 'minimum-wage', 'resigning', 'depends', 'pall', 'spiegel', 'bears', 'shaky', 'flies', 'planes', 'reaching', 'cushion', 'carl', 'jacobs', 'latter', 'distinct', 'reserved', 'till', 'issuer', 'longtime', 'battered', 'gillett', 'restructurings', 'balloon', 'mellon', 'spun', 'capitalize', 'bread-and-butter', 'jonathan', 'sanford', 'nyse', 'genetics', 'bone', 'protein', 'dna', 'treating', 'induce', 'formation', 'proteins', 'defects', 'cancers', 'qualified', 'liberal', 'narrowed', 'discrimination', 'jurisdiction', 'judgment', 'runkel', 'bench', 'vacancy', 'ferdinand', 'specialized', 'pipes', 'g.m.b', 'shedding', 'backlog', 'rulings', 'investigate', 'assess', 'apparel', 'agreements', 'wilmington', 'questioned', 'wertheim', 'schroder', 'trim', 'bidder', 'unwanted', 'suitors', 'trimmed', 'feeding', 'holt', 'wedtech', 'reassuring', 'co-author', 'integrity', 'trip', 'bronx', 'fingers', 'bribe', 'harrison', 'path', 'traveled', 'inception', 'vital', 'army', 'born', 'puerto', 'rico', 'races', 'famous', 'carter', 'rebuilding', "'80s", 'mario', 'sentence', 'bribery', 'peddling', 'politically', 'respectable', 'corrupt', 'scheme', 'bag', 'scams', 'auditors', 'mercedes', 'clothes', 'irving', 'arrested', 'rigid', 'thieves', 'scandals', 'hud', 'insider', 'whenever', 'bloc', 'stern', 'urban', 'bankrupt', 'oy', 'andersson', 'lease', 'swift', 'dashed', 'repay', 'norfolk', 'ralston', 'battery', 'carbon', 'cereal', 'bread', 'five-cent', 'percent', '300-day', 'binge', 'pipeline', 'exporters', 'trains', 'ports', 'strain', 'mississippi', 'upper', 'corps', 'missouri', 'feeds', 'sank', 'alleviate', 'slowed', 'bushel', 'harvest', 'rebuild', 'gather', 'storage', 'permits', 'port', 'coast', 'relieve', 'delayed', 'refinery', 'tightening', 'sell-off', 'silver', 'platinum', 'influenced', 'elders', 'equities', 'warehouses', 'miners', 'procedural', 'upbeat', 'mood', 'ncr', 'hub', 'riding', 'industrywide', 'chunks', 'marcus', 'steelmakers', 'kobe', 'earning', 'bethlehem', 'inland', 'plummeted', 'exceeded', 'projections', 'bradford', 'richer', 'pipe', 'galvanized', 'coated', 'marathon', 'soliciting', 'icahn', 'barrett', 'leon', 'container', 'professors', 'laurence', 'tribe', 'procedure', 'scuttle', 'kennedy', 'spectrum', 'partial', 'shared', 'supports', 'reckless', 'trinity', 'theatre', 'leisure', 'kim', 'undermine', 'contend', 'exercises', 'prompt', 'ideal', 'streamline', 'shots', 'brian', 'counsel', 'lengthy', 'realities', 'object', 'tougher', 'proxy', 'statements', 'files', 'oversees', 'annuity', 'relax', 'ireland', 'excluded', 'weapons', 'asset-backed', 'choices', 'biotechnology', 'instrumentation', 'tyler', 'printing', 'solo', 'creative', 'stoltzman', 'audiences', 'piano', 'ranges', 'jazz', 'exceptions', 'embraced', 'listening', 'appearance', 'musical', 'concert', 'birthday', 'animals', 'glossy', 'buddy', 'eddie', 'two-part', 'singer', 'glamorous', 'sang', 'deep', 'etc', 'amazing', 'warm', 'reich', 'performer', 'lasts', 'illustrated', 'elegant', 'parker', 'accomplished', 'threatening', 'elements', 'accompanied', 'illustrate', 'passes', 'unesco', 'u.n.', 'culture', 'desperate', 'lobbying', 'foreseeable', 'uncovered', 'kgb', 'extreme', 'ridiculous', 'obliged', 'founding', 'peoples', 'conferences', 'principles', 'glasnost', 'eduard', 'shevardnadze', 'undermined', 'doomed', 'manipulation', 'executions', 'hard-line', 'directs', 'human-rights', 'restrictive', 'minds', 'governments', 'genetic', 'belgium', 'hybrid', 'isolated', 'gene', 'thereby', 'robinson', 'delta', 'distance', 'seeds', '1930s', 'naturally', 'inch', 'jan', 'goldberg', 'greenhouse', 'susceptible', 'monsanto', 'eli', 'supposed', 'reproductive', 'sciences', 'virus', 'genetically', 'automatically', 'wind', 'breeding', 'erupted', 'clash', 'circles', 'clout', 'unprecedented', 'roles', 'embarrassed', 'postpone', 'aside', 'injured', 'aligned', 'suggestions', 'bail', 'irresponsible', 'confrontation', 'anxiety', 'opposing', 'credit-card', 'face-to-face', 'itel', 'surprises', 'plastics', 'robots', 'investment-banking', 'nikko', 'oakland', 'thief', 'blair', 'luis', 'indian', 'pro', 'tennis', 'golf', 'quit', 'legendary', 'league', 'commissioner', 'heroes', 'developers', 'shoes', 'palm', 'midmorning', 'uniform', 'fun', 'dave', 'dream', 'kid', 'alex', 'salesman', 'workout', 'tourists', 'seasons', 'pete', 'midday', 'heat', 'camp', 'senators', 'nagging', 'lean', 'pitching', 'proves', 'walked', 'chevy', 'pickup', 'feat', 'roy', 'sport', 'resident', 'lawrence', 'prepares', 'habits', '45-year-old', 'anymore', 'stolen', 'throwing', 'mph', 'throw', 'love', 'cellular', 'franchise', 's.c.', 'pse', 'diesel', 'exchanged', 'bay', 'inflation-adjusted', 'recreational', 'undervalued', 'loan-loss', 'obstacle', 'ridley', 'commons', 'luxury-car', 'unwelcome', 'lobbied', 'eventual', 'reitman', 'barber', 'capel', 'pays', 'lawson', 'silly', 'defenses', 'weakened', 'intensive', 'invitation', 'cooperative', 'secure', 'shocked', 'thatcher', 'privatized', 'allowing', 'hinted', 'combining', 'projected', 'dataproducts', 'dpc', 'mandatory', 'background', 'jerome', 'soften', 'retreating', 'rebounded', 'topic', 'realty', 'shifted', 'losers', 'dealer', 'dragged', 'neck', 'narrower', 'abolish', 'dates', 'alter', 'parliamentary', 'chancellor', 'exchequer', 'selective', 'dax', 'additionally', 'beef', 'pork', 'performances', 'metromedia', 'renamed', 'itt', 'provider', 'long-distance', 'korean', 'projecting', 'postal', 'capable', 'cd', 'banxquote', 'mehl', 'lag', 'jumbo', 'denominations', 'respectively', 'broker-dealer', 'specifically', 'flagship', 'bankamerica', 'ride', 'last-minute', 'journalism', 'athletics', 'suggestion', 'ballooning', 'aviation', 'swelled', 'enthusiasts', 'resemble', 'entry', 'adjacent', 'mountain', 'a.m.', 'attraction', 'hate', 'pilot', 'noon', 'lots', 'deciding', 'passengers', 'steer', 'propane', 'seldom', 'bet', 'leaping', 'tanks', 'wearing', 'blast', 'pleasure', 'farmer', 'precision', 'nursing', 'figured', 'minimize', 'intervened', 'foreign-exchange', 'recognized', 'restrain', 'manuel', 'fundamentals', 'testimony', 'insisting', 'acknowledged', 'vienna', 'destruction', 'treaty', 'europeans', 'soil', 'nato', 'retains', 'arms-control', 'ally', 'parity', 'bargaining', 'timetable', 'seemingly', 'tank', 'negotiation', 'blocking', 'suspected', 'sloppy', 'theirs', 'danger', 'miss', 'somewhere', 'unraveled', 'jets', 'norway', 'jobless', 'soft-drink', 'fraser', 'bottling', 'franchisee', 'eagerness', 'disposable', 'aging', 'heaviest', 'soda', 'boards', 'sharing', 'giovanni', 'agnelli', 'strengthen', 'fiat', 'admit', 'prince', 'aga', 'khan', 'private-sector', 'maria', 'sister', 'ifi', 's.a.', 'lire', 'sailing', 'collaboration', 'weekend', 'readers', 'dangerous', 'mci', 'vetoed', 'newcomers', 'ranking', 'afterward', 'murphy', 'lighting', 'single-a-3', 'single-a-2', 'collateralized', 'generating', 'armstrong', 'carpet', 'upgraded', 'first-quarter', 'slight', 'lend', 'clue', 'horizon', 'witter', 'reynolds', 'sample', 'component', 'components', 'vendor', 'deliveries', 'poll', 'sometime', 'submitted', 'clifford', 'unnecessary', 'berkeley', 'jeans', 'rejection', 'marriage', 'egg', 'tale', 'spy', 'stealing', 'hess', 'secrets', 'eggs', 'unix', 'routinely', 'laboratory', 'user', 'hunter', 'valid', 'hacker', 'legitimate', 'alarm', 'portable', 'nights', 'desk', 'boss', 'chores', 'accessible', 'missile', 'drag', 'terminal', 'tape', 'linking', 'hunt', 'invited', 'theft', 'humor', 'mips', 'microsystems', 'digital', 'sunnyvale', 'instructions', 'risc', 'wedge', 'hewlett-packard', 'motorola', 'speeds', 'essential', 'tandem', 'siemens', 'a.g.', 'minn.', 'brewing', 'bailout', 'counted', 'rtc', 'deficit-reduction', 'gramm-rudman', 'sick', 'allows', 'insolvent', 's&ls', 'meantime', 'mullins', 'dump', 'okla.', 'cubic', 'transmission', 'pipelines', 'cup', 'notebook', 'shere', 'leads', 'driven', 'tips', 'alice', 'cooking', 'ice', 'fool', 'repeated', 'constant', 'moral', 'regularly', 'butter', 'overwhelming', 'newport', 'incumbent', 'milton', 'hollander', 'weekes', 'ousted', 'titles', '2-for-1', 'lumber', 'healthcare', 'standstill', 'healthvest', 'health-care', 'overdue', 'rent', 'owes', 'rehabilitation', '20th', 'myself', 'redemptions', 'painfully', "o'kicki", 'cambria', 'marched', 'flowing', 'king', 'foothills', 'behaved', 'arrogant', 'communities', 'indicted', 'secretaries', 'imperial', 'lawn', 'fix', 'bizarre', 'chambers', 'occasions', 'underwear', 'testified', 'courthouse', 'investigators', 'abused', 'pervasive', 'maurice', 'challenges', 'loath', 'thornburgh', 'married', 'daughter', 'clerk', 'grasp', 'columnist', 'spoken', 'misconduct', 'verdict', 'unexpectedly', 'residents', 'dance', 'lid', 'tightly', 'chair', 'hats', 'jailed', 'seniority', 'fiercely', 'proud', 'dealership', 'unrelated', 'laurel', 'intervene', 'adversary', 'inclined', 'reconsider', 'unfortunate', 'onerous', 'defend', 'brick', 'pre-trial', 'intact', 'innocent', 'comic', 'sand', 'await', 'charitable', 'whatever', 'newsprint', 'excellent', 'beatrice', 'scaled', 'reset', 'float', 'wastewater', 'uninsured', 'remic', '20-year', 'weighted', 'j.c.', 'penney', 'railway', 'monte', 'di', 'deutsche', 'owe', 'minpeco', 'minerals', 'abramson', 'demanded', 'gamble', 'packaged', 'courses', 'drum', 'dipped', 'prepaid', 'reinvest', 'homeowners', 'posed', 'takeovers', 'rage', 'colors', 'adoption', 'reduces', 'evaluate', 'viable', 'chances', 'forever', 'pennzoil', 'prudent', 'intergroup', 'wisconsin', 'informal', 'enfield', 'ontario', 'libel', 'hees', 'financiers', 'bronfman', 'alley', 'backlash', 'reactions', 'avoided', 'heels', 'mid-1980s', 'arco', 'partnerships', 'landmark', 'cooperate', 'interfere', 'rely', 'western-style', 'dubious', 'literally', 'tenants', 'robust', 'lagged', 'finances', 'enemies', 'hollywood', 'bros.', 'signaled', 'injunction', 'guber-peters', 'accusations', 'mgm', 'sworn', 'affidavits', 'walter', 'yetnikoff', 'affidavit', 'co-chief', 'enjoys', 'relationships', 'alongside', 'fray', 'fulfill', 'exclude', 'lie', 'studios', 'batman', 'blockbuster', 'conspiracy', 'evasion', 'conspired', 'spare', 'plead', 'leonard', 'frequent', 'grab', 'seagram', 'upgrade', 'repeat', 'mounting', 'liquor', 'risky', 'penn', 'greene', 'scottish', 'scope', 'leg', '12-month', 'hispanic', 'cholesterol', 'hispanics', 'grocery', 'jewelry', 'cosmetics', 'fake', 'fashionable', 'karen', 'launching', 'hiring', 'sporting', 'borrow', 'animal', 'focuses', 'procter', 'tide', 'convenience', 'oat', 'bran', 'northwest', 'jetliner', 'nwa', 'mcdonnell', 'pursuing', 'airport', 'cook', 'improperly', 'detected', 'survived', 'brother', 'alabama', 'unification', 'mystery', 'multimillion-dollar', 'messiah', 'unified', 'spreading', 'enterprises', 'tax-exempt', 'economically', 'infrastructure', 'spokesmen', 'yankee', 'income-tax', 'enthusiastic', 'misleading', 'flowers', 'peanuts', 'marble', 'sociologist', 'likewise', 'burgess', 'transferred', 'fishing', 'vessels', 'budgets', 'exotic', 'dual', 'investigator', 'hosts', 'retreat', 'respectability', 'intellectuals', 'salvage', 'goals', 'strange', 'scare', 'manville', 'forest', 'recapitalization', 'pravda', 'bloated', 'republics', 'border', 'slashed', 'continent', 'tourist', 'flying', 'agent', 'flight', 'transport', 'israeli', 'protocol', 'venezuela', 'participating', 'song', 'identity', 'marcos', 'privileges', 'spouses', 'keenan', 'racketeering', 'crimes', 'threw', 'objections', 'chicago-based', 'morris', 'advising', 'bruce', 'tap', 'intentionally', 'wars', 'legislator', 'proposes', 'senator', 'embarrassment', 'wore', 'abolished', 'freight', 'schedules', 'dress', 'staffers', 'sung', 'sedan', 'postwar', 'discover', 'kume', 'toyota', 'honda', 'dropping', 'tasks', 'boring', 'hoping', 'dealerships', 'trendy', 'trips', 'productive', 'liked', 'lobby', 'exhibit', 'shifts', 'resisted', 'neglected', 'revamped', 'grabbed', 'recoup', 'optimistic', 'designs', 'batch', 'successes', 'compact', 'trails', 'stanza', 'engine', 'rebates', 'civic', 'infiniti', 'lexus', 'rocks', 'hurry', 'gap', 'headaches', 'tennessee', 'organizing', 'overseeing', 'momentum', 'mca', 'azoff', 'summit', 'naval', 'vessel', 'disrupt', 'announcing', 'diversion', 'ethnic', 'unrest', 'reacting', 'communism', 'damp', 'critic', 'arranging', 'advisers', 'appearances', 'u.s.-soviet', 'terrorism', 'tariffs', 'grow', 'kremlin', 'comparisons', 'gathering', 'readily', 'transform', 'loosen', 'grip', 'suggesting', 'marking', 'bnl', 'fiduciary', 'suitable', 'rome', 'outlined', 'dragging', 'anti-nuclear', 'activists', 'gandhi', 'non-violent', 'disobedience', 'passive', 'webster', 'indians', 'affected', 'truly', 'excuse', 'parks', 'bus', 'illustration', 'tendency', 'violent', 'gestures', 'posture', 'apt', 'site', 'criminals', 'excitement', 'demonstration', 'speeding', 'dies', 'airing', 'politician', 'cameras', 'phil', 'indirectly', 'vietnam', 'hide', 'fabric', 'enemy', 'cambodia', 'morally', 'draft', 'dignity', 'extraordinarily', 'informed', 'probable', 'calm', 'rational', 'deukmejian', 'repairs', 'quake', 'bipartisan', 'consumer-products', 'discounting', 'audits', 'pigs', 'update', 'selection', 'specially', 'deductions', 'dependents', 'satisfied', 'overhaul', 'enactment', 'modified', 'backs', 'negligence', 'urges', 'assessment', 'oregon', 'advises', 'mile', 'ira', 'wash', 'violates', 'delegation', 'rode', 'horses', 'conservation', 'sideline', 'horse', 'inspector', 'arbitrator', 'arbitration', 'burton', 'collect', 'acceptable', 'unwarranted', 'delicate', 'supervisors', 'ironically', 'achenbaum', 'beating', 'worldwide', 'hesitate', 'saatchi', 'hyundai', 'searches', 'invites', 'strokes', 'retire', 'outfit', 'thompson', 'best-known', 'frustrated', 'struck', 'integrate', 'pure', 'accomplish', 'johnston', 'outspoken', 'spots', 'opted', 'macy', 'parade', 'nfl', 'orthodox', 'scene', 'missing', 'deeper', 'prosperity', 'sustain', 'mosbacher', 'deficits', 'satisfy', 'consumed', 'turbulence', 'preserve', 'filipino', 'tries', 'royalties', 'sen', 'knock', 'khmer', 'rouge', 'levy', 'tumor', 'clinic', 'shocks', 'nerves', 'monitored', 'tragedy', 'probe', 'safer', 'multiple', 'workings', 'hand-held', 'finger', 'circuits', 'measuring', 'function', 'clinical', 'stimulate', 'diego', 'handy', 'brains', 'stimulators', 'retrieve', 'seizures', 'sam', 'muscle', 'someday', 'hearts', 'possibilities', 'probing', 'reveal', 'exploring', 'jolted', 'jolt', 'theories', 'jeopardize', 'potent', 'forth', 'palo', 'alto', 'receivables', 'thereafter', 'trustee', 'dizzying', 'musicians', 'perfect', 'wider', 'ocean', 'discipline', 'unpopular', 'revolutionary', 'consume', 'inflows', 'fruit', 'gate', 'appreciate', 'petrochemicals', 'stems', 'leftist', 'trusts', 'bosses', 'catching', 'pledge', 'tumbling', 'pessimistic', 'near-term', 'mildly', 'bullish', 'owen', 'kleinwort', 'benson', 'narrowing', 'influences', 'marked', 'tumble', 'widening', 'knocking', 'predictions', 'regain', 'contended', 'bear', 'culmination', 'hastings', 'repeal', 'helmut', 'tends', 'taxed', 'budgetary', 'coalition', 'mess', 'second-quarter', 'incidents', 'echo', 'restated', 'monitoring', 'vulnerable', 'assumptions', 'wathen', 'pinkerton', 'encountered', 'olympics', 'divestiture', 'demonstrate', 'lbo', 'unprofitable', 'rid', 'boasts', 'alleges', 'concerning', 'falcon', 'long-awaited', 'describing', 'ferry', 'method', 'tucker', 'proceed', 'tighten', 'subsidies', 'affluent', 'pot', 'fha', 'insure', 'persuaded', 'sour', 'idle', 'defaults', 'cranston', 'ensuring', 'ink', 'kemp', 'lies', 'pork-barrel', 'hemorrhaging', 'solving', 'gillette', 'tass', 'rubles', 'ruble', 'sequester', 'thoughts', 'frustration', 'pros', 'daffynition', 'applause', 'drilling', 'arkansas', 'seidman', 'free-market', 'rush', 'radical', 'revision', 'gnp', 'eroding', 'surging', 'contentious', 'strikes', 'pittston', 'nynex', 'capitalists', 'false', 'revamping', 'catastrophic', 'medicare', 'medicaid', 'magic', 'bullet', 'sharon', 'nam', 'availability', 'effectiveness', 'guidelines', 'leased', 'employ', 'utah', 'reception', 'hopeful', 'consolidation', 'landscape', 'oppenheimer', 'weyerhaeuser', 'presents', 'dive', 'strengthening', 'boxes', 'exported', 'rod', 'resource', 'lipton', 'rosen', 'katz', 'barring', 'rallied', 'flurry', 'duff', 'phelps', 'soar', 'underscored', 'leaped', 'variations', 'coatings', 'flexible', 'stateswest', 'mesa', 'abandoning', 'pursuit', 'nevada', 'wyoming', 'depress', 'comply', 'tangible', 'ratio', 'celebration', 'anniversary', 'kraft', 'roots', 'blanket', 'shelves', 'prime-time', 'tucson', 'themes', 'portraying', 'killer', 'mafia', 'rjr', 'nabisco', 'touting', 'legitimacy', 'doldrums', 'microsoft', 'phenomenon', 'robin', 'dominate', 'rallies', 'outperform', 'hambrecht', 'quist', 'stalled', 'in-house', 'suitor', 'gen-probe', 'chugai', 'delaying', 'outcome', 'low-income', 'sessions', 'prohibition', 'alaska', 'prescribed', 'far-reaching', 'refinancing', 'high-interest', 'subsidized', 'disruptions', 'forge', 'aeronautics', 'satellite', 'nasa', 'room', 'byrd', 'neb.', 'denver', 'confusion', 'absence', 'authorization', 'coda', 'consolidate', 'repurchase', 'd', 'ted', 'dedicated', 'loral', 'scenarios', 'stearns', 'presumed', 'contemplating', 'poorer', 'cnw', 'spin', 'belief', 'earthquakes', 'fetch', 'orkem', 'state-controlled', 'coates', 'spencer', 'lincoln', 'regulator', 'deputies', 'danny', 'panamanian', 'keating', 'meat', 'gonzalez', 'kevin', "o'connell", 'fraudulent', 'oddly', 'donated', 'recycling', 'aroused', 'erosion', 'discounted', 'footing', 'dallas-based', 'modernize', 'reverse', 'hot-dipped', 'assurances', '13-week', '26-week', 'tenders', 'televised', 'idaho', 'potato', 'cohen', 'boyer', 'bacteria', 'injection', 'genentech', 'insulin', 'diabetics', 'applying', 'lyonnais', 'cie.', 'mixte', 'societe', 'cie', 'financiere', 'unocal', 'refining', 'revolving', 'underwritten', 'terminals', 'company-owned', 'streamlining', 'accompanying', 'popularity', 'child-care', 'corresponding', 'write-offs', 'averages', 'surface', 'psychological', 'cease', 'cascade', 'pfizer', 'schering-plough', 'chevron', 'definitively', 'ogden', 'cilcorp', 'analytical', 'occupied', 'crush', 'confiscated', 'unpaid', 'staged', 'nicaragua', 'condemn', 'cancel', 'managua', 'afghanistan', 'kabul', 'civilian', 'conservatives', 'dissent', 'whites', 'hall', 'vietnamese', 'tanker', 'fairfield', 'preparing', 'diplomat', 'constituents', 'representation', 'constituency', 'convincing', 'mimic', 'revisions', 'geared', 'craig', 'looms', 'miniscribe', 'disk-drive', 'strategist', 'sharpest', 'tapped', 'pinpoint', 'trough', 'triggering', 'drill', 'bond-equivalent', 'floating', 'contemplated', 'lists', 'calendar', 'tunnel', 'municipals', 'reoffered', 'interpreted', 'prentice', 'electronically', 'governing', 'fasb', 'entities', 'gasb', 'depreciation', 'avery', 'uniroyal', 'intends', 'one-half', 'volumes', 'shelters', 'breath', 'firmer', 'aftershocks', 'creeping', 'fever', 'rescue', 'recessions', 'pepsico', 'cheap', 'flamboyant', 'lows', 'price-earnings', 'ratios', 'restricted', 'bunch', 'likes', 'ozone', 'negligible', 'birth', 'solar', 'radiation', 'doubtful', 'toxic', 'chlorofluorocarbons', 'depletion', 'earth', 'montreal', 'uv-b', 'measurements', 'russians', 'max', 'hence', 'greeted', 'credible', 'cfcs', 'subsequently', 'underground', 'epa', 'honest', 'discovery', 'experiments', 'chemistry', 'quotes', 'teagan', 'atmospheric', 'expertise', 'rifenburgh', 'announcements', 'questioning', 'detailing', 'shipment', 'defective', 'coopers', 'flags', 'flaws', 'definition', 'intimate', 'sexual', 'vivid', 'underwriting', 'underwrite', 'challenged', 'approvals', 'treasurys', 'sinyard', 'cycling', 'bike', 'bicycle', 'desks', 'bikes', 'mountain-bike', 'entrepreneurial', 'eidsmo', 'tighter', 'painful', 'lined', 'frame', 'titanium', 'burke', 'taiwanese', 'niche', 'distributors', 'wholly', 'consolidating', 'pfeiffer', 'manufactures', 'mainframes', 'minicomputers', 'franco', 'accumulation', 'consortium', 'conceded', 'channels', 'installation', 'rupert', 'murdoch', 'courtaulds', 'daimler-benz', 'advancers', 'foreign-currency', 'unstable', 'ig', 'metall', 'tentative', 'capitalization', 'raiders', 'logical', 'enhance', 'kerry', 'zoete', 'wedd', 'affiliated', 'prevention', 'franklin', 'vacated', 'iron', 'busy', 'burst', 'tonight', 'comptroller', 'hastily', 'narrows', 'widens', 'dated', 'sinking', 'new-issue', 'kohlberg', 'kravis', 'roberts', 'carolinas', 'issuance', 'home-equity', 'peat', 'adams', 'diagnostic', 'bologna', 'know-how', 'mich', 'pump', 'technological', 'detect', 'deficiency', 'pitch', 'weaknesses', 'recorders', 'theoretical', 'journalist', 'bets', 'toshiba', 'hurdle', 'credited', 'horrible', 'roper', 'gear', 'fitness', 'intentions', 'doyle', 'desert', 'potatoes', 'bound', 'garden', 'vigorous', 'skiing', 'fad', 'wiped', 'walks', 'leslie', 'bryant', 'bodies', 'bowling', 'endangered', 'commissioned', 'uncertain', 'mills', 'md', 'ghosts', 'haunts', 'skeptics', 'explanation', 'pizza', 'bed', 'dreams', 'hyman', 'celebrating', 'halloween', 'occasion', 'ghost', 'aliens', 'cadillac', 'suits', 'lexington', 'vacuum', 'flew', 'kitchen', 'dog', 'receiver', 'investigated', 'omni', 'shrink', 'carpenter', 'illusion', 'shadow', 'attacked', 'smiling', 'burns', 'sons', 'ken', 'temblor', 'k', 'physicians', 'lets', 'towers', 'burger', 'grades', 'ski', 'debates', 'whoever', 'commanding', 'jose', 'electoral', 'brink', 'platform', 'hedge', 'inefficient', 'uncommon', 'dial', 'interrupted', 'finishing', 'freeze', 'devaluation', 'shield', 'subsidizing', 'runaway', 'humana', 'prevented', 'physician', 'infection', 'nurses', 'knight-ridder', 'nrm', 'restrict', 'distributions', 'cumulative', 'edisto', 'angeles-based', 'jittery', 'reinvested', 'outflows', 'withdrawals', 'maintains', '13th', 'buffer', 'wondering', 'minorities', 'abundant', 'hints', 'quayle', 'voiced', 'pouring', 'gatt', 'soap', 'writes', 'unidentified', 'leaks', 'cheney', 'euphoria', 'jordan', 'voter', 'ryder', 'thurmond', 's.c', 'racial', 'drug-related', 'statutes', 'assassination', 'sentences', 'customs', 'boyd', 'ironic', 'statistical', 'considerations', 'relevant', 'severely', 'convictions', 'chancery', 'realistic', 'lone', 'exclusivity', 'adjusters', 'settling', 'epicenter', 'shook', 'rocked', 'shaking', 'carpeting', 'evacuation', 'wrap', 'proceeding', 'fireman', 'rattled', 'morristown', 'brown-forman', 'jackson', 'jeep', 'struggles', 'translate', 'payroll', 'revco', 'bankruptcy-court', 'bondholders', 'acadia', 'fort', 'woes', 'stein', 'highland', 'cananea', 'mint', 'respective', 'computer-guided', 'demler', 'soybean', 'colombia', 'costa', 'rica', 'colombian', 'cooperatives', 'inevitably', 'politburo', 'demonstrated', 'conform', 'self-employed', 'exercising', 'resolutions', 'wishes', 'solve', 'insufficient', 'conceptual', 'emergence', 'organic', 'blueprint', 'undertaken', 'associations', 'dependent', 'construct', 'heir', 'rein', 'contracting', 'across-the-board', 'single-a-1', 'junior', 'profile', 'quotron', 'four-game', 'sweep', 'widen', 'abc', 'incur', 'disrupted', 'broadcast', 'cities\\/abc', 'playoffs', 'pilson', 'decreased', 'streak', 'towel', 'gainers', 'resignations', 'rows', 'personal-computer', 'concluding', 'poised', 'lotus', 'bureaucrats', 'sits', 'efficiently', 'commonly', 'plug', 'chart', 'disappeared', 'rooms', 'tables', 'contact', 'printer', 'unavailable', 'listen', 'practically', 'explicit', 'subordinates', 'laptop', 'fm', 'compaq', 'specifications', 'blame', 'ru-486', 'surgical', 'swedish', 'bleeding', 'misses', 'pregnancy', 'mortality', 'steering', 'bother', 'contraceptive', 'hoechst', 'prediction', 'legally', 'ignorance', 'supervision', 'aborted', 'furthermore', 'embryo', 'dalkon', 'recording', 'surveys', 'gathered', 'freeman', 'talent', 'reimburse', 'lazard', 'weiss', 'orderly', 'forming', 'ge', 'welch', 'post-crash', 'layoffs', 'greenberg', 'crazy', 'break-even', 'catalyst', 'kemper', 'roth', 'supporting', 'shakespeare', 'ourselves', 'strips', 'gould', 'disney', 'sherman', 'sentenced', 'disappearance', 'en', 'transit', 'garcia', 'cocaine', 'republic', 'leipzig', 'cemetery', 'romantic', 'tacked', 'taped', 'conversation', 'paintings', 'dearborn', 'noise', 'warranty', 'guaranty', 'termination', 'assumes', 'donating', 'cross', 'messages', 'donations', 'hat', 'rank', 'devastation', 'charleston', 'instant', 'dip', 'woo', 'charity', 'chasing', 'barry', 'earmarked', 'franchisees', 'sponsored', 'refusing', 'squibb', 'fda', 'inability', 'limitations', 'award', 'dangers', 'rejecting', 'appealed', 'nestle', 'conclusion', 'topple', 'mandate', 'highways', 'foes', 'terrible', 'justifies', 'c', 'wire', 'traffickers', 'laundering', 'identifying', 'recipients', 'facsimile', 'tpa', 'hardest', 'tax-loss', 'ralph', 'awaiting', 'closings', 'casualty', 'vogelstein', 'colon', 'deadly', 'tumor-suppressor', 'prone', 'retinoblastoma', 'p53', 'chromosome', 'analyzing', 'fox', 'experiment', 'eric', 'confusing', 'confirming', 'hughes', 'levine', 'mice', 'labs', 'rushing', 'bristol-myers', 'accuse', 'replied', 'peladeau', 'quebecor', 'supplement', 'quebec', 'thomson', 'distributing', 'z', 'manitoba', 'searle', 'rubin', 'unlawful', 'law-enforcement', 'omnibus', 'vigorously', 'hancock', 'kelly', 'tops', 'resumed', 'oral', 'terminate', 'nonsense', 'coach', 'doug', 'mediator', 'mighty', 'deferred', 'b-2', 'tactical', 'fighter', 'accords', 'wichita', 'kan.', 'helicopter', 'hourly', 'cost-of-living', 'reiterated', 'aluminum', '1\\/2-year', 'grounds', 'arctic', 'calgary-based', 'alberta', 'vancouver', 'kick', 'mackenzie', 'transcanada', 'tenneco', 'alaskan', 'connecting', 'lived', 'counties', 'bank-holding', 'deferring', 'finish', 'ernst', 'remics', 'rochester', 'zero-coupon', 'a$', 'allianz', 'reinforced', 'dresdner', 'commerzbank', 'canceled', 'regrets', 'noranda', 'carol', 'fletcher', 'amoco', 'undeveloped', 'monitors', 'disks', 'freely', 'prepayments', 'bancroft', 'treasurer', 'patterson', 'wendy', 'midler', 'backup', 'distinctive', 'spawned', 'bobby', 'mack', 'vitro', 'spate', 'bargain-hunting', 'teeth', 'tailspin', 'margaret', 'ems', 'omaha', 'egon', 'arab', 'apartheid', 'fluor', 'prevail', 'insurers', 'swell', 'corr', 'telesis', 'deductible', 'prelude', 'tax-free', 'splitting', 'breakup', 'reinforce', 'phased', 'transatlantic', 'department-store', 'generates', 'mail-order', 'outperformed', 'prompting', 'honeywell', 'editorial-page', 'col.', 'gen.', 'figuring', 'mccall', 'pearce', 'simpson', 'arabs', 'sidhpur', 'fame', 'psyllium', 'patel', 'centuries', 'folk', 'ciba-geigy', 'p&g', 'displayed', 'kellogg', 'marginal', 'middlemen', 'glad', 'crest', 'sr.', 'reasonable', 'md.', 'small-business', 'waxman', 'overly', 'mild', 'priorities', 'prohibit', 'mandated', 'reinsurance', 'belgian', 'provisional', 'nashua', 'slumped', 'outsider', 'proclaimed', 'titled', 'carr', 'regained', 'marxist', 'sovereignty', 'arose', 'emigration', 'uneasy', 'averaging', 'shock', 'flavor', 'baring', 'fallout', 'govern', 'refugees', 'repression', 'projection', 'seng', 'consequently', 'fisher', 'artificially', 'coordination', 'episode', 'decent', 'ben', 'attitudes', 'small-town', 'sohmer', 'pbs', 'jewish', 'kate', 'thrust', 'outset', 'guess', 'naked', 'motel', 'crossland', 'write-down', 'revise', 'non-interest', 'afloat', 'calgary', 'intensely', 'input', 'relocation', 'shippers', 'ivy', 'shoppers', 'coats', 'photo', 'rainbow', 'loud', 'roberti', 'observes', 'paterson', 'casual', 'unused', 'abruptly', 'mtm', 'tvs', 'blues', 'gatward', 'broadcasters', 'connaught', 'vaccine', 'merieux', 'chiron', 'corner', 'peru', 'mineral', 'carry-forward', 'expiration', 'avondale', 'sutton', 'concord', 'gang', 'characteristic', 'amex', 'ufo', 'timothy', 'beam', 'hell', 'destroying', 'shooting', 'memorandum', 'californians', 'beings', 'robot', 'universe', 'russia', 'radar', 'sverdlovsk', 'debris', 'creatures', 'rocky', 'trustcorp', 'akzo', 'bergsma', 'alliances', 'petrochemical', 'bleak', 'universal', 'captured', 'guardian', 'reveals', 'divide', 'camps', 'stabilize', 'stabilizing', 'aggregates', 'deployed', 'chorus', 'tie', 'surrounded', 'freed', 'rampant', 'natwest', 'delegate', 'heller', 'wcrs', 'della', 'femina', 'mcnamee', 'reebok', 'hedges', 'r.h.', 'federated', 'bullock', 'finkelstein', 'viability', 'altman', 'teller', 'chains', 'bloomingdale', 'saks', 'windows', 'candy', 'kkr', 'aetna', 'catastrophe', 'township', 'anc', 'unity', 'sisulu', 'shouted', 'westinghouse', 'steam', 'combustion', 'supplied', 'asea', 'boveri', 'renaissance', 'marous', 'richter', 'pale', 'propelled', 'unfriendly', 'victories', 'joke', 'inning', 'stir', 'scored', 'terry', 'prevailed', 'kirk', 'regular-season', 'homer', 'twelve', 'rationale', 'grumman', 'scheduling', 'concludes', 'estimating', 'mentality', 'deadlines', 'burlington', 'taping', 'audio', 'resale', 'raider', 'werner', 'coup', 'bally', 'jean', 'airplanes', 'grenfell', 'two-day', 'cocom', 'deserves', 'unisys', 'sheer', 'worrisome', 'approaching', 'straszheim', 'commentary', 'newsletter', 'annuities', 'threaten', 'prisoner', 'trusted', 'denounced', 'thrown', 'exchange-rate', 'reunification', 'neutral', 'broderick', 'rental', 'fatal', 'perlman', 'ignore', 'commute', 'criticisms', 'harbor', 'keen', 'neatly', 'greens', 'necessity', 'va', 'neighbor', 'venice', 'catalog', 'translated', 'daewoo', 'sri', 'koreans', 'daly', 'semel', 'hanging', 'canton', 'kurt', 'cleaning', 'mgm\\/ua', 'encouragement', 'suburb', 'darman', 'reminder', 'divisive', 'cms', 'polly', 'sansui', 'adjust', 'tokyo-based', 'offshore', 'competitiveness', 'doubling', 'bellsouth', 'lin', 'mccaw', 'payout', 'flawed', 'declares', 'affects', 'attributable', 'dapuzzo', 'tele-communications', 'resilience', 'upheaval', 'warburg', 'hanson', 'von', 'humanitarian', 'staging', 'flaw', 'installations', 'scrap', 'afghan', 'roads', 'aoun', 'syrian', 'pullout', 'championship', 'unilever', 'nervously', 'packaged-goods', 'faberge', 'noxell', 'makeup', 'mass-market', 'detergent', 'lauder', 'artistic', 'rolls', 'nationally', 'coupons', 'aged', 'supermarkets', 'ordinarily', 'high-end', 'colgate-palmolive', 'colgate', 'covert', '14-year-old', 'wipe', 'coups', 'dictator', 'endanger', 'boren', 'sens.', 'specter', 'internationally', 'harbors', 'iran', 'friday-the-13th', 'gte', 'rand', 'besieged', 'maneuver', 'reinforcing', 'tim', 'ages', 'veterans', 'youngest', 'korotich', 'educate', 'cheered', 'recognizes', 'honesty', 'cynthia', 'greenspan', 'chris', 'dodd', 'accelerate', 'infringement', 'misstated', 'racked', 'cathcart', 'mall', 'incorrectly', 'luzon', 'polyethylene', 'mateo', 'speaker', 'waves', 'marlin', 'fitzwater', 'restoring', 'buried', 'bart', 'hertz', 'bracing', 'listings', 'quantum', 'istat', 'banco', 'exterior', 'privatization', 'pesetas', 'technicians', 'prosecutions', 'conclusions', 'bono', 'justin', 'pretrial', 'grossly', 'ron', 'conspiring', 'wildlife', 'spill', 'havoc', 'instantly', 'jamie', 'whitten', 'remark', 'restraint', 'rican', 'arias', 'interpret', 'wyss', 'totals', 'mega-issues', 'prospectus', 'larsen', 'toubro', 'oversubscribed', 'ministries', 'nathan', 'brewer', 'finnair', 'archrival', 'abbie', 'activist', 'script', 'chung', 'actors', 'benjamin', 'stops', 'theater', 'syndicated', 'joan', 'drama', 'dentsu', 'eurocom', 'corning', 'comedy', 'grid', 'heavier', 'census', 'sears', 'roebuck', 'mart', 'weakest', 'explosions', 'destroyed', 'catastrophes', 'syndicates', 'leaseway', 'equation', 'slipping', 'periodic', 'diverted', 'midst', 'sandinistas', 'abrams', 'bernard', 'petrie', 'deb', 'unanimously', 'liquidate', 'jayark', 'cftc', 'comex', 'conway', 'pits', '24-hour', 'react', 'attribute', 'dire', 'recommends', 'absorb', 'scramble', 'crumbling', 'mayoral', 'unloading', 'spree', 'stop-loss', 'lockheed', 'innopac', 'mobil', 'yards', 'jokes', 'caterpillar', 'whittle', 'dillon', 'octel', 'e', 'revealed', 'briefly', 'scary', 'connolly', 'lbos', 'troubling', 'disappointments', 'richfield', 'faa', 'temperatures', 'flooding', 'coastal', 'atoms', 'dioxide', 'burning', 'fossil', 'chairmen', 'foley', 'subsidy', 'fortunately', 'mainstream', 'parental', 'apartments', 'plumbing', 'middle-class', 'schaeffer', 'vanguard', 'compounded', 'lower-than-expected', 'shifting', 'toseland', 'hypoglycemia', 'costing', 'strategists', 'ivory', 'wsj', 'baseline', 'chiefs', 'strengths', 'discouraged', 'confirmation', 'softness', 'disabilities', 'disabled', 'handicapped', 'deliberations', 'addressed', 'aspirations', 'eurodollar', 'envy', 'shanghai', 'chartered', 'hk$', 'defaulted', 'taxi', 'bullion', 'processed', 'noncallable', 'tampa', 'bausch', 'lenses', 'jolla', 'southwest', 'vincent', 'preceding', 'generale', 'lubricants', 'manic', 'phones', 'bsn', 'fleming', 'trinova', 'biological', 'bacterium', 'diversity', 'bruno', 'pile', 'dunes', 'yard', 'mideast', 'exporting', 'opec', 'cox', 'roller-coaster', 'gallons', 'roller', 'pitched', 'persian', 'iranian', 'rig', 'southwestern', 'boat', 'dun', 'bradstreet', 'setbacks', 'arrow', 'supervisor', 'vault', 'gardens', 'dorfman', 'conner', 'texans', 'attendants', 'controllers', 'house-passed', 'indexation', 'supplying', 'poughkeepsie', 'whooping', 'cough', 'pertussis', 'toxin', 'rothschilds', 'glare', '19th', 'recalled', 'mainstay', 'erich', 'buffett', 'outweigh', 'defenders', 'blaming', 'fares', 'ab', 'skf', 'kronor', 'bearings', 'redford', 'environmentally', 'environmentalism', 'windsor', 'ncaa', 'athletes', 'basketball', 'high-school', 'coaches', 'portrait', 'mad', 'pencils', 'circus', 'fdic', 'refined', 'agip', 'libya', 'upgrading', 'refineries', 'kuwait', 'academy', 'engelken', 'clutter', 'shouting', 'lifetime', 'reads', 'turnpike', 'southam', 'norton', 'eastman', 'roh', 'ordinance', 'espectador', 'custody', 'koch', 'opponent', 'roadways', 'carson', 'withheld', 'endorsement', 'incinerator', 'leventhal', 'flom', 'determination', 'adjuster', 'policyholders', 'rubble', 'parcel', 'shattered', 'hammack', 'earthquake-related', 'assessing', 'dealt', 'linear', 'reviews', 'gently', 'roadway', 'fcc', 'commissioners', 'brouwer', 'tremors', 'lipper', 'anticipate', 'deflator', 'nutritional', 'assassinations', 'grave', 'abuses', 'gridlock', 'remembered', 'accustomed', 'lacking', 'turner', 'mo.', 'newark', 'plain', 'horn', 'resorts', 'qintex', 'skase', 'antar', 'seizure', 'forfeiture', 'tainted', 'applies', 'correction', 'widened', 'accelerating', 'pumped', 'gun', 'workstations', 'fabrication', 'ranch', 'ima', 'willful', 'mahfouz', 'cairo', 'novels', 'span', 'colonial', 'bureaucrat', 'experimental', 'endure', 'poorest', 'norwood', 'weirton', 'restoration', 'leverage', 'northrop', 'iverson', 'thriving', 'dynamic', 'morishita', 'gallery', 'christies', 'aichi', 'holmes', 'wherever', 'nonperforming', 'batibot', 'decker', 'emhart', 'fournier', 'defer', 'pinnacle', 'traub', 'management-led', 'vatican', 'pope', 'distribute', 'unscrupulous', 'collectors', 'giorgio', 'dig', 'notification', 'ernest', 'rep', 'jefferies', 'bakker', 'helmsley', 'bottled', 'packwood', 'tuition', 'marsh', 'mclennan', 'peripherals', 'wang', 'trucking', 'allowance', 'landed', 'deaver', 'shannon', 'confessed', 'perjury', 'topiary', 'retreated', 'lavelle', 'hart', 'fund-raising', 'acquitted', 'cuba', 'oas', 'guinness', 'fish', 'burmah', 'refiners', 'benefiting', 'edgar', 'mitterrand', 'ohbayashi', 'schwarz', 'farrell', 'posner', 'reconciliation', 'epo', 'amgen', 'conception', 'two-tier', 'circulating', 'kodak', 'maynard', 'armonk', 'stunning', 'abm', 'krasnoyarsk', 'cineplex', 'odeon', 'drabinsky', 'cherry', 'workstation', 'bare-faced', 'leval', 'salinger', 'remedy', 'oakes', 'masson', 'admission', 'dishonesty', 'smart', 'atlas', 'arby', 'franchisers', 'seabrook', 'renault', 'panels', 'pwa', 'airbus', 'bartlett', 'ethiopia', 'aeroflot', 'vila', 'pesticide', 'pesticides', 'hazard', 'extract', 'trapped', 'relying', 'dogged', 'sporadic', 'delmed', 'dialysis', 'ehrlich', 'fresenius', 'coins', 'coin', 'cooling', 'coping', 'constructed', 'woods', 'expenditure', 'venture-capital', 'mit', 'entrepreneurs', 'edt', 'broadway', 'hbo', 'showtime', 's', 'benton', 'merksamer', 'jewelers', 'l.j.', 'hooker', 'court-appointed', 'switches', 'arrange', 'hectic', 'ramada', 'drawings', 'shy', 'bpca', 'cornell', 'incorrect', 'revoke', 'medium', 'patience', 'bronner', 'interactive', 'mushrooms', 'medication', 'mirage', 'hotel-casino', 'nugget', 'hilton', 'casinos', 'mom', 'influx', 'suites', 'proliferation', 'overbuilt', 'foreclosed', 'mouth', 'recreation', 'armco', 'jacobson', 'flat-rolled', 'allied-signal', 'provigo', 'non-food', 'lortie', 'nadeau', 'showroom', 'aided', 'nye', 'anheuser', 'beneath', 'brewery', 'abortion-rights', 'zeta', 'seal', 'cboe', 'undoubtedly', 'utilization', 'h.h.', 'ivan', 'boesky', 'class-action', 'cutler', 'ferguson', 'plaintiff', 'simpler', 'businessland', 'schwartz', 'esselte', 'f-14', 'menlo', 'slim', 'butcher', 'tw', 'battled', 'usair', 'importing', 'daikin', 'azt', 'usage', 'pediatric', 'infected', 'subpoena', 'jeff', 'uncle', 'chicken', 'dlj', '\\*', '\\*\\*', 'espn', 'tisch', 'bitterly', 'meredith', 'lineup', 'comsat', 'bozell', 'eroded', 'dictaphone', 'salesmen', 'shack', 'vax', 'fried', 'haas', 'laff', 'lang', 'lorin', 'claimants', 'robins', 'ingersoll', 'fleets', 'maxicare', 'saab-scania', 'saab', 'superfund', 'micro', 'quickview', 'programmers', 'cupertino', 'kageyama', 'motivated', 'fla', 'fusion', 'roe', 'wade', 'dispatched', 'curbs', 'goodyear', 'roderick', 'corry', 'minimills', 'thoroughbred', 'hunting', 'lynn', 'breathing', 'terrorist', 'shamir', 'glazer', 'bookings', 'adm.', 'capita', 'ldp', 'commuters', 'special-interest', 'diet', 'developing-country', 'carat', 'diamonds', 'thick', 'deduction', 'geographic', 'sheraton', 'gorky', 'delicious', 'fujis', 'worm', 'gerrymandering', 'legislatures', 'tro', 'adobe', 'openness', 'anthrax', 'peasant', 'dying', 'peasants', 'fertilizer', 'poles', 'naczelnik', 'urgency', 'centered', 'prosecutorial', 'opera', 'violetta', 'wooden', 'festival', 'madrid', 'convex', 'unfavorable', 'inspectors', 'wonderful', 'whittington', 'planet', 'cypress', 'parkway', 'codes', 'bogart', 'theatrical', 'shea', 'esb', 'inco', 'homefed', 'pepsi', 'murata', 'vickers', 'pons', 'reputable', 'bebear', 'axa', 'applicants', 'cable-tv', 'imo', 'arena', 'berry', 'apogee', 'a.m', 'kaiser', 'prudential', 'junk-holders', 'storer', 'con', 'rubbermaid', 'mmi', 'asarco', 'h.f.', 'ahmanson', 'vista', 'cubs', 'refcorp', 'swung', 'mouse', 'yeast', 'arteries', 'artery', 'gum', 'irish', 'merkur', 'scorpio', 'francis', 'wachovia', 'antibody', 'competent', 'kohl', 'olivetti', 'conasupo', 'fernandez', 'cans', 'probability', 'crandall', 'digest', 'grey', 'advertiser', 'vinson', 'jurors', 'artists', 'personal-injury', 'massage', 'refrigerators', 'freeways', 'subway', 'on-site', 'kasparov', 'd.t.', 'notorious', 'pawn', 'blumenfeld', 'bourbon', 'whiskey', 'garratt', 'ecological', 'legent', 'underestimated', 'clinton', 'tesoro', 'eagle', 'conservatorship', 'dec', 'incompetent', 'boiler', 'permanently', 'ashland', 'lyondell', 'agricole', 'downey', 'cnbc', 'merabank', 'goodson', 'sorrell', 'mattel', 'suez', 'savaiko', 'palladium', 'curve', 'emerson', 'fujisawa', 'enviropact', 'less-developed', 'fazio', 'atlantis', 'galileo', 'detrex', 'guideline', 'volokh', 'polaroid', 'prefers', 'nuovo', 'ambrosiano', 'hard-disk', 'translation', 'southmark', 'name-dropping', 'rhone-poulenc', 'high-definition', 'hdtv', 'pachinko', 'mcduffie', 'recital', 'violin', 'durkin', 'maidenform', 'brawer', 'lesk', 'rosenthal', 'ups', 'impeachment', 'mancuso', 'h&r', 'nsc', 'briggs', 'bureaus', 'sperry', 'lesko', 'intensify', 'cowboys', 'cuban', 'cubans', 'seita', 'preamble', 'barre', 'somalia', 'runway', 'mengistu', 'rafale', 'goupil', 'crusaders', 'dassault', 'cathay', 'air-freight', 'eddington', 'lufthansa', 'agnos', 'bursts', 'massages', 'cafeteria', 'vermont-slauson', 'watts', 'detectors', 'zones', 'banponce', 'ortiz', 'salespeople', 'mcdonough', 'carpets', 'ducks', 'ferranti', 'realist', 'winnebago', 'stabilized', 'tva', 'ekco', 'nora', 'shv', 'banc', '190.58-point', 'anacomp', 'comair', 'quack', 'mansion', 'a.p.', 'trelleborg', 'falconbridge', 'enserch', 'picop', 'imf', 'tremor', 'spacecraft', 'honecker', 'tritium', 'fleischmann', 'feedlots', 'hut', 'deloitte', 'haskins', 'iafp', 'peterson', 'median', 'u', 'tharp', 'enforcers', 'provinces', 'rorer', 'merck', 'soo', 'xtra', 'gintel', 'hasbro', 'stress-related', "d'arcy", 'fur', 'furriers', 'furs', 'mink', 'jackets', 'hepatitis', 'westridge', 'nguyen', 'chan', 'thi', 'bumiputra', 'basir', 'm$', 'plummet', 'skiers', 'outer', 'contested', 'seismic', 'callable', 'newsletters', 'westmoreland', 'devoe', 'erased', 'marlowe', 'cela', 'nimitz', 'reinforcement', 'bonn', 'pro-life', 'isler', 'skipper', 'mural', 'hoelzer', 'belli', 'andreas', 'prisons', 'shutdown', 'gaubert', 'moss', 'benefit-seeking', 'cruz', 'fema', 'biscuits', 'doman', 'amdura', 'commerciale', 'discovision', 'cherokee', 'jupiter', 'dayton', 'corsica', 'beretta', 'enron', 'verwoerd', 'morgenzon', 'guzman', 'cabrera', 'quina', 'pemex', 'daf', 'whitbread', 'beefeater', 'gin', 'redmond', 'salmonella', 'capcom', 'jal', 'petco', 'fossett', 'equitec', 'steinhardt', 'arkla', 'bay-area', 'caltrans', 'lionel', 'caffeine-free', 'bork', 'fk-506', 'laband', 'steppenwolf', 'sagan', 'templeton', 'beebes', 'craven', 'firstsouth', 'gelbart', 'gutfreund', 'warner-lambert', 'shah', 'torrijos', 'lung-cancer', 'bikers', 'bofors', 'parsow', 'caci', 'isi', 'chestman', 'tci', 'trecker', 'unilab']

[batch =  100] train_perplexity =  7388.10, train_loss =  8.91, learning_rate = 20.00000000


[batch =  200] train_perplexity =  2640.31, train_loss =  7.88, learning_rate = 20.00000000


[batch =  300] train_perplexity =  1063.77, train_loss =  6.97, learning_rate = 20.00000000


[batch =  400] train_perplexity =   691.61, train_loss =  6.54, learning_rate = 20.00000000


[batch =  500] train_perplexity =   535.68, train_loss =  6.28, learning_rate = 20.00000000


[batch =  600] train_perplexity =   445.64, train_loss =  6.10, learning_rate = 20.00000000


[batch =  700] train_perplexity =   380.72, train_loss =  5.94, learning_rate = 20.00000000


[batch =  800] train_perplexity =   321.44, train_loss =  5.77, learning_rate = 20.00000000


[batch =  900] train_perplexity =   302.04, train_loss =  5.71, learning_rate = 20.00000000


[batch = 1000] train_perplexity =   294.82, train_loss =  5.69, learning_rate = 20.00000000


[batch = 1100] train_perplexity =   255.95, train_loss =  5.54, learning_rate = 20.00000000


[batch = 1200] train_perplexity =   249.67, train_loss =  5.52, learning_rate = 20.00000000


[batch = 1300] train_perplexity =   234.06, train_loss =  5.46, learning_rate = 20.00000000


[epoch =   1] validation_perplexity =   237.82, validation_loss =  5.47

Saved model to 'e200/ptb_word_gru.pt'...
[epoch =   1] min_validation_perplexity =   237.82, min_validation_loss =  5.47


[batch =  100] train_perplexity =   235.96, train_loss =  5.46, learning_rate = 20.00000000


[batch =  200] train_perplexity =   237.85, train_loss =  5.47, learning_rate = 20.00000000


[batch =  300] train_perplexity =   232.54, train_loss =  5.45, learning_rate = 20.00000000


[batch =  400] train_perplexity =   213.81, train_loss =  5.37, learning_rate = 20.00000000


[batch =  500] train_perplexity =   208.19, train_loss =  5.34, learning_rate = 20.00000000


[batch =  600] train_perplexity =   212.93, train_loss =  5.36, learning_rate = 20.00000000


[batch =  700] train_perplexity =   198.27, train_loss =  5.29, learning_rate = 20.00000000


[batch =  800] train_perplexity =   181.34, train_loss =  5.20, learning_rate = 20.00000000


[batch =  900] train_perplexity =   187.99, train_loss =  5.24, learning_rate = 20.00000000


[batch = 1000] train_perplexity =   191.72, train_loss =  5.26, learning_rate = 20.00000000


[batch = 1100] train_perplexity =   170.30, train_loss =  5.14, learning_rate = 20.00000000


[batch = 1200] train_perplexity =   172.30, train_loss =  5.15, learning_rate = 20.00000000


[batch = 1300] train_perplexity =   168.38, train_loss =  5.13, learning_rate = 20.00000000


[epoch =   2] validation_perplexity =   180.26, validation_loss =  5.19

Saved model to 'e200/ptb_word_gru.pt'...
[epoch =   2] min_validation_perplexity =   180.26, min_validation_loss =  5.19


[batch =  100] train_perplexity =   170.59, train_loss =  5.14, learning_rate = 20.00000000


[batch =  200] train_perplexity =   179.54, train_loss =  5.19, learning_rate = 20.00000000


[batch =  300] train_perplexity =   177.37, train_loss =  5.18, learning_rate = 20.00000000


[batch =  400] train_perplexity =   165.08, train_loss =  5.11, learning_rate = 20.00000000


[batch =  500] train_perplexity =   163.83, train_loss =  5.10, learning_rate = 20.00000000


[batch =  600] train_perplexity =   171.90, train_loss =  5.15, learning_rate = 20.00000000


[batch =  700] train_perplexity =   164.47, train_loss =  5.10, learning_rate = 20.00000000


[batch =  800] train_perplexity =   150.91, train_loss =  5.02, learning_rate = 20.00000000


[batch =  900] train_perplexity =   159.75, train_loss =  5.07, learning_rate = 20.00000000


[batch = 1000] train_perplexity =   164.18, train_loss =  5.10, learning_rate = 20.00000000


[batch = 1100] train_perplexity =   145.98, train_loss =  4.98, learning_rate = 20.00000000


[batch = 1200] train_perplexity =   149.02, train_loss =  5.00, learning_rate = 20.00000000


[batch = 1300] train_perplexity =   146.62, train_loss =  4.99, learning_rate = 20.00000000


[epoch =   3] validation_perplexity =   159.25, validation_loss =  5.07

Saved model to 'e200/ptb_word_gru.pt'...
[epoch =   3] min_validation_perplexity =   159.25, min_validation_loss =  5.07


[batch =  100] train_perplexity =   150.55, train_loss =  5.01, learning_rate = 20.00000000


[batch =  200] train_perplexity =   158.55, train_loss =  5.07, learning_rate = 20.00000000


[batch =  300] train_perplexity =   157.71, train_loss =  5.06, learning_rate = 20.00000000


[batch =  400] train_perplexity =   147.55, train_loss =  4.99, learning_rate = 20.00000000


[batch =  500] train_perplexity =   146.86, train_loss =  4.99, learning_rate = 20.00000000


[batch =  600] train_perplexity =   154.63, train_loss =  5.04, learning_rate = 20.00000000


[batch =  700] train_perplexity =   149.94, train_loss =  5.01, learning_rate = 20.00000000


[batch =  800] train_perplexity =   136.84, train_loss =  4.92, learning_rate = 20.00000000


[batch =  900] train_perplexity =   146.67, train_loss =  4.99, learning_rate = 20.00000000


[batch = 1000] train_perplexity =   152.34, train_loss =  5.03, learning_rate = 20.00000000


[batch = 1100] train_perplexity =   134.56, train_loss =  4.90, learning_rate = 20.00000000


[batch = 1200] train_perplexity =   137.78, train_loss =  4.93, learning_rate = 20.00000000


[batch = 1300] train_perplexity =   136.46, train_loss =  4.92, learning_rate = 20.00000000


[epoch =   4] validation_perplexity =   149.60, validation_loss =  5.01

Saved model to 'e200/ptb_word_gru.pt'...
[epoch =   4] min_validation_perplexity =   149.60, min_validation_loss =  5.01


[batch =  100] train_perplexity =   138.79, train_loss =  4.93, learning_rate = 20.00000000


[batch =  200] train_perplexity =   147.80, train_loss =  5.00, learning_rate = 20.00000000


[batch =  300] train_perplexity =   147.94, train_loss =  5.00, learning_rate = 20.00000000


[batch =  400] train_perplexity =   137.78, train_loss =  4.93, learning_rate = 20.00000000


[batch =  500] train_perplexity =   137.99, train_loss =  4.93, learning_rate = 20.00000000


[batch =  600] train_perplexity =   146.17, train_loss =  4.98, learning_rate = 20.00000000


[batch =  700] train_perplexity =   139.87, train_loss =  4.94, learning_rate = 20.00000000


[batch =  800] train_perplexity =   129.84, train_loss =  4.87, learning_rate = 20.00000000


[batch =  900] train_perplexity =   139.34, train_loss =  4.94, learning_rate = 20.00000000


[batch = 1000] train_perplexity =   144.94, train_loss =  4.98, learning_rate = 20.00000000


[batch = 1100] train_perplexity =   127.56, train_loss =  4.85, learning_rate = 20.00000000


[batch = 1200] train_perplexity =   129.76, train_loss =  4.87, learning_rate = 20.00000000


[batch = 1300] train_perplexity =   128.65, train_loss =  4.86, learning_rate = 20.00000000


[epoch =   5] validation_perplexity =   145.21, validation_loss =  4.98

Saved model to 'e200/ptb_word_gru.pt'...
[epoch =   5] min_validation_perplexity =   145.21, min_validation_loss =  4.98


[batch =  100] train_perplexity =   132.46, train_loss =  4.89, learning_rate = 20.00000000


[batch =  200] train_perplexity =   140.69, train_loss =  4.95, learning_rate = 20.00000000


[batch =  300] train_perplexity =   141.00, train_loss =  4.95, learning_rate = 20.00000000


[batch =  400] train_perplexity =   131.74, train_loss =  4.88, learning_rate = 20.00000000


[batch =  500] train_perplexity =   133.16, train_loss =  4.89, learning_rate = 20.00000000


[batch =  600] train_perplexity =   139.81, train_loss =  4.94, learning_rate = 20.00000000


[batch =  700] train_perplexity =   135.69, train_loss =  4.91, learning_rate = 20.00000000


[batch =  800] train_perplexity =   126.36, train_loss =  4.84, learning_rate = 20.00000000


[batch =  900] train_perplexity =   134.51, train_loss =  4.90, learning_rate = 20.00000000


[batch = 1000] train_perplexity =   140.01, train_loss =  4.94, learning_rate = 20.00000000


[batch = 1100] train_perplexity =   123.53, train_loss =  4.82, learning_rate = 20.00000000


[batch = 1200] train_perplexity =   126.89, train_loss =  4.84, learning_rate = 20.00000000


[batch = 1300] train_perplexity =   126.08, train_loss =  4.84, learning_rate = 20.00000000


[epoch =   6] validation_perplexity =   141.99, validation_loss =  4.96

Saved model to 'e200/ptb_word_gru.pt'...
[epoch =   6] min_validation_perplexity =   141.99, min_validation_loss =  4.96


[batch =  100] train_perplexity =   130.56, train_loss =  4.87, learning_rate = 20.00000000


[batch =  200] train_perplexity =   136.74, train_loss =  4.92, learning_rate = 20.00000000


[batch =  300] train_perplexity =   137.50, train_loss =  4.92, learning_rate = 20.00000000


[batch =  400] train_perplexity =   129.49, train_loss =  4.86, learning_rate = 20.00000000


[batch =  500] train_perplexity =   131.29, train_loss =  4.88, learning_rate = 20.00000000


[batch =  600] train_perplexity =   137.72, train_loss =  4.93, learning_rate = 20.00000000


[batch =  700] train_perplexity =   133.38, train_loss =  4.89, learning_rate = 20.00000000


[batch =  800] train_perplexity =   125.42, train_loss =  4.83, learning_rate = 20.00000000


[batch =  900] train_perplexity =   132.29, train_loss =  4.88, learning_rate = 20.00000000


[batch = 1000] train_perplexity =   139.31, train_loss =  4.94, learning_rate = 20.00000000


[batch = 1100] train_perplexity =   123.92, train_loss =  4.82, learning_rate = 20.00000000


[batch = 1200] train_perplexity =   128.50, train_loss =  4.86, learning_rate = 20.00000000


[batch = 1300] train_perplexity =   127.31, train_loss =  4.85, learning_rate = 20.00000000


[epoch =   7] validation_perplexity =   142.05, validation_loss =  4.96

[epoch =   7] annealing learning_rate = 5.00000000

[batch =  100] train_perplexity =   122.18, train_loss =  4.81, learning_rate = 5.00000000


[batch =  200] train_perplexity =   120.65, train_loss =  4.79, learning_rate = 5.00000000


[batch =  300] train_perplexity =   117.07, train_loss =  4.76, learning_rate = 5.00000000


[batch =  400] train_perplexity =   107.17, train_loss =  4.67, learning_rate = 5.00000000


[batch =  500] train_perplexity =   105.26, train_loss =  4.66, learning_rate = 5.00000000


[batch =  600] train_perplexity =   109.49, train_loss =  4.70, learning_rate = 5.00000000


[batch =  700] train_perplexity =   103.46, train_loss =  4.64, learning_rate = 5.00000000


[batch =  800] train_perplexity =    94.30, train_loss =  4.55, learning_rate = 5.00000000


[batch =  900] train_perplexity =    99.21, train_loss =  4.60, learning_rate = 5.00000000


[batch = 1000] train_perplexity =   102.93, train_loss =  4.63, learning_rate = 5.00000000


[batch = 1100] train_perplexity =    87.76, train_loss =  4.47, learning_rate = 5.00000000


[batch = 1200] train_perplexity =    90.13, train_loss =  4.50, learning_rate = 5.00000000


[batch = 1300] train_perplexity =    87.90, train_loss =  4.48, learning_rate = 5.00000000


[epoch =   8] validation_perplexity =   114.79, validation_loss =  4.74

Saved model to 'e200/ptb_word_gru.pt'...
[epoch =   8] min_validation_perplexity =   114.79, min_validation_loss =  4.74


[batch =  100] train_perplexity =    96.97, train_loss =  4.57, learning_rate = 5.00000000


[batch =  200] train_perplexity =   102.20, train_loss =  4.63, learning_rate = 5.00000000


[batch =  300] train_perplexity =   101.66, train_loss =  4.62, learning_rate = 5.00000000


[batch =  400] train_perplexity =    93.60, train_loss =  4.54, learning_rate = 5.00000000


[batch =  500] train_perplexity =    93.36, train_loss =  4.54, learning_rate = 5.00000000


[batch =  600] train_perplexity =    97.71, train_loss =  4.58, learning_rate = 5.00000000


[batch =  700] train_perplexity =    94.09, train_loss =  4.54, learning_rate = 5.00000000


[batch =  800] train_perplexity =    86.09, train_loss =  4.46, learning_rate = 5.00000000


[batch =  900] train_perplexity =    91.35, train_loss =  4.51, learning_rate = 5.00000000


[batch = 1000] train_perplexity =    95.64, train_loss =  4.56, learning_rate = 5.00000000


[batch = 1100] train_perplexity =    82.43, train_loss =  4.41, learning_rate = 5.00000000


[batch = 1200] train_perplexity =    83.92, train_loss =  4.43, learning_rate = 5.00000000


[batch = 1300] train_perplexity =    83.55, train_loss =  4.43, learning_rate = 5.00000000


[epoch =   9] validation_perplexity =   110.81, validation_loss =  4.71

Saved model to 'e200/ptb_word_gru.pt'...
[epoch =   9] min_validation_perplexity =   110.81, min_validation_loss =  4.71


[batch =  100] train_perplexity =    90.00, train_loss =  4.50, learning_rate = 5.00000000


[batch =  200] train_perplexity =    95.16, train_loss =  4.56, learning_rate = 5.00000000


[batch =  300] train_perplexity =    95.29, train_loss =  4.56, learning_rate = 5.00000000


[batch =  400] train_perplexity =    87.46, train_loss =  4.47, learning_rate = 5.00000000


[batch =  500] train_perplexity =    87.49, train_loss =  4.47, learning_rate = 5.00000000


[batch =  600] train_perplexity =    92.12, train_loss =  4.52, learning_rate = 5.00000000


[batch =  700] train_perplexity =    89.07, train_loss =  4.49, learning_rate = 5.00000000


[batch =  800] train_perplexity =    82.14, train_loss =  4.41, learning_rate = 5.00000000


[batch =  900] train_perplexity =    87.50, train_loss =  4.47, learning_rate = 5.00000000


[batch = 1000] train_perplexity =    91.77, train_loss =  4.52, learning_rate = 5.00000000


[batch = 1100] train_perplexity =    78.80, train_loss =  4.37, learning_rate = 5.00000000


[batch = 1200] train_perplexity =    81.08, train_loss =  4.40, learning_rate = 5.00000000


[batch = 1300] train_perplexity =    80.22, train_loss =  4.38, learning_rate = 5.00000000


[epoch =  10] validation_perplexity =   108.47, validation_loss =  4.69

Saved model to 'e200/ptb_word_gru.pt'...
[epoch =  10] min_validation_perplexity =   108.47, min_validation_loss =  4.69


[batch =  100] train_perplexity =    85.63, train_loss =  4.45, learning_rate = 5.00000000


[batch =  200] train_perplexity =    91.04, train_loss =  4.51, learning_rate = 5.00000000


[batch =  300] train_perplexity =    91.56, train_loss =  4.52, learning_rate = 5.00000000


[batch =  400] train_perplexity =    83.79, train_loss =  4.43, learning_rate = 5.00000000


[batch =  500] train_perplexity =    83.61, train_loss =  4.43, learning_rate = 5.00000000


[batch =  600] train_perplexity =    88.33, train_loss =  4.48, learning_rate = 5.00000000


[batch =  700] train_perplexity =    85.31, train_loss =  4.45, learning_rate = 5.00000000


[batch =  800] train_perplexity =    78.39, train_loss =  4.36, learning_rate = 5.00000000


[batch =  900] train_perplexity =    84.45, train_loss =  4.44, learning_rate = 5.00000000


[batch = 1000] train_perplexity =    88.82, train_loss =  4.49, learning_rate = 5.00000000


[batch = 1100] train_perplexity =    76.35, train_loss =  4.34, learning_rate = 5.00000000


[batch = 1200] train_perplexity =    77.87, train_loss =  4.36, learning_rate = 5.00000000


[batch = 1300] train_perplexity =    77.94, train_loss =  4.36, learning_rate = 5.00000000


[epoch =  11] validation_perplexity =   106.83, validation_loss =  4.67

Saved model to 'e200/ptb_word_gru.pt'...
[epoch =  11] min_validation_perplexity =   106.83, min_validation_loss =  4.67


[batch =  100] train_perplexity =    82.26, train_loss =  4.41, learning_rate = 5.00000000


[batch =  200] train_perplexity =    87.93, train_loss =  4.48, learning_rate = 5.00000000


[batch =  300] train_perplexity =    87.72, train_loss =  4.47, learning_rate = 5.00000000


[batch =  400] train_perplexity =    81.26, train_loss =  4.40, learning_rate = 5.00000000


[batch =  500] train_perplexity =    80.89, train_loss =  4.39, learning_rate = 5.00000000


[batch =  600] train_perplexity =    85.55, train_loss =  4.45, learning_rate = 5.00000000


[batch =  700] train_perplexity =    82.73, train_loss =  4.42, learning_rate = 5.00000000


[batch =  800] train_perplexity =    76.03, train_loss =  4.33, learning_rate = 5.00000000


[batch =  900] train_perplexity =    81.96, train_loss =  4.41, learning_rate = 5.00000000


[batch = 1000] train_perplexity =    86.14, train_loss =  4.46, learning_rate = 5.00000000


[batch = 1100] train_perplexity =    74.24, train_loss =  4.31, learning_rate = 5.00000000


[batch = 1200] train_perplexity =    75.69, train_loss =  4.33, learning_rate = 5.00000000


[batch = 1300] train_perplexity =    76.29, train_loss =  4.33, learning_rate = 5.00000000


[epoch =  12] validation_perplexity =   105.45, validation_loss =  4.66

Saved model to 'e200/ptb_word_gru.pt'...
[epoch =  12] min_validation_perplexity =   105.45, min_validation_loss =  4.66


[batch =  100] train_perplexity =    80.03, train_loss =  4.38, learning_rate = 5.00000000


[batch =  200] train_perplexity =    85.22, train_loss =  4.45, learning_rate = 5.00000000


[batch =  300] train_perplexity =    85.31, train_loss =  4.45, learning_rate = 5.00000000


[batch =  400] train_perplexity =    78.64, train_loss =  4.36, learning_rate = 5.00000000


[batch =  500] train_perplexity =    78.75, train_loss =  4.37, learning_rate = 5.00000000


[batch =  600] train_perplexity =    83.27, train_loss =  4.42, learning_rate = 5.00000000


[batch =  700] train_perplexity =    80.78, train_loss =  4.39, learning_rate = 5.00000000


[batch =  800] train_perplexity =    74.04, train_loss =  4.30, learning_rate = 5.00000000


[batch =  900] train_perplexity =    80.27, train_loss =  4.39, learning_rate = 5.00000000


[batch = 1000] train_perplexity =    84.20, train_loss =  4.43, learning_rate = 5.00000000


[batch = 1100] train_perplexity =    72.36, train_loss =  4.28, learning_rate = 5.00000000


[batch = 1200] train_perplexity =    74.14, train_loss =  4.31, learning_rate = 5.00000000


[batch = 1300] train_perplexity =    74.04, train_loss =  4.30, learning_rate = 5.00000000


[epoch =  13] validation_perplexity =   105.02, validation_loss =  4.65

Saved model to 'e200/ptb_word_gru.pt'...
[epoch =  13] min_validation_perplexity =   105.02, min_validation_loss =  4.65


[batch =  100] train_perplexity =    77.76, train_loss =  4.35, learning_rate = 5.00000000


[batch =  200] train_perplexity =    82.72, train_loss =  4.42, learning_rate = 5.00000000


[batch =  300] train_perplexity =    83.19, train_loss =  4.42, learning_rate = 5.00000000


[batch =  400] train_perplexity =    76.83, train_loss =  4.34, learning_rate = 5.00000000


[batch =  500] train_perplexity =    76.23, train_loss =  4.33, learning_rate = 5.00000000


[batch =  600] train_perplexity =    81.15, train_loss =  4.40, learning_rate = 5.00000000


[batch =  700] train_perplexity =    78.70, train_loss =  4.37, learning_rate = 5.00000000


[batch =  800] train_perplexity =    72.15, train_loss =  4.28, learning_rate = 5.00000000


[batch =  900] train_perplexity =    78.09, train_loss =  4.36, learning_rate = 5.00000000


[batch = 1000] train_perplexity =    82.54, train_loss =  4.41, learning_rate = 5.00000000


[batch = 1100] train_perplexity =    70.82, train_loss =  4.26, learning_rate = 5.00000000


[batch = 1200] train_perplexity =    72.68, train_loss =  4.29, learning_rate = 5.00000000


[batch = 1300] train_perplexity =    72.54, train_loss =  4.28, learning_rate = 5.00000000


[epoch =  14] validation_perplexity =   103.99, validation_loss =  4.64

Saved model to 'e200/ptb_word_gru.pt'...
[epoch =  14] min_validation_perplexity =   103.99, min_validation_loss =  4.64


[batch =  100] train_perplexity =    75.67, train_loss =  4.33, learning_rate = 5.00000000


[batch =  200] train_perplexity =    81.07, train_loss =  4.40, learning_rate = 5.00000000


[batch =  300] train_perplexity =    81.46, train_loss =  4.40, learning_rate = 5.00000000


[batch =  400] train_perplexity =    74.94, train_loss =  4.32, learning_rate = 5.00000000


[batch =  500] train_perplexity =    74.77, train_loss =  4.31, learning_rate = 5.00000000


[batch =  600] train_perplexity =    79.31, train_loss =  4.37, learning_rate = 5.00000000


[batch =  700] train_perplexity =    76.74, train_loss =  4.34, learning_rate = 5.00000000


[batch =  800] train_perplexity =    71.09, train_loss =  4.26, learning_rate = 5.00000000


[batch =  900] train_perplexity =    77.23, train_loss =  4.35, learning_rate = 5.00000000


[batch = 1000] train_perplexity =    80.58, train_loss =  4.39, learning_rate = 5.00000000


[batch = 1100] train_perplexity =    69.56, train_loss =  4.24, learning_rate = 5.00000000


[batch = 1200] train_perplexity =    70.71, train_loss =  4.26, learning_rate = 5.00000000


[batch = 1300] train_perplexity =    71.22, train_loss =  4.27, learning_rate = 5.00000000


[epoch =  15] validation_perplexity =   103.59, validation_loss =  4.64

Saved model to 'e200/ptb_word_gru.pt'...
[epoch =  15] min_validation_perplexity =   103.59, min_validation_loss =  4.64


[batch =  100] train_perplexity =    74.41, train_loss =  4.31, learning_rate = 5.00000000


[batch =  200] train_perplexity =    79.69, train_loss =  4.38, learning_rate = 5.00000000


[batch =  300] train_perplexity =    79.84, train_loss =  4.38, learning_rate = 5.00000000


[batch =  400] train_perplexity =    73.69, train_loss =  4.30, learning_rate = 5.00000000


[batch =  500] train_perplexity =    73.07, train_loss =  4.29, learning_rate = 5.00000000


[batch =  600] train_perplexity =    77.74, train_loss =  4.35, learning_rate = 5.00000000


[batch =  700] train_perplexity =    75.04, train_loss =  4.32, learning_rate = 5.00000000


[batch =  800] train_perplexity =    69.26, train_loss =  4.24, learning_rate = 5.00000000


[batch =  900] train_perplexity =    75.48, train_loss =  4.32, learning_rate = 5.00000000


[batch = 1000] train_perplexity =    79.53, train_loss =  4.38, learning_rate = 5.00000000


[batch = 1100] train_perplexity =    68.16, train_loss =  4.22, learning_rate = 5.00000000


[batch = 1200] train_perplexity =    69.82, train_loss =  4.25, learning_rate = 5.00000000


[batch = 1300] train_perplexity =    70.30, train_loss =  4.25, learning_rate = 5.00000000


[epoch =  16] validation_perplexity =   103.12, validation_loss =  4.64

Saved model to 'e200/ptb_word_gru.pt'...
[epoch =  16] min_validation_perplexity =   103.12, min_validation_loss =  4.64


[batch =  100] train_perplexity =    72.79, train_loss =  4.29, learning_rate = 5.00000000


[batch =  200] train_perplexity =    77.36, train_loss =  4.35, learning_rate = 5.00000000


[batch =  300] train_perplexity =    78.47, train_loss =  4.36, learning_rate = 5.00000000


[batch =  400] train_perplexity =    71.90, train_loss =  4.28, learning_rate = 5.00000000


[batch =  500] train_perplexity =    72.23, train_loss =  4.28, learning_rate = 5.00000000


[batch =  600] train_perplexity =    76.05, train_loss =  4.33, learning_rate = 5.00000000


[batch =  700] train_perplexity =    73.91, train_loss =  4.30, learning_rate = 5.00000000


[batch =  800] train_perplexity =    67.82, train_loss =  4.22, learning_rate = 5.00000000


[batch =  900] train_perplexity =    74.36, train_loss =  4.31, learning_rate = 5.00000000


[batch = 1000] train_perplexity =    77.96, train_loss =  4.36, learning_rate = 5.00000000


[batch = 1100] train_perplexity =    67.01, train_loss =  4.20, learning_rate = 5.00000000


[batch = 1200] train_perplexity =    68.40, train_loss =  4.23, learning_rate = 5.00000000


[batch = 1300] train_perplexity =    68.72, train_loss =  4.23, learning_rate = 5.00000000


[epoch =  17] validation_perplexity =   102.74, validation_loss =  4.63

Saved model to 'e200/ptb_word_gru.pt'...
[epoch =  17] min_validation_perplexity =   102.74, min_validation_loss =  4.63


[batch =  100] train_perplexity =    71.74, train_loss =  4.27, learning_rate = 5.00000000


[batch =  200] train_perplexity =    76.65, train_loss =  4.34, learning_rate = 5.00000000


[batch =  300] train_perplexity =    76.58, train_loss =  4.34, learning_rate = 5.00000000


[batch =  400] train_perplexity =    70.72, train_loss =  4.26, learning_rate = 5.00000000


[batch =  500] train_perplexity =    70.56, train_loss =  4.26, learning_rate = 5.00000000


[batch =  600] train_perplexity =    74.88, train_loss =  4.32, learning_rate = 5.00000000


[batch =  700] train_perplexity =    72.69, train_loss =  4.29, learning_rate = 5.00000000


[batch =  800] train_perplexity =    67.10, train_loss =  4.21, learning_rate = 5.00000000


[batch =  900] train_perplexity =    73.25, train_loss =  4.29, learning_rate = 5.00000000


[batch = 1000] train_perplexity =    76.77, train_loss =  4.34, learning_rate = 5.00000000


[batch = 1100] train_perplexity =    66.10, train_loss =  4.19, learning_rate = 5.00000000


[batch = 1200] train_perplexity =    67.13, train_loss =  4.21, learning_rate = 5.00000000


[batch = 1300] train_perplexity =    67.84, train_loss =  4.22, learning_rate = 5.00000000


[epoch =  18] validation_perplexity =   102.23, validation_loss =  4.63

Saved model to 'e200/ptb_word_gru.pt'...
[epoch =  18] min_validation_perplexity =   102.23, min_validation_loss =  4.63


[batch =  100] train_perplexity =    70.02, train_loss =  4.25, learning_rate = 5.00000000


[batch =  200] train_perplexity =    74.81, train_loss =  4.31, learning_rate = 5.00000000


[batch =  300] train_perplexity =    75.05, train_loss =  4.32, learning_rate = 5.00000000


[batch =  400] train_perplexity =    69.62, train_loss =  4.24, learning_rate = 5.00000000


[batch =  500] train_perplexity =    69.44, train_loss =  4.24, learning_rate = 5.00000000


[batch =  600] train_perplexity =    73.68, train_loss =  4.30, learning_rate = 5.00000000


[batch =  700] train_perplexity =    71.74, train_loss =  4.27, learning_rate = 5.00000000


[batch =  800] train_perplexity =    66.03, train_loss =  4.19, learning_rate = 5.00000000


[batch =  900] train_perplexity =    72.01, train_loss =  4.28, learning_rate = 5.00000000


[batch = 1000] train_perplexity =    75.87, train_loss =  4.33, learning_rate = 5.00000000


[batch = 1100] train_perplexity =    64.65, train_loss =  4.17, learning_rate = 5.00000000


[batch = 1200] train_perplexity =    66.47, train_loss =  4.20, learning_rate = 5.00000000


[batch = 1300] train_perplexity =    67.12, train_loss =  4.21, learning_rate = 5.00000000


[epoch =  19] validation_perplexity =   102.19, validation_loss =  4.63

Saved model to 'e200/ptb_word_gru.pt'...
[epoch =  19] min_validation_perplexity =   102.19, min_validation_loss =  4.63


[batch =  100] train_perplexity =    69.24, train_loss =  4.24, learning_rate = 5.00000000


[batch =  200] train_perplexity =    73.76, train_loss =  4.30, learning_rate = 5.00000000


[batch =  300] train_perplexity =    74.39, train_loss =  4.31, learning_rate = 5.00000000


[batch =  400] train_perplexity =    68.89, train_loss =  4.23, learning_rate = 5.00000000


[batch =  500] train_perplexity =    68.67, train_loss =  4.23, learning_rate = 5.00000000


[batch =  600] train_perplexity =    72.91, train_loss =  4.29, learning_rate = 5.00000000


[batch =  700] train_perplexity =    70.72, train_loss =  4.26, learning_rate = 5.00000000


[batch =  800] train_perplexity =    65.00, train_loss =  4.17, learning_rate = 5.00000000


[batch =  900] train_perplexity =    71.32, train_loss =  4.27, learning_rate = 5.00000000


[batch = 1000] train_perplexity =    74.75, train_loss =  4.31, learning_rate = 5.00000000


[batch = 1100] train_perplexity =    64.07, train_loss =  4.16, learning_rate = 5.00000000


[batch = 1200] train_perplexity =    65.94, train_loss =  4.19, learning_rate = 5.00000000


[batch = 1300] train_perplexity =    65.80, train_loss =  4.19, learning_rate = 5.00000000


[epoch =  20] validation_perplexity =   101.35, validation_loss =  4.62

Saved model to 'e200/ptb_word_gru.pt'...
[epoch =  20] min_validation_perplexity =   101.35, min_validation_loss =  4.62


[batch =  100] train_perplexity =    68.63, train_loss =  4.23, learning_rate = 5.00000000


[batch =  200] train_perplexity =    72.99, train_loss =  4.29, learning_rate = 5.00000000


[batch =  300] train_perplexity =    72.90, train_loss =  4.29, learning_rate = 5.00000000


[batch =  400] train_perplexity =    67.43, train_loss =  4.21, learning_rate = 5.00000000


[batch =  500] train_perplexity =    67.07, train_loss =  4.21, learning_rate = 5.00000000


[batch =  600] train_perplexity =    71.73, train_loss =  4.27, learning_rate = 5.00000000


[batch =  700] train_perplexity =    69.69, train_loss =  4.24, learning_rate = 5.00000000


[batch =  800] train_perplexity =    64.27, train_loss =  4.16, learning_rate = 5.00000000


[batch =  900] train_perplexity =    70.28, train_loss =  4.25, learning_rate = 5.00000000


[batch = 1000] train_perplexity =    73.59, train_loss =  4.30, learning_rate = 5.00000000


[batch = 1100] train_perplexity =    63.31, train_loss =  4.15, learning_rate = 5.00000000


[batch = 1200] train_perplexity =    64.66, train_loss =  4.17, learning_rate = 5.00000000


[batch = 1300] train_perplexity =    65.62, train_loss =  4.18, learning_rate = 5.00000000


[epoch =  21] validation_perplexity =   101.39, validation_loss =  4.62

[epoch =  21] annealing learning_rate = 1.25000000

[batch =  100] train_perplexity =    67.40, train_loss =  4.21, learning_rate = 1.25000000


[batch =  200] train_perplexity =    71.51, train_loss =  4.27, learning_rate = 1.25000000


[batch =  300] train_perplexity =    70.80, train_loss =  4.26, learning_rate = 1.25000000


[batch =  400] train_perplexity =    64.71, train_loss =  4.17, learning_rate = 1.25000000


[batch =  500] train_perplexity =    64.23, train_loss =  4.16, learning_rate = 1.25000000


[batch =  600] train_perplexity =    67.90, train_loss =  4.22, learning_rate = 1.25000000


[batch =  700] train_perplexity =    64.97, train_loss =  4.17, learning_rate = 1.25000000


[batch =  800] train_perplexity =    60.07, train_loss =  4.10, learning_rate = 1.25000000


[batch =  900] train_perplexity =    64.81, train_loss =  4.17, learning_rate = 1.25000000


[batch = 1000] train_perplexity =    67.75, train_loss =  4.22, learning_rate = 1.25000000


[batch = 1100] train_perplexity =    57.59, train_loss =  4.05, learning_rate = 1.25000000


[batch = 1200] train_perplexity =    58.10, train_loss =  4.06, learning_rate = 1.25000000


[batch = 1300] train_perplexity =    58.29, train_loss =  4.07, learning_rate = 1.25000000


[epoch =  22] validation_perplexity =    97.85, validation_loss =  4.58

Saved model to 'e200/ptb_word_gru.pt'...
[epoch =  22] min_validation_perplexity =    97.85, min_validation_loss =  4.58


[batch =  100] train_perplexity =    64.41, train_loss =  4.17, learning_rate = 1.25000000


[batch =  200] train_perplexity =    68.87, train_loss =  4.23, learning_rate = 1.25000000


[batch =  300] train_perplexity =    68.45, train_loss =  4.23, learning_rate = 1.25000000


[batch =  400] train_perplexity =    62.83, train_loss =  4.14, learning_rate = 1.25000000


[batch =  500] train_perplexity =    62.49, train_loss =  4.14, learning_rate = 1.25000000


[batch =  600] train_perplexity =    66.44, train_loss =  4.20, learning_rate = 1.25000000


[batch =  700] train_perplexity =    63.78, train_loss =  4.16, learning_rate = 1.25000000


[batch =  800] train_perplexity =    58.65, train_loss =  4.07, learning_rate = 1.25000000


[batch =  900] train_perplexity =    64.12, train_loss =  4.16, learning_rate = 1.25000000


[batch = 1000] train_perplexity =    66.95, train_loss =  4.20, learning_rate = 1.25000000


[batch = 1100] train_perplexity =    56.83, train_loss =  4.04, learning_rate = 1.25000000


[batch = 1200] train_perplexity =    57.53, train_loss =  4.05, learning_rate = 1.25000000


[batch = 1300] train_perplexity =    57.85, train_loss =  4.06, learning_rate = 1.25000000


[epoch =  23] validation_perplexity =    97.41, validation_loss =  4.58

Saved model to 'e200/ptb_word_gru.pt'...
[epoch =  23] min_validation_perplexity =    97.41, min_validation_loss =  4.58


[batch =  100] train_perplexity =    63.28, train_loss =  4.15, learning_rate = 1.25000000


[batch =  200] train_perplexity =    67.74, train_loss =  4.22, learning_rate = 1.25000000


[batch =  300] train_perplexity =    67.27, train_loss =  4.21, learning_rate = 1.25000000


[batch =  400] train_perplexity =    62.15, train_loss =  4.13, learning_rate = 1.25000000


[batch =  500] train_perplexity =    61.48, train_loss =  4.12, learning_rate = 1.25000000


[batch =  600] train_perplexity =    65.59, train_loss =  4.18, learning_rate = 1.25000000


[batch =  700] train_perplexity =    63.14, train_loss =  4.15, learning_rate = 1.25000000


[batch =  800] train_perplexity =    58.16, train_loss =  4.06, learning_rate = 1.25000000


[batch =  900] train_perplexity =    63.47, train_loss =  4.15, learning_rate = 1.25000000


[batch = 1000] train_perplexity =    66.29, train_loss =  4.19, learning_rate = 1.25000000


[batch = 1100] train_perplexity =    56.73, train_loss =  4.04, learning_rate = 1.25000000


[batch = 1200] train_perplexity =    57.50, train_loss =  4.05, learning_rate = 1.25000000


[batch = 1300] train_perplexity =    58.01, train_loss =  4.06, learning_rate = 1.25000000


[epoch =  24] validation_perplexity =    97.12, validation_loss =  4.58

Saved model to 'e200/ptb_word_gru.pt'...
[epoch =  24] min_validation_perplexity =    97.12, min_validation_loss =  4.58


[batch =  100] train_perplexity =    62.50, train_loss =  4.14, learning_rate = 1.25000000


[batch =  200] train_perplexity =    67.03, train_loss =  4.21, learning_rate = 1.25000000


[batch =  300] train_perplexity =    66.66, train_loss =  4.20, learning_rate = 1.25000000


[batch =  400] train_perplexity =    61.62, train_loss =  4.12, learning_rate = 1.25000000


[batch =  500] train_perplexity =    61.09, train_loss =  4.11, learning_rate = 1.25000000


[batch =  600] train_perplexity =    64.69, train_loss =  4.17, learning_rate = 1.25000000


[batch =  700] train_perplexity =    63.11, train_loss =  4.14, learning_rate = 1.25000000


[batch =  800] train_perplexity =    57.91, train_loss =  4.06, learning_rate = 1.25000000


[batch =  900] train_perplexity =    62.86, train_loss =  4.14, learning_rate = 1.25000000


[batch = 1000] train_perplexity =    65.83, train_loss =  4.19, learning_rate = 1.25000000


[batch = 1100] train_perplexity =    56.79, train_loss =  4.04, learning_rate = 1.25000000


[batch = 1200] train_perplexity =    57.21, train_loss =  4.05, learning_rate = 1.25000000


[batch = 1300] train_perplexity =    57.71, train_loss =  4.06, learning_rate = 1.25000000


[epoch =  25] validation_perplexity =    97.12, validation_loss =  4.58

[epoch =  25] annealing learning_rate = 0.31250000

[batch =  100] train_perplexity =    62.50, train_loss =  4.14, learning_rate = 0.31250000


[batch =  200] train_perplexity =    67.03, train_loss =  4.21, learning_rate = 0.31250000


[batch =  300] train_perplexity =    66.77, train_loss =  4.20, learning_rate = 0.31250000


[batch =  400] train_perplexity =    61.00, train_loss =  4.11, learning_rate = 0.31250000


[batch =  500] train_perplexity =    60.42, train_loss =  4.10, learning_rate = 0.31250000


[batch =  600] train_perplexity =    64.39, train_loss =  4.17, learning_rate = 0.31250000


[batch =  700] train_perplexity =    62.19, train_loss =  4.13, learning_rate = 0.31250000


[batch =  800] train_perplexity =    56.70, train_loss =  4.04, learning_rate = 0.31250000


[batch =  900] train_perplexity =    61.46, train_loss =  4.12, learning_rate = 0.31250000


[batch = 1000] train_perplexity =    64.72, train_loss =  4.17, learning_rate = 0.31250000


[batch = 1100] train_perplexity =    54.95, train_loss =  4.01, learning_rate = 0.31250000


[batch = 1200] train_perplexity =    55.56, train_loss =  4.02, learning_rate = 0.31250000


[batch = 1300] train_perplexity =    55.86, train_loss =  4.02, learning_rate = 0.31250000


[epoch =  26] validation_perplexity =    96.33, validation_loss =  4.57

Saved model to 'e200/ptb_word_gru.pt'...
[epoch =  26] min_validation_perplexity =    96.33, min_validation_loss =  4.57


[batch =  100] train_perplexity =    61.93, train_loss =  4.13, learning_rate = 0.31250000


[batch =  200] train_perplexity =    65.85, train_loss =  4.19, learning_rate = 0.31250000


[batch =  300] train_perplexity =    65.82, train_loss =  4.19, learning_rate = 0.31250000


[batch =  400] train_perplexity =    60.69, train_loss =  4.11, learning_rate = 0.31250000


[batch =  500] train_perplexity =    60.48, train_loss =  4.10, learning_rate = 0.31250000


[batch =  600] train_perplexity =    64.12, train_loss =  4.16, learning_rate = 0.31250000


[batch =  700] train_perplexity =    61.67, train_loss =  4.12, learning_rate = 0.31250000


[batch =  800] train_perplexity =    56.28, train_loss =  4.03, learning_rate = 0.31250000


[batch =  900] train_perplexity =    61.50, train_loss =  4.12, learning_rate = 0.31250000


[batch = 1000] train_perplexity =    64.22, train_loss =  4.16, learning_rate = 0.31250000


[batch = 1100] train_perplexity =    54.74, train_loss =  4.00, learning_rate = 0.31250000


[batch = 1200] train_perplexity =    55.58, train_loss =  4.02, learning_rate = 0.31250000


[batch = 1300] train_perplexity =    55.81, train_loss =  4.02, learning_rate = 0.31250000


[epoch =  27] validation_perplexity =    96.23, validation_loss =  4.57

Saved model to 'e200/ptb_word_gru.pt'...
[epoch =  27] min_validation_perplexity =    96.23, min_validation_loss =  4.57


[batch =  100] train_perplexity =    61.60, train_loss =  4.12, learning_rate = 0.31250000


[batch =  200] train_perplexity =    65.90, train_loss =  4.19, learning_rate = 0.31250000


[batch =  300] train_perplexity =    65.47, train_loss =  4.18, learning_rate = 0.31250000


[batch =  400] train_perplexity =    60.41, train_loss =  4.10, learning_rate = 0.31250000


[batch =  500] train_perplexity =    59.85, train_loss =  4.09, learning_rate = 0.31250000


[batch =  600] train_perplexity =    63.60, train_loss =  4.15, learning_rate = 0.31250000


[batch =  700] train_perplexity =    61.28, train_loss =  4.12, learning_rate = 0.31250000


[batch =  800] train_perplexity =    56.51, train_loss =  4.03, learning_rate = 0.31250000


[batch =  900] train_perplexity =    61.19, train_loss =  4.11, learning_rate = 0.31250000


[batch = 1000] train_perplexity =    64.24, train_loss =  4.16, learning_rate = 0.31250000


[batch = 1100] train_perplexity =    54.78, train_loss =  4.00, learning_rate = 0.31250000


[batch = 1200] train_perplexity =    55.12, train_loss =  4.01, learning_rate = 0.31250000


[batch = 1300] train_perplexity =    55.90, train_loss =  4.02, learning_rate = 0.31250000


[epoch =  28] validation_perplexity =    96.15, validation_loss =  4.57

Saved model to 'e200/ptb_word_gru.pt'...
[epoch =  28] min_validation_perplexity =    96.15, min_validation_loss =  4.57


[batch =  100] train_perplexity =    61.17, train_loss =  4.11, learning_rate = 0.31250000


[batch =  200] train_perplexity =    65.64, train_loss =  4.18, learning_rate = 0.31250000


[batch =  300] train_perplexity =    65.17, train_loss =  4.18, learning_rate = 0.31250000


[batch =  400] train_perplexity =    59.73, train_loss =  4.09, learning_rate = 0.31250000


[batch =  500] train_perplexity =    59.40, train_loss =  4.08, learning_rate = 0.31250000


[batch =  600] train_perplexity =    63.40, train_loss =  4.15, learning_rate = 0.31250000


[batch =  700] train_perplexity =    61.02, train_loss =  4.11, learning_rate = 0.31250000


[batch =  800] train_perplexity =    56.38, train_loss =  4.03, learning_rate = 0.31250000


[batch =  900] train_perplexity =    60.84, train_loss =  4.11, learning_rate = 0.31250000


[batch = 1000] train_perplexity =    64.11, train_loss =  4.16, learning_rate = 0.31250000


[batch = 1100] train_perplexity =    54.84, train_loss =  4.00, learning_rate = 0.31250000


[batch = 1200] train_perplexity =    55.75, train_loss =  4.02, learning_rate = 0.31250000


[batch = 1300] train_perplexity =    56.05, train_loss =  4.03, learning_rate = 0.31250000


[epoch =  29] validation_perplexity =    96.04, validation_loss =  4.56

Saved model to 'e200/ptb_word_gru.pt'...
[epoch =  29] min_validation_perplexity =    96.04, min_validation_loss =  4.56


[batch =  100] train_perplexity =    60.76, train_loss =  4.11, learning_rate = 0.31250000


[batch =  200] train_perplexity =    64.97, train_loss =  4.17, learning_rate = 0.31250000


[batch =  300] train_perplexity =    65.25, train_loss =  4.18, learning_rate = 0.31250000


[batch =  400] train_perplexity =    59.57, train_loss =  4.09, learning_rate = 0.31250000


[batch =  500] train_perplexity =    59.44, train_loss =  4.08, learning_rate = 0.31250000


[batch =  600] train_perplexity =    63.34, train_loss =  4.15, learning_rate = 0.31250000


[batch =  700] train_perplexity =    60.80, train_loss =  4.11, learning_rate = 0.31250000


[batch =  800] train_perplexity =    56.26, train_loss =  4.03, learning_rate = 0.31250000


[batch =  900] train_perplexity =    61.12, train_loss =  4.11, learning_rate = 0.31250000


[batch = 1000] train_perplexity =    64.18, train_loss =  4.16, learning_rate = 0.31250000


[batch = 1100] train_perplexity =    54.70, train_loss =  4.00, learning_rate = 0.31250000


[batch = 1200] train_perplexity =    55.53, train_loss =  4.02, learning_rate = 0.31250000


[batch = 1300] train_perplexity =    55.82, train_loss =  4.02, learning_rate = 0.31250000


[epoch =  30] validation_perplexity =    96.01, validation_loss =  4.56

Saved model to 'e200/ptb_word_gru.pt'...
[epoch =  30] min_validation_perplexity =    96.01, min_validation_loss =  4.56


[batch =  100] train_perplexity =    60.58, train_loss =  4.10, learning_rate = 0.31250000


[batch =  200] train_perplexity =    64.72, train_loss =  4.17, learning_rate = 0.31250000


[batch =  300] train_perplexity =    64.98, train_loss =  4.17, learning_rate = 0.31250000


[batch =  400] train_perplexity =    59.46, train_loss =  4.09, learning_rate = 0.31250000


[batch =  500] train_perplexity =    59.18, train_loss =  4.08, learning_rate = 0.31250000


[batch =  600] train_perplexity =    63.25, train_loss =  4.15, learning_rate = 0.31250000


[batch =  700] train_perplexity =    60.85, train_loss =  4.11, learning_rate = 0.31250000


[batch =  800] train_perplexity =    55.84, train_loss =  4.02, learning_rate = 0.31250000


[batch =  900] train_perplexity =    60.92, train_loss =  4.11, learning_rate = 0.31250000


[batch = 1000] train_perplexity =    64.03, train_loss =  4.16, learning_rate = 0.31250000


[batch = 1100] train_perplexity =    54.61, train_loss =  4.00, learning_rate = 0.31250000


[batch = 1200] train_perplexity =    55.36, train_loss =  4.01, learning_rate = 0.31250000


[batch = 1300] train_perplexity =    55.72, train_loss =  4.02, learning_rate = 0.31250000


[epoch =  31] validation_perplexity =    95.95, validation_loss =  4.56

Saved model to 'e200/ptb_word_gru.pt'...
[epoch =  31] min_validation_perplexity =    95.95, min_validation_loss =  4.56


[batch =  100] train_perplexity =    60.69, train_loss =  4.11, learning_rate = 0.31250000


[batch =  200] train_perplexity =    64.88, train_loss =  4.17, learning_rate = 0.31250000


[batch =  300] train_perplexity =    64.71, train_loss =  4.17, learning_rate = 0.31250000


[batch =  400] train_perplexity =    59.30, train_loss =  4.08, learning_rate = 0.31250000


[batch =  500] train_perplexity =    59.04, train_loss =  4.08, learning_rate = 0.31250000


[batch =  600] train_perplexity =    63.05, train_loss =  4.14, learning_rate = 0.31250000


[batch =  700] train_perplexity =    60.75, train_loss =  4.11, learning_rate = 0.31250000


[batch =  800] train_perplexity =    56.01, train_loss =  4.03, learning_rate = 0.31250000


[batch =  900] train_perplexity =    60.75, train_loss =  4.11, learning_rate = 0.31250000


[batch = 1000] train_perplexity =    63.73, train_loss =  4.15, learning_rate = 0.31250000


[batch = 1100] train_perplexity =    54.39, train_loss =  4.00, learning_rate = 0.31250000


[batch = 1200] train_perplexity =    55.29, train_loss =  4.01, learning_rate = 0.31250000


[batch = 1300] train_perplexity =    55.45, train_loss =  4.02, learning_rate = 0.31250000


[epoch =  32] validation_perplexity =    95.92, validation_loss =  4.56

Saved model to 'e200/ptb_word_gru.pt'...
[epoch =  32] min_validation_perplexity =    95.92, min_validation_loss =  4.56


[batch =  100] train_perplexity =    60.53, train_loss =  4.10, learning_rate = 0.31250000


[batch =  200] train_perplexity =    64.77, train_loss =  4.17, learning_rate = 0.31250000


[batch =  300] train_perplexity =    64.54, train_loss =  4.17, learning_rate = 0.31250000


[batch =  400] train_perplexity =    59.32, train_loss =  4.08, learning_rate = 0.31250000


[batch =  500] train_perplexity =    59.22, train_loss =  4.08, learning_rate = 0.31250000


[batch =  600] train_perplexity =    62.96, train_loss =  4.14, learning_rate = 0.31250000


[batch =  700] train_perplexity =    60.91, train_loss =  4.11, learning_rate = 0.31250000


[batch =  800] train_perplexity =    55.56, train_loss =  4.02, learning_rate = 0.31250000


[batch =  900] train_perplexity =    60.85, train_loss =  4.11, learning_rate = 0.31250000


[batch = 1000] train_perplexity =    63.97, train_loss =  4.16, learning_rate = 0.31250000


[batch = 1100] train_perplexity =    54.28, train_loss =  3.99, learning_rate = 0.31250000


[batch = 1200] train_perplexity =    55.14, train_loss =  4.01, learning_rate = 0.31250000


[batch = 1300] train_perplexity =    55.70, train_loss =  4.02, learning_rate = 0.31250000


[epoch =  33] validation_perplexity =    95.89, validation_loss =  4.56

Saved model to 'e200/ptb_word_gru.pt'...
[epoch =  33] min_validation_perplexity =    95.89, min_validation_loss =  4.56


[batch =  100] train_perplexity =    60.24, train_loss =  4.10, learning_rate = 0.31250000


[batch =  200] train_perplexity =    64.52, train_loss =  4.17, learning_rate = 0.31250000


[batch =  300] train_perplexity =    64.40, train_loss =  4.17, learning_rate = 0.31250000


[batch =  400] train_perplexity =    59.26, train_loss =  4.08, learning_rate = 0.31250000


[batch =  500] train_perplexity =    58.92, train_loss =  4.08, learning_rate = 0.31250000


[batch =  600] train_perplexity =    62.57, train_loss =  4.14, learning_rate = 0.31250000


[batch =  700] train_perplexity =    60.33, train_loss =  4.10, learning_rate = 0.31250000


[batch =  800] train_perplexity =    55.62, train_loss =  4.02, learning_rate = 0.31250000


[batch =  900] train_perplexity =    60.90, train_loss =  4.11, learning_rate = 0.31250000


[batch = 1000] train_perplexity =    63.57, train_loss =  4.15, learning_rate = 0.31250000


[batch = 1100] train_perplexity =    54.33, train_loss =  4.00, learning_rate = 0.31250000


[batch = 1200] train_perplexity =    54.78, train_loss =  4.00, learning_rate = 0.31250000


[batch = 1300] train_perplexity =    55.56, train_loss =  4.02, learning_rate = 0.31250000


[epoch =  34] validation_perplexity =    95.90, validation_loss =  4.56

[epoch =  34] annealing learning_rate = 0.07812500

[batch =  100] train_perplexity =    60.53, train_loss =  4.10, learning_rate = 0.07812500


[batch =  200] train_perplexity =    64.64, train_loss =  4.17, learning_rate = 0.07812500


[batch =  300] train_perplexity =    64.56, train_loss =  4.17, learning_rate = 0.07812500


[batch =  400] train_perplexity =    58.81, train_loss =  4.07, learning_rate = 0.07812500


[batch =  500] train_perplexity =    58.76, train_loss =  4.07, learning_rate = 0.07812500


[batch =  600] train_perplexity =    62.46, train_loss =  4.13, learning_rate = 0.07812500


[batch =  700] train_perplexity =    60.41, train_loss =  4.10, learning_rate = 0.07812500


[batch =  800] train_perplexity =    55.21, train_loss =  4.01, learning_rate = 0.07812500


[batch =  900] train_perplexity =    60.25, train_loss =  4.10, learning_rate = 0.07812500


[batch = 1000] train_perplexity =    63.35, train_loss =  4.15, learning_rate = 0.07812500


[batch = 1100] train_perplexity =    53.88, train_loss =  3.99, learning_rate = 0.07812500


[batch = 1200] train_perplexity =    54.60, train_loss =  4.00, learning_rate = 0.07812500


[batch = 1300] train_perplexity =    54.80, train_loss =  4.00, learning_rate = 0.07812500


[epoch =  35] validation_perplexity =    95.73, validation_loss =  4.56

Saved model to 'e200/ptb_word_gru.pt'...
[epoch =  35] min_validation_perplexity =    95.73, min_validation_loss =  4.56


[batch =  100] train_perplexity =    60.30, train_loss =  4.10, learning_rate = 0.07812500


[batch =  200] train_perplexity =    64.20, train_loss =  4.16, learning_rate = 0.07812500


[batch =  300] train_perplexity =    64.30, train_loss =  4.16, learning_rate = 0.07812500


[batch =  400] train_perplexity =    58.80, train_loss =  4.07, learning_rate = 0.07812500


[batch =  500] train_perplexity =    58.67, train_loss =  4.07, learning_rate = 0.07812500


[batch =  600] train_perplexity =    62.32, train_loss =  4.13, learning_rate = 0.07812500


[batch =  700] train_perplexity =    60.41, train_loss =  4.10, learning_rate = 0.07812500


[batch =  800] train_perplexity =    55.31, train_loss =  4.01, learning_rate = 0.07812500


[batch =  900] train_perplexity =    60.42, train_loss =  4.10, learning_rate = 0.07812500


[batch = 1000] train_perplexity =    63.20, train_loss =  4.15, learning_rate = 0.07812500


[batch = 1100] train_perplexity =    54.07, train_loss =  3.99, learning_rate = 0.07812500


[batch = 1200] train_perplexity =    54.74, train_loss =  4.00, learning_rate = 0.07812500


[batch = 1300] train_perplexity =    55.02, train_loss =  4.01, learning_rate = 0.07812500


[epoch =  36] validation_perplexity =    95.66, validation_loss =  4.56

Saved model to 'e200/ptb_word_gru.pt'...
[epoch =  36] min_validation_perplexity =    95.66, min_validation_loss =  4.56


[batch =  100] train_perplexity =    60.06, train_loss =  4.10, learning_rate = 0.07812500


[batch =  200] train_perplexity =    64.33, train_loss =  4.16, learning_rate = 0.07812500


[batch =  300] train_perplexity =    64.50, train_loss =  4.17, learning_rate = 0.07812500


[batch =  400] train_perplexity =    58.90, train_loss =  4.08, learning_rate = 0.07812500


[batch =  500] train_perplexity =    58.68, train_loss =  4.07, learning_rate = 0.07812500


[batch =  600] train_perplexity =    62.44, train_loss =  4.13, learning_rate = 0.07812500


[batch =  700] train_perplexity =    60.31, train_loss =  4.10, learning_rate = 0.07812500


[batch =  800] train_perplexity =    55.36, train_loss =  4.01, learning_rate = 0.07812500


[batch =  900] train_perplexity =    60.41, train_loss =  4.10, learning_rate = 0.07812500


[batch = 1000] train_perplexity =    63.43, train_loss =  4.15, learning_rate = 0.07812500


[batch = 1100] train_perplexity =    53.99, train_loss =  3.99, learning_rate = 0.07812500


[batch = 1200] train_perplexity =    54.50, train_loss =  4.00, learning_rate = 0.07812500


[batch = 1300] train_perplexity =    54.87, train_loss =  4.00, learning_rate = 0.07812500


[epoch =  37] validation_perplexity =    95.65, validation_loss =  4.56

Saved model to 'e200/ptb_word_gru.pt'...
[epoch =  37] min_validation_perplexity =    95.65, min_validation_loss =  4.56


[batch =  100] train_perplexity =    60.14, train_loss =  4.10, learning_rate = 0.07812500


[batch =  200] train_perplexity =    64.02, train_loss =  4.16, learning_rate = 0.07812500


[batch =  300] train_perplexity =    64.08, train_loss =  4.16, learning_rate = 0.07812500


[batch =  400] train_perplexity =    58.71, train_loss =  4.07, learning_rate = 0.07812500


[batch =  500] train_perplexity =    58.70, train_loss =  4.07, learning_rate = 0.07812500


[batch =  600] train_perplexity =    62.56, train_loss =  4.14, learning_rate = 0.07812500


[batch =  700] train_perplexity =    59.99, train_loss =  4.09, learning_rate = 0.07812500


[batch =  800] train_perplexity =    55.24, train_loss =  4.01, learning_rate = 0.07812500


[batch =  900] train_perplexity =    60.42, train_loss =  4.10, learning_rate = 0.07812500


[batch = 1000] train_perplexity =    63.00, train_loss =  4.14, learning_rate = 0.07812500


[batch = 1100] train_perplexity =    53.68, train_loss =  3.98, learning_rate = 0.07812500


[batch = 1200] train_perplexity =    54.67, train_loss =  4.00, learning_rate = 0.07812500


[batch = 1300] train_perplexity =    55.33, train_loss =  4.01, learning_rate = 0.07812500


[epoch =  38] validation_perplexity =    95.63, validation_loss =  4.56

Saved model to 'e200/ptb_word_gru.pt'...
[epoch =  38] min_validation_perplexity =    95.63, min_validation_loss =  4.56


[batch =  100] train_perplexity =    60.06, train_loss =  4.10, learning_rate = 0.07812500


[batch =  200] train_perplexity =    64.43, train_loss =  4.17, learning_rate = 0.07812500


[batch =  300] train_perplexity =    63.97, train_loss =  4.16, learning_rate = 0.07812500


[batch =  400] train_perplexity =    58.89, train_loss =  4.08, learning_rate = 0.07812500


[batch =  500] train_perplexity =    58.34, train_loss =  4.07, learning_rate = 0.07812500


[batch =  600] train_perplexity =    62.20, train_loss =  4.13, learning_rate = 0.07812500


[batch =  700] train_perplexity =    60.27, train_loss =  4.10, learning_rate = 0.07812500


[batch =  800] train_perplexity =    55.08, train_loss =  4.01, learning_rate = 0.07812500


[batch =  900] train_perplexity =    60.70, train_loss =  4.11, learning_rate = 0.07812500


[batch = 1000] train_perplexity =    62.90, train_loss =  4.14, learning_rate = 0.07812500


[batch = 1100] train_perplexity =    53.79, train_loss =  3.99, learning_rate = 0.07812500


[batch = 1200] train_perplexity =    54.62, train_loss =  4.00, learning_rate = 0.07812500


[batch = 1300] train_perplexity =    55.03, train_loss =  4.01, learning_rate = 0.07812500


[epoch =  39] validation_perplexity =    95.62, validation_loss =  4.56

Saved model to 'e200/ptb_word_gru.pt'...
[epoch =  39] min_validation_perplexity =    95.62, min_validation_loss =  4.56


[batch =  100] train_perplexity =    59.75, train_loss =  4.09, learning_rate = 0.07812500


[batch =  200] train_perplexity =    64.19, train_loss =  4.16, learning_rate = 0.07812500


[batch =  300] train_perplexity =    63.98, train_loss =  4.16, learning_rate = 0.07812500


[batch =  400] train_perplexity =    58.58, train_loss =  4.07, learning_rate = 0.07812500


[batch =  500] train_perplexity =    58.50, train_loss =  4.07, learning_rate = 0.07812500


[batch =  600] train_perplexity =    62.33, train_loss =  4.13, learning_rate = 0.07812500


[batch =  700] train_perplexity =    60.46, train_loss =  4.10, learning_rate = 0.07812500


[batch =  800] train_perplexity =    55.27, train_loss =  4.01, learning_rate = 0.07812500


[batch =  900] train_perplexity =    60.16, train_loss =  4.10, learning_rate = 0.07812500


[batch = 1000] train_perplexity =    63.10, train_loss =  4.14, learning_rate = 0.07812500


[batch = 1100] train_perplexity =    53.96, train_loss =  3.99, learning_rate = 0.07812500


[batch = 1200] train_perplexity =    54.76, train_loss =  4.00, learning_rate = 0.07812500


[batch = 1300] train_perplexity =    55.32, train_loss =  4.01, learning_rate = 0.07812500


[epoch =  40] validation_perplexity =    95.60, validation_loss =  4.56

Saved model to 'e200/ptb_word_gru.pt'...
[epoch =  40] min_validation_perplexity =    95.60, min_validation_loss =  4.56


[batch =  100] train_perplexity =    60.03, train_loss =  4.09, learning_rate = 0.07812500


[batch =  200] train_perplexity =    64.08, train_loss =  4.16, learning_rate = 0.07812500


[batch =  300] train_perplexity =    64.13, train_loss =  4.16, learning_rate = 0.07812500


[batch =  400] train_perplexity =    58.77, train_loss =  4.07, learning_rate = 0.07812500


[batch =  500] train_perplexity =    58.66, train_loss =  4.07, learning_rate = 0.07812500


[batch =  600] train_perplexity =    61.97, train_loss =  4.13, learning_rate = 0.07812500


[batch =  700] train_perplexity =    60.18, train_loss =  4.10, learning_rate = 0.07812500


[batch =  800] train_perplexity =    55.20, train_loss =  4.01, learning_rate = 0.07812500


[batch =  900] train_perplexity =    60.34, train_loss =  4.10, learning_rate = 0.07812500


[batch = 1000] train_perplexity =    63.16, train_loss =  4.15, learning_rate = 0.07812500


[batch = 1100] train_perplexity =    54.06, train_loss =  3.99, learning_rate = 0.07812500


[batch = 1200] train_perplexity =    54.76, train_loss =  4.00, learning_rate = 0.07812500


[batch = 1300] train_perplexity =    54.89, train_loss =  4.01, learning_rate = 0.07812500


[epoch =  41] validation_perplexity =    95.58, validation_loss =  4.56

Saved model to 'e200/ptb_word_gru.pt'...
[epoch =  41] min_validation_perplexity =    95.58, min_validation_loss =  4.56


[batch =  100] train_perplexity =    59.99, train_loss =  4.09, learning_rate = 0.07812500


[batch =  200] train_perplexity =    63.79, train_loss =  4.16, learning_rate = 0.07812500


[batch =  300] train_perplexity =    64.12, train_loss =  4.16, learning_rate = 0.07812500


[batch =  400] train_perplexity =    58.62, train_loss =  4.07, learning_rate = 0.07812500


[batch =  500] train_perplexity =    58.25, train_loss =  4.06, learning_rate = 0.07812500


[batch =  600] train_perplexity =    62.25, train_loss =  4.13, learning_rate = 0.07812500


[batch =  700] train_perplexity =    60.09, train_loss =  4.10, learning_rate = 0.07812500


[batch =  800] train_perplexity =    55.23, train_loss =  4.01, learning_rate = 0.07812500


[batch =  900] train_perplexity =    60.02, train_loss =  4.09, learning_rate = 0.07812500


[batch = 1000] train_perplexity =    63.18, train_loss =  4.15, learning_rate = 0.07812500


[batch = 1100] train_perplexity =    53.84, train_loss =  3.99, learning_rate = 0.07812500


[batch = 1200] train_perplexity =    54.54, train_loss =  4.00, learning_rate = 0.07812500


[batch = 1300] train_perplexity =    54.75, train_loss =  4.00, learning_rate = 0.07812500


[epoch =  42] validation_perplexity =    95.60, validation_loss =  4.56

[epoch =  42] annealing learning_rate = 0.01953125

[batch =  100] train_perplexity =    59.91, train_loss =  4.09, learning_rate = 0.01953125


[batch =  200] train_perplexity =    64.13, train_loss =  4.16, learning_rate = 0.01953125


[batch =  300] train_perplexity =    63.93, train_loss =  4.16, learning_rate = 0.01953125


[batch =  400] train_perplexity =    58.43, train_loss =  4.07, learning_rate = 0.01953125


[batch =  500] train_perplexity =    58.34, train_loss =  4.07, learning_rate = 0.01953125


[batch =  600] train_perplexity =    62.41, train_loss =  4.13, learning_rate = 0.01953125


[batch =  700] train_perplexity =    60.27, train_loss =  4.10, learning_rate = 0.01953125


[batch =  800] train_perplexity =    55.12, train_loss =  4.01, learning_rate = 0.01953125


[batch =  900] train_perplexity =    60.30, train_loss =  4.10, learning_rate = 0.01953125


[batch = 1000] train_perplexity =    62.62, train_loss =  4.14, learning_rate = 0.01953125


[batch = 1100] train_perplexity =    53.87, train_loss =  3.99, learning_rate = 0.01953125


[batch = 1200] train_perplexity =    54.29, train_loss =  3.99, learning_rate = 0.01953125


[batch = 1300] train_perplexity =    54.89, train_loss =  4.01, learning_rate = 0.01953125


[epoch =  43] validation_perplexity =    95.57, validation_loss =  4.56

Saved model to 'e200/ptb_word_gru.pt'...
[epoch =  43] min_validation_perplexity =    95.57, min_validation_loss =  4.56


[batch =  100] train_perplexity =    59.75, train_loss =  4.09, learning_rate = 0.01953125


[batch =  200] train_perplexity =    64.11, train_loss =  4.16, learning_rate = 0.01953125


[batch =  300] train_perplexity =    63.88, train_loss =  4.16, learning_rate = 0.01953125


[batch =  400] train_perplexity =    58.47, train_loss =  4.07, learning_rate = 0.01953125


[batch =  500] train_perplexity =    58.31, train_loss =  4.07, learning_rate = 0.01953125


[batch =  600] train_perplexity =    62.05, train_loss =  4.13, learning_rate = 0.01953125


[batch =  700] train_perplexity =    59.73, train_loss =  4.09, learning_rate = 0.01953125


[batch =  800] train_perplexity =    55.21, train_loss =  4.01, learning_rate = 0.01953125


[batch =  900] train_perplexity =    60.04, train_loss =  4.10, learning_rate = 0.01953125


[batch = 1000] train_perplexity =    62.89, train_loss =  4.14, learning_rate = 0.01953125


[batch = 1100] train_perplexity =    53.83, train_loss =  3.99, learning_rate = 0.01953125


[batch = 1200] train_perplexity =    54.60, train_loss =  4.00, learning_rate = 0.01953125


[batch = 1300] train_perplexity =    54.78, train_loss =  4.00, learning_rate = 0.01953125


[epoch =  44] validation_perplexity =    95.57, validation_loss =  4.56

Saved model to 'e200/ptb_word_gru.pt'...
[epoch =  44] min_validation_perplexity =    95.57, min_validation_loss =  4.56


[batch =  100] train_perplexity =    59.90, train_loss =  4.09, learning_rate = 0.01953125


[batch =  200] train_perplexity =    64.20, train_loss =  4.16, learning_rate = 0.01953125


[batch =  300] train_perplexity =    63.87, train_loss =  4.16, learning_rate = 0.01953125


[batch =  400] train_perplexity =    58.60, train_loss =  4.07, learning_rate = 0.01953125


[batch =  500] train_perplexity =    58.41, train_loss =  4.07, learning_rate = 0.01953125


[batch =  600] train_perplexity =    62.17, train_loss =  4.13, learning_rate = 0.01953125


[batch =  700] train_perplexity =    60.04, train_loss =  4.10, learning_rate = 0.01953125


[batch =  800] train_perplexity =    54.76, train_loss =  4.00, learning_rate = 0.01953125


[batch =  900] train_perplexity =    59.90, train_loss =  4.09, learning_rate = 0.01953125


[batch = 1000] train_perplexity =    62.84, train_loss =  4.14, learning_rate = 0.01953125


[batch = 1100] train_perplexity =    53.60, train_loss =  3.98, learning_rate = 0.01953125


[batch = 1200] train_perplexity =    54.50, train_loss =  4.00, learning_rate = 0.01953125


[batch = 1300] train_perplexity =    54.74, train_loss =  4.00, learning_rate = 0.01953125


[epoch =  45] validation_perplexity =    95.56, validation_loss =  4.56

Saved model to 'e200/ptb_word_gru.pt'...
[epoch =  45] min_validation_perplexity =    95.56, min_validation_loss =  4.56


[batch =  100] train_perplexity =    59.78, train_loss =  4.09, learning_rate = 0.01953125


[batch =  200] train_perplexity =    63.79, train_loss =  4.16, learning_rate = 0.01953125


[batch =  300] train_perplexity =    63.60, train_loss =  4.15, learning_rate = 0.01953125


[batch =  400] train_perplexity =    58.75, train_loss =  4.07, learning_rate = 0.01953125


[batch =  500] train_perplexity =    58.13, train_loss =  4.06, learning_rate = 0.01953125


[batch =  600] train_perplexity =    62.07, train_loss =  4.13, learning_rate = 0.01953125


[batch =  700] train_perplexity =    59.64, train_loss =  4.09, learning_rate = 0.01953125


[batch =  800] train_perplexity =    54.98, train_loss =  4.01, learning_rate = 0.01953125


[batch =  900] train_perplexity =    60.26, train_loss =  4.10, learning_rate = 0.01953125


[batch = 1000] train_perplexity =    62.86, train_loss =  4.14, learning_rate = 0.01953125


[batch = 1100] train_perplexity =    53.81, train_loss =  3.99, learning_rate = 0.01953125


[batch = 1200] train_perplexity =    54.17, train_loss =  3.99, learning_rate = 0.01953125


[batch = 1300] train_perplexity =    55.18, train_loss =  4.01, learning_rate = 0.01953125


[epoch =  46] validation_perplexity =    95.56, validation_loss =  4.56

[epoch =  46] annealing learning_rate = 0.00488281

[batch =  100] train_perplexity =    59.72, train_loss =  4.09, learning_rate = 0.00488281


[batch =  200] train_perplexity =    64.08, train_loss =  4.16, learning_rate = 0.00488281


[batch =  300] train_perplexity =    63.94, train_loss =  4.16, learning_rate = 0.00488281


[batch =  400] train_perplexity =    58.80, train_loss =  4.07, learning_rate = 0.00488281


[batch =  500] train_perplexity =    58.19, train_loss =  4.06, learning_rate = 0.00488281


[batch =  600] train_perplexity =    62.13, train_loss =  4.13, learning_rate = 0.00488281


[batch =  700] train_perplexity =    59.93, train_loss =  4.09, learning_rate = 0.00488281


[batch =  800] train_perplexity =    55.16, train_loss =  4.01, learning_rate = 0.00488281


[batch =  900] train_perplexity =    60.24, train_loss =  4.10, learning_rate = 0.00488281


[batch = 1000] train_perplexity =    62.81, train_loss =  4.14, learning_rate = 0.00488281


[batch = 1100] train_perplexity =    53.64, train_loss =  3.98, learning_rate = 0.00488281


[batch = 1200] train_perplexity =    54.05, train_loss =  3.99, learning_rate = 0.00488281


[batch = 1300] train_perplexity =    55.03, train_loss =  4.01, learning_rate = 0.00488281


[epoch =  47] validation_perplexity =    95.56, validation_loss =  4.56

[epoch =  47] annealing learning_rate = 0.00122070

[batch =  100] train_perplexity =    59.96, train_loss =  4.09, learning_rate = 0.00122070


[batch =  200] train_perplexity =    64.45, train_loss =  4.17, learning_rate = 0.00122070


[batch =  300] train_perplexity =    63.90, train_loss =  4.16, learning_rate = 0.00122070


[batch =  400] train_perplexity =    58.18, train_loss =  4.06, learning_rate = 0.00122070


[batch =  500] train_perplexity =    58.23, train_loss =  4.06, learning_rate = 0.00122070


[batch =  600] train_perplexity =    61.96, train_loss =  4.13, learning_rate = 0.00122070


[batch =  700] train_perplexity =    60.21, train_loss =  4.10, learning_rate = 0.00122070


[batch =  800] train_perplexity =    54.80, train_loss =  4.00, learning_rate = 0.00122070


[batch =  900] train_perplexity =    60.44, train_loss =  4.10, learning_rate = 0.00122070


[batch = 1000] train_perplexity =    62.99, train_loss =  4.14, learning_rate = 0.00122070


[batch = 1100] train_perplexity =    53.76, train_loss =  3.98, learning_rate = 0.00122070


[batch = 1200] train_perplexity =    54.42, train_loss =  4.00, learning_rate = 0.00122070


[batch = 1300] train_perplexity =    55.13, train_loss =  4.01, learning_rate = 0.00122070


[epoch =  48] validation_perplexity =    95.56, validation_loss =  4.56

[epoch =  48] annealing learning_rate = 0.00030518

[batch =  100] train_perplexity =    59.62, train_loss =  4.09, learning_rate = 0.00030518


[batch =  200] train_perplexity =    64.21, train_loss =  4.16, learning_rate = 0.00030518


[batch =  300] train_perplexity =    64.14, train_loss =  4.16, learning_rate = 0.00030518


[batch =  400] train_perplexity =    58.79, train_loss =  4.07, learning_rate = 0.00030518


[batch =  500] train_perplexity =    58.33, train_loss =  4.07, learning_rate = 0.00030518


[batch =  600] train_perplexity =    62.36, train_loss =  4.13, learning_rate = 0.00030518


[batch =  700] train_perplexity =    59.77, train_loss =  4.09, learning_rate = 0.00030518


[batch =  800] train_perplexity =    54.95, train_loss =  4.01, learning_rate = 0.00030518


[batch =  900] train_perplexity =    60.01, train_loss =  4.09, learning_rate = 0.00030518


[batch = 1000] train_perplexity =    62.72, train_loss =  4.14, learning_rate = 0.00030518


[batch = 1100] train_perplexity =    53.90, train_loss =  3.99, learning_rate = 0.00030518


[batch = 1200] train_perplexity =    54.34, train_loss =  4.00, learning_rate = 0.00030518


[batch = 1300] train_perplexity =    55.02, train_loss =  4.01, learning_rate = 0.00030518


[epoch =  49] validation_perplexity =    95.56, validation_loss =  4.56

[epoch =  49] annealing learning_rate = 0.00007629

[batch =  100] train_perplexity =    59.86, train_loss =  4.09, learning_rate = 0.00007629


[batch =  200] train_perplexity =    64.11, train_loss =  4.16, learning_rate = 0.00007629


[batch =  300] train_perplexity =    63.73, train_loss =  4.15, learning_rate = 0.00007629


[batch =  400] train_perplexity =    58.62, train_loss =  4.07, learning_rate = 0.00007629


[batch =  500] train_perplexity =    58.22, train_loss =  4.06, learning_rate = 0.00007629


[batch =  600] train_perplexity =    61.98, train_loss =  4.13, learning_rate = 0.00007629


[batch =  700] train_perplexity =    59.81, train_loss =  4.09, learning_rate = 0.00007629


[batch =  800] train_perplexity =    55.21, train_loss =  4.01, learning_rate = 0.00007629


[batch =  900] train_perplexity =    59.83, train_loss =  4.09, learning_rate = 0.00007629


[batch = 1000] train_perplexity =    62.81, train_loss =  4.14, learning_rate = 0.00007629


[batch = 1100] train_perplexity =    53.39, train_loss =  3.98, learning_rate = 0.00007629


[batch = 1200] train_perplexity =    54.50, train_loss =  4.00, learning_rate = 0.00007629


[batch = 1300] train_perplexity =    54.91, train_loss =  4.01, learning_rate = 0.00007629


[epoch =  50] validation_perplexity =    95.56, validation_loss =  4.56

[epoch =  50] annealing learning_rate = 0.00001907

[batch =  100] train_perplexity =    59.88, train_loss =  4.09, learning_rate = 0.00001907


[batch =  200] train_perplexity =    63.88, train_loss =  4.16, learning_rate = 0.00001907


[batch =  300] train_perplexity =    63.90, train_loss =  4.16, learning_rate = 0.00001907


[batch =  400] train_perplexity =    58.70, train_loss =  4.07, learning_rate = 0.00001907


[batch =  500] train_perplexity =    58.21, train_loss =  4.06, learning_rate = 0.00001907


[batch =  600] train_perplexity =    62.05, train_loss =  4.13, learning_rate = 0.00001907


[batch =  700] train_perplexity =    59.87, train_loss =  4.09, learning_rate = 0.00001907


[batch =  800] train_perplexity =    55.09, train_loss =  4.01, learning_rate = 0.00001907


[batch =  900] train_perplexity =    59.99, train_loss =  4.09, learning_rate = 0.00001907


[batch = 1000] train_perplexity =    62.79, train_loss =  4.14, learning_rate = 0.00001907


[batch = 1100] train_perplexity =    53.79, train_loss =  3.99, learning_rate = 0.00001907


[batch = 1200] train_perplexity =    54.56, train_loss =  4.00, learning_rate = 0.00001907


[batch = 1300] train_perplexity =    54.80, train_loss =  4.00, learning_rate = 0.00001907


[epoch =  51] validation_perplexity =    95.56, validation_loss =  4.56

[epoch =  51] annealing learning_rate = 0.00000477

[batch =  100] train_perplexity =    59.55, train_loss =  4.09, learning_rate = 0.00000477


[batch =  200] train_perplexity =    64.15, train_loss =  4.16, learning_rate = 0.00000477


[batch =  300] train_perplexity =    63.79, train_loss =  4.16, learning_rate = 0.00000477


[batch =  400] train_perplexity =    58.84, train_loss =  4.07, learning_rate = 0.00000477


[batch =  500] train_perplexity =    58.61, train_loss =  4.07, learning_rate = 0.00000477


[batch =  600] train_perplexity =    62.07, train_loss =  4.13, learning_rate = 0.00000477


[batch =  700] train_perplexity =    59.63, train_loss =  4.09, learning_rate = 0.00000477


[batch =  800] train_perplexity =    55.04, train_loss =  4.01, learning_rate = 0.00000477


[batch =  900] train_perplexity =    59.91, train_loss =  4.09, learning_rate = 0.00000477


[batch = 1000] train_perplexity =    62.93, train_loss =  4.14, learning_rate = 0.00000477


[batch = 1100] train_perplexity =    53.68, train_loss =  3.98, learning_rate = 0.00000477


[batch = 1200] train_perplexity =    54.32, train_loss =  3.99, learning_rate = 0.00000477


[batch = 1300] train_perplexity =    54.75, train_loss =  4.00, learning_rate = 0.00000477


[epoch =  52] validation_perplexity =    95.56, validation_loss =  4.56

[epoch =  52] annealing learning_rate = 0.00000119

[batch =  100] train_perplexity =    59.94, train_loss =  4.09, learning_rate = 0.00000119


[batch =  200] train_perplexity =    64.52, train_loss =  4.17, learning_rate = 0.00000119


[batch =  300] train_perplexity =    63.78, train_loss =  4.16, learning_rate = 0.00000119


[batch =  400] train_perplexity =    58.64, train_loss =  4.07, learning_rate = 0.00000119


[batch =  500] train_perplexity =    57.93, train_loss =  4.06, learning_rate = 0.00000119


[batch =  600] train_perplexity =    62.11, train_loss =  4.13, learning_rate = 0.00000119


[batch =  700] train_perplexity =    59.86, train_loss =  4.09, learning_rate = 0.00000119


[batch =  800] train_perplexity =    55.26, train_loss =  4.01, learning_rate = 0.00000119


[batch =  900] train_perplexity =    60.06, train_loss =  4.10, learning_rate = 0.00000119


[batch = 1000] train_perplexity =    63.12, train_loss =  4.15, learning_rate = 0.00000119


[batch = 1100] train_perplexity =    53.63, train_loss =  3.98, learning_rate = 0.00000119


[batch = 1200] train_perplexity =    54.35, train_loss =  4.00, learning_rate = 0.00000119


[batch = 1300] train_perplexity =    54.87, train_loss =  4.00, learning_rate = 0.00000119


[epoch =  53] validation_perplexity =    95.56, validation_loss =  4.56

[epoch =  53] annealing learning_rate = 0.00000030

[batch =  100] train_perplexity =    59.98, train_loss =  4.09, learning_rate = 0.00000030


[batch =  200] train_perplexity =    64.23, train_loss =  4.16, learning_rate = 0.00000030


[batch =  300] train_perplexity =    63.91, train_loss =  4.16, learning_rate = 0.00000030


[batch =  400] train_perplexity =    58.74, train_loss =  4.07, learning_rate = 0.00000030


[batch =  500] train_perplexity =    58.51, train_loss =  4.07, learning_rate = 0.00000030


[batch =  600] train_perplexity =    62.22, train_loss =  4.13, learning_rate = 0.00000030


[batch =  700] train_perplexity =    59.71, train_loss =  4.09, learning_rate = 0.00000030


[batch =  800] train_perplexity =    54.91, train_loss =  4.01, learning_rate = 0.00000030


[batch =  900] train_perplexity =    59.92, train_loss =  4.09, learning_rate = 0.00000030


[batch = 1000] train_perplexity =    63.16, train_loss =  4.15, learning_rate = 0.00000030


[batch = 1100] train_perplexity =    53.68, train_loss =  3.98, learning_rate = 0.00000030


[batch = 1200] train_perplexity =    54.38, train_loss =  4.00, learning_rate = 0.00000030


[batch = 1300] train_perplexity =    54.92, train_loss =  4.01, learning_rate = 0.00000030


[epoch =  54] validation_perplexity =    95.56, validation_loss =  4.56

[epoch =  54] annealing learning_rate = 0.00000007

[batch =  100] train_perplexity =    59.63, train_loss =  4.09, learning_rate = 0.00000007


[batch =  200] train_perplexity =    63.99, train_loss =  4.16, learning_rate = 0.00000007


[batch =  300] train_perplexity =    64.31, train_loss =  4.16, learning_rate = 0.00000007


[batch =  400] train_perplexity =    58.58, train_loss =  4.07, learning_rate = 0.00000007


[batch =  500] train_perplexity =    58.36, train_loss =  4.07, learning_rate = 0.00000007


[batch =  600] train_perplexity =    61.91, train_loss =  4.13, learning_rate = 0.00000007


[batch =  700] train_perplexity =    59.76, train_loss =  4.09, learning_rate = 0.00000007


[batch =  800] train_perplexity =    55.22, train_loss =  4.01, learning_rate = 0.00000007


[batch =  900] train_perplexity =    60.24, train_loss =  4.10, learning_rate = 0.00000007


[batch = 1000] train_perplexity =    62.75, train_loss =  4.14, learning_rate = 0.00000007


[batch = 1100] train_perplexity =    53.46, train_loss =  3.98, learning_rate = 0.00000007


[batch = 1200] train_perplexity =    54.31, train_loss =  3.99, learning_rate = 0.00000007


[batch = 1300] train_perplexity =    54.63, train_loss =  4.00, learning_rate = 0.00000007


[epoch =  55] validation_perplexity =    95.56, validation_loss =  4.56

[epoch =  55] annealing learning_rate = 0.00000002

[batch =  100] train_perplexity =    60.08, train_loss =  4.10, learning_rate = 0.00000002


[batch =  200] train_perplexity =    63.95, train_loss =  4.16, learning_rate = 0.00000002


[batch =  300] train_perplexity =    64.23, train_loss =  4.16, learning_rate = 0.00000002


[batch =  400] train_perplexity =    58.58, train_loss =  4.07, learning_rate = 0.00000002


[batch =  500] train_perplexity =    58.46, train_loss =  4.07, learning_rate = 0.00000002


[batch =  600] train_perplexity =    61.92, train_loss =  4.13, learning_rate = 0.00000002


[batch =  700] train_perplexity =    59.66, train_loss =  4.09, learning_rate = 0.00000002


[batch =  800] train_perplexity =    55.05, train_loss =  4.01, learning_rate = 0.00000002


[batch =  900] train_perplexity =    60.10, train_loss =  4.10, learning_rate = 0.00000002


[batch = 1000] train_perplexity =    62.77, train_loss =  4.14, learning_rate = 0.00000002


[batch = 1100] train_perplexity =    53.57, train_loss =  3.98, learning_rate = 0.00000002


[batch = 1200] train_perplexity =    54.37, train_loss =  4.00, learning_rate = 0.00000002


[batch = 1300] train_perplexity =    54.90, train_loss =  4.01, learning_rate = 0.00000002


[epoch =  56] validation_perplexity =    95.56, validation_loss =  4.56

[epoch =  56] annealing learning_rate = 0.00000000

[batch =  100] train_perplexity =    59.49, train_loss =  4.09, learning_rate = 0.00000000


[batch =  200] train_perplexity =    64.02, train_loss =  4.16, learning_rate = 0.00000000


[batch =  300] train_perplexity =    63.76, train_loss =  4.16, learning_rate = 0.00000000


[batch =  400] train_perplexity =    58.73, train_loss =  4.07, learning_rate = 0.00000000


[batch =  500] train_perplexity =    58.17, train_loss =  4.06, learning_rate = 0.00000000


[batch =  600] train_perplexity =    62.26, train_loss =  4.13, learning_rate = 0.00000000


[batch =  700] train_perplexity =    59.73, train_loss =  4.09, learning_rate = 0.00000000


[batch =  800] train_perplexity =    54.91, train_loss =  4.01, learning_rate = 0.00000000


[batch =  900] train_perplexity =    59.83, train_loss =  4.09, learning_rate = 0.00000000


[batch = 1000] train_perplexity =    62.89, train_loss =  4.14, learning_rate = 0.00000000


[batch = 1100] train_perplexity =    53.65, train_loss =  3.98, learning_rate = 0.00000000


[batch = 1200] train_perplexity =    54.29, train_loss =  3.99, learning_rate = 0.00000000


[batch = 1300] train_perplexity =    54.95, train_loss =  4.01, learning_rate = 0.00000000


[epoch =  57] validation_perplexity =    95.56, validation_loss =  4.56

[epoch =  57] annealing learning_rate = 0.00000000

[batch =  100] train_perplexity =    59.96, train_loss =  4.09, learning_rate = 0.00000000


[batch =  200] train_perplexity =    64.11, train_loss =  4.16, learning_rate = 0.00000000


[batch =  300] train_perplexity =    63.63, train_loss =  4.15, learning_rate = 0.00000000


[batch =  400] train_perplexity =    58.42, train_loss =  4.07, learning_rate = 0.00000000


[batch =  500] train_perplexity =    58.17, train_loss =  4.06, learning_rate = 0.00000000


[batch =  600] train_perplexity =    62.13, train_loss =  4.13, learning_rate = 0.00000000


[batch =  700] train_perplexity =    59.77, train_loss =  4.09, learning_rate = 0.00000000


[batch =  800] train_perplexity =    55.05, train_loss =  4.01, learning_rate = 0.00000000


[batch =  900] train_perplexity =    60.17, train_loss =  4.10, learning_rate = 0.00000000


[batch = 1000] train_perplexity =    62.81, train_loss =  4.14, learning_rate = 0.00000000


[batch = 1100] train_perplexity =    53.68, train_loss =  3.98, learning_rate = 0.00000000


[batch = 1200] train_perplexity =    54.29, train_loss =  3.99, learning_rate = 0.00000000


[batch = 1300] train_perplexity =    54.68, train_loss =  4.00, learning_rate = 0.00000000


[epoch =  58] validation_perplexity =    95.56, validation_loss =  4.56

[epoch =  58] annealing learning_rate = 0.00000000

[batch =  100] train_perplexity =    59.90, train_loss =  4.09, learning_rate = 0.00000000


[batch =  200] train_perplexity =    64.29, train_loss =  4.16, learning_rate = 0.00000000


[batch =  300] train_perplexity =    63.92, train_loss =  4.16, learning_rate = 0.00000000


[batch =  400] train_perplexity =    58.18, train_loss =  4.06, learning_rate = 0.00000000


[batch =  500] train_perplexity =    58.13, train_loss =  4.06, learning_rate = 0.00000000


[batch =  600] train_perplexity =    61.98, train_loss =  4.13, learning_rate = 0.00000000


[batch =  700] train_perplexity =    59.98, train_loss =  4.09, learning_rate = 0.00000000


[batch =  800] train_perplexity =    54.87, train_loss =  4.01, learning_rate = 0.00000000


[batch =  900] train_perplexity =    60.23, train_loss =  4.10, learning_rate = 0.00000000


[batch = 1000] train_perplexity =    62.67, train_loss =  4.14, learning_rate = 0.00000000


[batch = 1100] train_perplexity =    53.76, train_loss =  3.98, learning_rate = 0.00000000


[batch = 1200] train_perplexity =    54.66, train_loss =  4.00, learning_rate = 0.00000000


[batch = 1300] train_perplexity =    54.89, train_loss =  4.01, learning_rate = 0.00000000


[epoch =  59] validation_perplexity =    95.56, validation_loss =  4.56

[epoch =  59] annealing learning_rate = 0.00000000

[batch =  100] train_perplexity =    59.77, train_loss =  4.09, learning_rate = 0.00000000


[batch =  200] train_perplexity =    63.94, train_loss =  4.16, learning_rate = 0.00000000


[batch =  300] train_perplexity =    63.90, train_loss =  4.16, learning_rate = 0.00000000


[batch =  400] train_perplexity =    58.72, train_loss =  4.07, learning_rate = 0.00000000


[batch =  500] train_perplexity =    58.22, train_loss =  4.06, learning_rate = 0.00000000


[batch =  600] train_perplexity =    62.01, train_loss =  4.13, learning_rate = 0.00000000


[batch =  700] train_perplexity =    59.74, train_loss =  4.09, learning_rate = 0.00000000


[batch =  800] train_perplexity =    55.02, train_loss =  4.01, learning_rate = 0.00000000


[batch =  900] train_perplexity =    59.72, train_loss =  4.09, learning_rate = 0.00000000


[batch = 1000] train_perplexity =    63.15, train_loss =  4.15, learning_rate = 0.00000000


[batch = 1100] train_perplexity =    53.86, train_loss =  3.99, learning_rate = 0.00000000


[batch = 1200] train_perplexity =    54.34, train_loss =  4.00, learning_rate = 0.00000000


[batch = 1300] train_perplexity =    54.81, train_loss =  4.00, learning_rate = 0.00000000


[epoch =  60] validation_perplexity =    95.56, validation_loss =  4.56

[epoch =  60] annealing learning_rate = 0.00000000

[batch =  100] train_perplexity =    59.50, train_loss =  4.09, learning_rate = 0.00000000


[batch =  200] train_perplexity =    63.98, train_loss =  4.16, learning_rate = 0.00000000


[batch =  300] train_perplexity =    63.93, train_loss =  4.16, learning_rate = 0.00000000


[batch =  400] train_perplexity =    58.53, train_loss =  4.07, learning_rate = 0.00000000


[batch =  500] train_perplexity =    58.30, train_loss =  4.07, learning_rate = 0.00000000


[batch =  600] train_perplexity =    62.19, train_loss =  4.13, learning_rate = 0.00000000


[batch =  700] train_perplexity =    59.79, train_loss =  4.09, learning_rate = 0.00000000


[batch =  800] train_perplexity =    54.95, train_loss =  4.01, learning_rate = 0.00000000


[batch =  900] train_perplexity =    59.77, train_loss =  4.09, learning_rate = 0.00000000


[batch = 1000] train_perplexity =    63.00, train_loss =  4.14, learning_rate = 0.00000000


[batch = 1100] train_perplexity =    53.65, train_loss =  3.98, learning_rate = 0.00000000


[batch = 1200] train_perplexity =    54.50, train_loss =  4.00, learning_rate = 0.00000000


[batch = 1300] train_perplexity =    54.69, train_loss =  4.00, learning_rate = 0.00000000


[epoch =  61] validation_perplexity =    95.56, validation_loss =  4.56

[epoch =  61] annealing learning_rate = 0.00000000

[batch =  100] train_perplexity =    59.78, train_loss =  4.09, learning_rate = 0.00000000


[batch =  200] train_perplexity =    64.11, train_loss =  4.16, learning_rate = 0.00000000


[batch =  300] train_perplexity =    63.81, train_loss =  4.16, learning_rate = 0.00000000


[batch =  400] train_perplexity =    58.39, train_loss =  4.07, learning_rate = 0.00000000


[batch =  500] train_perplexity =    58.09, train_loss =  4.06, learning_rate = 0.00000000


[batch =  600] train_perplexity =    62.07, train_loss =  4.13, learning_rate = 0.00000000


[batch =  700] train_perplexity =    59.94, train_loss =  4.09, learning_rate = 0.00000000


[batch =  800] train_perplexity =    55.08, train_loss =  4.01, learning_rate = 0.00000000


[batch =  900] train_perplexity =    59.81, train_loss =  4.09, learning_rate = 0.00000000


[batch = 1000] train_perplexity =    63.03, train_loss =  4.14, learning_rate = 0.00000000


[batch = 1100] train_perplexity =    53.37, train_loss =  3.98, learning_rate = 0.00000000


[batch = 1200] train_perplexity =    54.32, train_loss =  3.99, learning_rate = 0.00000000


[batch = 1300] train_perplexity =    54.96, train_loss =  4.01, learning_rate = 0.00000000


[epoch =  62] validation_perplexity =    95.56, validation_loss =  4.56

[epoch =  62] annealing learning_rate = 0.00000000

[batch =  100] train_perplexity =    59.49, train_loss =  4.09, learning_rate = 0.00000000


[batch =  200] train_perplexity =    64.15, train_loss =  4.16, learning_rate = 0.00000000


[batch =  300] train_perplexity =    63.99, train_loss =  4.16, learning_rate = 0.00000000


[batch =  400] train_perplexity =    58.42, train_loss =  4.07, learning_rate = 0.00000000


[batch =  500] train_perplexity =    58.34, train_loss =  4.07, learning_rate = 0.00000000


[batch =  600] train_perplexity =    62.00, train_loss =  4.13, learning_rate = 0.00000000


[batch =  700] train_perplexity =    60.19, train_loss =  4.10, learning_rate = 0.00000000


[batch =  800] train_perplexity =    55.05, train_loss =  4.01, learning_rate = 0.00000000


[batch =  900] train_perplexity =    59.97, train_loss =  4.09, learning_rate = 0.00000000


[batch = 1000] train_perplexity =    62.88, train_loss =  4.14, learning_rate = 0.00000000


[batch = 1100] train_perplexity =    53.81, train_loss =  3.99, learning_rate = 0.00000000


[batch = 1200] train_perplexity =    54.11, train_loss =  3.99, learning_rate = 0.00000000


[batch = 1300] train_perplexity =    54.65, train_loss =  4.00, learning_rate = 0.00000000


[epoch =  63] validation_perplexity =    95.56, validation_loss =  4.56

[epoch =  63] annealing learning_rate = 0.00000000

[batch =  100] train_perplexity =    59.81, train_loss =  4.09, learning_rate = 0.00000000


[batch =  200] train_perplexity =    63.66, train_loss =  4.15, learning_rate = 0.00000000


[batch =  300] train_perplexity =    63.96, train_loss =  4.16, learning_rate = 0.00000000


[batch =  400] train_perplexity =    58.81, train_loss =  4.07, learning_rate = 0.00000000


[batch =  500] train_perplexity =    58.25, train_loss =  4.06, learning_rate = 0.00000000


[batch =  600] train_perplexity =    62.28, train_loss =  4.13, learning_rate = 0.00000000


[batch =  700] train_perplexity =    59.70, train_loss =  4.09, learning_rate = 0.00000000


[batch =  800] train_perplexity =    54.88, train_loss =  4.01, learning_rate = 0.00000000


[batch =  900] train_perplexity =    59.94, train_loss =  4.09, learning_rate = 0.00000000


[batch = 1000] train_perplexity =    62.71, train_loss =  4.14, learning_rate = 0.00000000


[batch = 1100] train_perplexity =    53.68, train_loss =  3.98, learning_rate = 0.00000000


[batch = 1200] train_perplexity =    54.54, train_loss =  4.00, learning_rate = 0.00000000


[batch = 1300] train_perplexity =    54.80, train_loss =  4.00, learning_rate = 0.00000000


[epoch =  64] validation_perplexity =    95.56, validation_loss =  4.56

[epoch =  64] annealing learning_rate = 0.00000000

[batch =  100] train_perplexity =    59.74, train_loss =  4.09, learning_rate = 0.00000000


[batch =  200] train_perplexity =    63.96, train_loss =  4.16, learning_rate = 0.00000000


[batch =  300] train_perplexity =    63.67, train_loss =  4.15, learning_rate = 0.00000000


[batch =  400] train_perplexity =    58.52, train_loss =  4.07, learning_rate = 0.00000000


[batch =  500] train_perplexity =    58.35, train_loss =  4.07, learning_rate = 0.00000000


[batch =  600] train_perplexity =    62.28, train_loss =  4.13, learning_rate = 0.00000000


[batch =  700] train_perplexity =    59.61, train_loss =  4.09, learning_rate = 0.00000000


[batch =  800] train_perplexity =    54.74, train_loss =  4.00, learning_rate = 0.00000000


[batch =  900] train_perplexity =    59.88, train_loss =  4.09, learning_rate = 0.00000000


[batch = 1000] train_perplexity =    62.97, train_loss =  4.14, learning_rate = 0.00000000


[batch = 1100] train_perplexity =    53.90, train_loss =  3.99, learning_rate = 0.00000000


[batch = 1200] train_perplexity =    54.23, train_loss =  3.99, learning_rate = 0.00000000


[batch = 1300] train_perplexity =    54.68, train_loss =  4.00, learning_rate = 0.00000000


[epoch =  65] validation_perplexity =    95.56, validation_loss =  4.56

[epoch =  65] annealing learning_rate = 0.00000000

[batch =  100] train_perplexity =    59.62, train_loss =  4.09, learning_rate = 0.00000000


[batch =  200] train_perplexity =    64.07, train_loss =  4.16, learning_rate = 0.00000000


[batch =  300] train_perplexity =    63.96, train_loss =  4.16, learning_rate = 0.00000000


[batch =  400] train_perplexity =    58.55, train_loss =  4.07, learning_rate = 0.00000000


[batch =  500] train_perplexity =    58.54, train_loss =  4.07, learning_rate = 0.00000000


[batch =  600] train_perplexity =    62.06, train_loss =  4.13, learning_rate = 0.00000000


[batch =  700] train_perplexity =    59.77, train_loss =  4.09, learning_rate = 0.00000000


[batch =  800] train_perplexity =    54.87, train_loss =  4.00, learning_rate = 0.00000000


[batch =  900] train_perplexity =    60.11, train_loss =  4.10, learning_rate = 0.00000000


[batch = 1000] train_perplexity =    63.10, train_loss =  4.14, learning_rate = 0.00000000


[batch = 1100] train_perplexity =    53.78, train_loss =  3.98, learning_rate = 0.00000000


[batch = 1200] train_perplexity =    54.28, train_loss =  3.99, learning_rate = 0.00000000


[batch = 1300] train_perplexity =    54.49, train_loss =  4.00, learning_rate = 0.00000000


[epoch =  66] validation_perplexity =    95.56, validation_loss =  4.56

[epoch =  66] annealing learning_rate = 0.00000000

[batch =  100] train_perplexity =    59.39, train_loss =  4.08, learning_rate = 0.00000000


[batch =  200] train_perplexity =    64.27, train_loss =  4.16, learning_rate = 0.00000000


[batch =  300] train_perplexity =    64.13, train_loss =  4.16, learning_rate = 0.00000000


[batch =  400] train_perplexity =    58.61, train_loss =  4.07, learning_rate = 0.00000000


[batch =  500] train_perplexity =    58.11, train_loss =  4.06, learning_rate = 0.00000000


[batch =  600] train_perplexity =    61.98, train_loss =  4.13, learning_rate = 0.00000000


[batch =  700] train_perplexity =    60.06, train_loss =  4.10, learning_rate = 0.00000000


[batch =  800] train_perplexity =    55.13, train_loss =  4.01, learning_rate = 0.00000000


[batch =  900] train_perplexity =    59.79, train_loss =  4.09, learning_rate = 0.00000000


[batch = 1000] train_perplexity =    63.10, train_loss =  4.14, learning_rate = 0.00000000


[batch = 1100] train_perplexity =    53.84, train_loss =  3.99, learning_rate = 0.00000000


[batch = 1200] train_perplexity =    54.58, train_loss =  4.00, learning_rate = 0.00000000


[batch = 1300] train_perplexity =    54.86, train_loss =  4.00, learning_rate = 0.00000000


[epoch =  67] validation_perplexity =    95.56, validation_loss =  4.56

[epoch =  67] annealing learning_rate = 0.00000000

[batch =  100] train_perplexity =    59.84, train_loss =  4.09, learning_rate = 0.00000000


[batch =  200] train_perplexity =    63.77, train_loss =  4.16, learning_rate = 0.00000000


[batch =  300] train_perplexity =    63.78, train_loss =  4.16, learning_rate = 0.00000000


[batch =  400] train_perplexity =    58.70, train_loss =  4.07, learning_rate = 0.00000000


[batch =  500] train_perplexity =    58.21, train_loss =  4.06, learning_rate = 0.00000000


[batch =  600] train_perplexity =    62.06, train_loss =  4.13, learning_rate = 0.00000000


[batch =  700] train_perplexity =    59.76, train_loss =  4.09, learning_rate = 0.00000000


[batch =  800] train_perplexity =    54.91, train_loss =  4.01, learning_rate = 0.00000000


[batch =  900] train_perplexity =    59.94, train_loss =  4.09, learning_rate = 0.00000000


[batch = 1000] train_perplexity =    62.81, train_loss =  4.14, learning_rate = 0.00000000


[batch = 1100] train_perplexity =    53.54, train_loss =  3.98, learning_rate = 0.00000000


[batch = 1200] train_perplexity =    54.35, train_loss =  4.00, learning_rate = 0.00000000


[batch = 1300] train_perplexity =    54.94, train_loss =  4.01, learning_rate = 0.00000000


[epoch =  68] validation_perplexity =    95.56, validation_loss =  4.56

[epoch =  68] annealing learning_rate = 0.00000000

[batch =  100] train_perplexity =    59.73, train_loss =  4.09, learning_rate = 0.00000000


[batch =  200] train_perplexity =    64.29, train_loss =  4.16, learning_rate = 0.00000000


[batch =  300] train_perplexity =    63.91, train_loss =  4.16, learning_rate = 0.00000000


[batch =  400] train_perplexity =    58.54, train_loss =  4.07, learning_rate = 0.00000000


[batch =  500] train_perplexity =    58.17, train_loss =  4.06, learning_rate = 0.00000000


[batch =  600] train_perplexity =    61.86, train_loss =  4.12, learning_rate = 0.00000000


[batch =  700] train_perplexity =    60.14, train_loss =  4.10, learning_rate = 0.00000000


[batch =  800] train_perplexity =    55.17, train_loss =  4.01, learning_rate = 0.00000000


[batch =  900] train_perplexity =    59.88, train_loss =  4.09, learning_rate = 0.00000000


[batch = 1000] train_perplexity =    62.65, train_loss =  4.14, learning_rate = 0.00000000


[batch = 1100] train_perplexity =    53.46, train_loss =  3.98, learning_rate = 0.00000000


[batch = 1200] train_perplexity =    54.53, train_loss =  4.00, learning_rate = 0.00000000


[batch = 1300] train_perplexity =    54.59, train_loss =  4.00, learning_rate = 0.00000000


[epoch =  69] validation_perplexity =    95.56, validation_loss =  4.56

[epoch =  69] annealing learning_rate = 0.00000000

[batch =  100] train_perplexity =    59.84, train_loss =  4.09, learning_rate = 0.00000000


[batch =  200] train_perplexity =    63.98, train_loss =  4.16, learning_rate = 0.00000000


[batch =  300] train_perplexity =    63.87, train_loss =  4.16, learning_rate = 0.00000000


[batch =  400] train_perplexity =    58.65, train_loss =  4.07, learning_rate = 0.00000000


[batch =  500] train_perplexity =    58.20, train_loss =  4.06, learning_rate = 0.00000000


[batch =  600] train_perplexity =    62.05, train_loss =  4.13, learning_rate = 0.00000000


[batch =  700] train_perplexity =    59.95, train_loss =  4.09, learning_rate = 0.00000000


[batch =  800] train_perplexity =    55.09, train_loss =  4.01, learning_rate = 0.00000000


[batch =  900] train_perplexity =    59.89, train_loss =  4.09, learning_rate = 0.00000000


[batch = 1000] train_perplexity =    63.05, train_loss =  4.14, learning_rate = 0.00000000


[batch = 1100] train_perplexity =    53.87, train_loss =  3.99, learning_rate = 0.00000000


[batch = 1200] train_perplexity =    54.21, train_loss =  3.99, learning_rate = 0.00000000


[batch = 1300] train_perplexity =    54.46, train_loss =  4.00, learning_rate = 0.00000000


[epoch =  70] validation_perplexity =    95.56, validation_loss =  4.56

[epoch =  70] annealing learning_rate = 0.00000000

[batch =  100] train_perplexity =    60.15, train_loss =  4.10, learning_rate = 0.00000000


[batch =  200] train_perplexity =    64.18, train_loss =  4.16, learning_rate = 0.00000000


[batch =  300] train_perplexity =    63.93, train_loss =  4.16, learning_rate = 0.00000000


[batch =  400] train_perplexity =    58.68, train_loss =  4.07, learning_rate = 0.00000000


[batch =  500] train_perplexity =    58.73, train_loss =  4.07, learning_rate = 0.00000000


[batch =  600] train_perplexity =    62.38, train_loss =  4.13, learning_rate = 0.00000000


[batch =  700] train_perplexity =    59.72, train_loss =  4.09, learning_rate = 0.00000000


[batch =  800] train_perplexity =    55.00, train_loss =  4.01, learning_rate = 0.00000000


[batch =  900] train_perplexity =    60.11, train_loss =  4.10, learning_rate = 0.00000000


[batch = 1000] train_perplexity =    62.87, train_loss =  4.14, learning_rate = 0.00000000


[batch = 1100] train_perplexity =    53.59, train_loss =  3.98, learning_rate = 0.00000000


[batch = 1200] train_perplexity =    54.32, train_loss =  3.99, learning_rate = 0.00000000


[batch = 1300] train_perplexity =    54.89, train_loss =  4.01, learning_rate = 0.00000000


[epoch =  71] validation_perplexity =    95.56, validation_loss =  4.56

[epoch =  71] annealing learning_rate = 0.00000000

[batch =  100] train_perplexity =    59.55, train_loss =  4.09, learning_rate = 0.00000000


[batch =  200] train_perplexity =    64.01, train_loss =  4.16, learning_rate = 0.00000000


[batch =  300] train_perplexity =    63.99, train_loss =  4.16, learning_rate = 0.00000000


[batch =  400] train_perplexity =    58.79, train_loss =  4.07, learning_rate = 0.00000000


[batch =  500] train_perplexity =    58.55, train_loss =  4.07, learning_rate = 0.00000000


[batch =  600] train_perplexity =    62.27, train_loss =  4.13, learning_rate = 0.00000000


[batch =  700] train_perplexity =    60.01, train_loss =  4.09, learning_rate = 0.00000000


[batch =  800] train_perplexity =    54.99, train_loss =  4.01, learning_rate = 0.00000000


[batch =  900] train_perplexity =    60.11, train_loss =  4.10, learning_rate = 0.00000000


[batch = 1000] train_perplexity =    63.02, train_loss =  4.14, learning_rate = 0.00000000


[batch = 1100] train_perplexity =    53.82, train_loss =  3.99, learning_rate = 0.00000000


[batch = 1200] train_perplexity =    54.28, train_loss =  3.99, learning_rate = 0.00000000


[batch = 1300] train_perplexity =    54.71, train_loss =  4.00, learning_rate = 0.00000000


[epoch =  72] validation_perplexity =    95.56, validation_loss =  4.56

[epoch =  72] annealing learning_rate = 0.00000000

[batch =  100] train_perplexity =    59.59, train_loss =  4.09, learning_rate = 0.00000000


[batch =  200] train_perplexity =    64.19, train_loss =  4.16, learning_rate = 0.00000000


[batch =  300] train_perplexity =    64.13, train_loss =  4.16, learning_rate = 0.00000000


[batch =  400] train_perplexity =    58.39, train_loss =  4.07, learning_rate = 0.00000000


[batch =  500] train_perplexity =    58.49, train_loss =  4.07, learning_rate = 0.00000000


[batch =  600] train_perplexity =    62.18, train_loss =  4.13, learning_rate = 0.00000000


[batch =  700] train_perplexity =    60.00, train_loss =  4.09, learning_rate = 0.00000000


[batch =  800] train_perplexity =    54.96, train_loss =  4.01, learning_rate = 0.00000000


[batch =  900] train_perplexity =    60.09, train_loss =  4.10, learning_rate = 0.00000000


[batch = 1000] train_perplexity =    62.82, train_loss =  4.14, learning_rate = 0.00000000


[batch = 1100] train_perplexity =    53.92, train_loss =  3.99, learning_rate = 0.00000000


[batch = 1200] train_perplexity =    54.55, train_loss =  4.00, learning_rate = 0.00000000


[batch = 1300] train_perplexity =    54.89, train_loss =  4.01, learning_rate = 0.00000000


[epoch =  73] validation_perplexity =    95.56, validation_loss =  4.56

[epoch =  73] annealing learning_rate = 0.00000000

[batch =  100] train_perplexity =    59.70, train_loss =  4.09, learning_rate = 0.00000000


[batch =  200] train_perplexity =    64.20, train_loss =  4.16, learning_rate = 0.00000000


[batch =  300] train_perplexity =    63.83, train_loss =  4.16, learning_rate = 0.00000000


[batch =  400] train_perplexity =    58.62, train_loss =  4.07, learning_rate = 0.00000000


[batch =  500] train_perplexity =    58.38, train_loss =  4.07, learning_rate = 0.00000000


[batch =  600] train_perplexity =    62.15, train_loss =  4.13, learning_rate = 0.00000000


[batch =  700] train_perplexity =    60.03, train_loss =  4.09, learning_rate = 0.00000000


[batch =  800] train_perplexity =    54.94, train_loss =  4.01, learning_rate = 0.00000000


[batch =  900] train_perplexity =    59.73, train_loss =  4.09, learning_rate = 0.00000000


[batch = 1000] train_perplexity =    62.58, train_loss =  4.14, learning_rate = 0.00000000


[batch = 1100] train_perplexity =    53.63, train_loss =  3.98, learning_rate = 0.00000000


[batch = 1200] train_perplexity =    54.48, train_loss =  4.00, learning_rate = 0.00000000


[batch = 1300] train_perplexity =    54.67, train_loss =  4.00, learning_rate = 0.00000000


[epoch =  74] validation_perplexity =    95.56, validation_loss =  4.56

[epoch =  74] annealing learning_rate = 0.00000000

[batch =  100] train_perplexity =    59.48, train_loss =  4.09, learning_rate = 0.00000000


[batch =  200] train_perplexity =    63.79, train_loss =  4.16, learning_rate = 0.00000000


[batch =  300] train_perplexity =    63.80, train_loss =  4.16, learning_rate = 0.00000000


[batch =  400] train_perplexity =    58.56, train_loss =  4.07, learning_rate = 0.00000000


[batch =  500] train_perplexity =    58.35, train_loss =  4.07, learning_rate = 0.00000000


[batch =  600] train_perplexity =    62.09, train_loss =  4.13, learning_rate = 0.00000000


[batch =  700] train_perplexity =    59.92, train_loss =  4.09, learning_rate = 0.00000000


[batch =  800] train_perplexity =    54.86, train_loss =  4.00, learning_rate = 0.00000000


[batch =  900] train_perplexity =    60.10, train_loss =  4.10, learning_rate = 0.00000000


[batch = 1000] train_perplexity =    63.11, train_loss =  4.14, learning_rate = 0.00000000


[batch = 1100] train_perplexity =    53.92, train_loss =  3.99, learning_rate = 0.00000000


[batch = 1200] train_perplexity =    54.30, train_loss =  3.99, learning_rate = 0.00000000


[batch = 1300] train_perplexity =    54.94, train_loss =  4.01, learning_rate = 0.00000000


[epoch =  75] validation_perplexity =    95.56, validation_loss =  4.56

[epoch =  75] annealing learning_rate = 0.00000000

[batch =  100] train_perplexity =    59.75, train_loss =  4.09, learning_rate = 0.00000000


[batch =  200] train_perplexity =    63.90, train_loss =  4.16, learning_rate = 0.00000000


[batch =  300] train_perplexity =    63.69, train_loss =  4.15, learning_rate = 0.00000000


[batch =  400] train_perplexity =    58.71, train_loss =  4.07, learning_rate = 0.00000000


[batch =  500] train_perplexity =    58.20, train_loss =  4.06, learning_rate = 0.00000000


[batch =  600] train_perplexity =    62.16, train_loss =  4.13, learning_rate = 0.00000000


[batch =  700] train_perplexity =    60.00, train_loss =  4.09, learning_rate = 0.00000000


[batch =  800] train_perplexity =    55.00, train_loss =  4.01, learning_rate = 0.00000000


[batch =  900] train_perplexity =    59.82, train_loss =  4.09, learning_rate = 0.00000000


[batch = 1000] train_perplexity =    63.10, train_loss =  4.14, learning_rate = 0.00000000


[batch = 1100] train_perplexity =    53.92, train_loss =  3.99, learning_rate = 0.00000000


[batch = 1200] train_perplexity =    54.43, train_loss =  4.00, learning_rate = 0.00000000


[batch = 1300] train_perplexity =    54.78, train_loss =  4.00, learning_rate = 0.00000000


[epoch =  76] validation_perplexity =    95.56, validation_loss =  4.56

[epoch =  76] annealing learning_rate = 0.00000000

[batch =  100] train_perplexity =    59.34, train_loss =  4.08, learning_rate = 0.00000000


[batch =  200] train_perplexity =    64.14, train_loss =  4.16, learning_rate = 0.00000000


[batch =  300] train_perplexity =    64.14, train_loss =  4.16, learning_rate = 0.00000000


[batch =  400] train_perplexity =    58.75, train_loss =  4.07, learning_rate = 0.00000000


[batch =  500] train_perplexity =    58.46, train_loss =  4.07, learning_rate = 0.00000000


[batch =  600] train_perplexity =    61.86, train_loss =  4.12, learning_rate = 0.00000000


[batch =  700] train_perplexity =    59.53, train_loss =  4.09, learning_rate = 0.00000000


[batch =  800] train_perplexity =    55.10, train_loss =  4.01, learning_rate = 0.00000000


[batch =  900] train_perplexity =    60.24, train_loss =  4.10, learning_rate = 0.00000000


[batch = 1000] train_perplexity =    62.61, train_loss =  4.14, learning_rate = 0.00000000


[batch = 1100] train_perplexity =    53.64, train_loss =  3.98, learning_rate = 0.00000000


[batch = 1200] train_perplexity =    54.42, train_loss =  4.00, learning_rate = 0.00000000


[batch = 1300] train_perplexity =    55.11, train_loss =  4.01, learning_rate = 0.00000000


[epoch =  77] validation_perplexity =    95.56, validation_loss =  4.56

[epoch =  77] annealing learning_rate = 0.00000000

[batch =  100] train_perplexity =    59.99, train_loss =  4.09, learning_rate = 0.00000000


[batch =  200] train_perplexity =    64.08, train_loss =  4.16, learning_rate = 0.00000000


[batch =  300] train_perplexity =    63.99, train_loss =  4.16, learning_rate = 0.00000000


[batch =  400] train_perplexity =    58.72, train_loss =  4.07, learning_rate = 0.00000000


[batch =  500] train_perplexity =    58.25, train_loss =  4.06, learning_rate = 0.00000000


[batch =  600] train_perplexity =    62.14, train_loss =  4.13, learning_rate = 0.00000000


[batch =  700] train_perplexity =    59.81, train_loss =  4.09, learning_rate = 0.00000000


[batch =  800] train_perplexity =    55.07, train_loss =  4.01, learning_rate = 0.00000000


[batch =  900] train_perplexity =    59.66, train_loss =  4.09, learning_rate = 0.00000000


[batch = 1000] train_perplexity =    62.88, train_loss =  4.14, learning_rate = 0.00000000


[batch = 1100] train_perplexity =    53.53, train_loss =  3.98, learning_rate = 0.00000000


[batch = 1200] train_perplexity =    54.48, train_loss =  4.00, learning_rate = 0.00000000


[batch = 1300] train_perplexity =    54.85, train_loss =  4.00, learning_rate = 0.00000000


[epoch =  78] validation_perplexity =    95.56, validation_loss =  4.56

[epoch =  78] annealing learning_rate = 0.00000000

[batch =  100] train_perplexity =    59.63, train_loss =  4.09, learning_rate = 0.00000000


[batch =  200] train_perplexity =    64.06, train_loss =  4.16, learning_rate = 0.00000000


[batch =  300] train_perplexity =    63.92, train_loss =  4.16, learning_rate = 0.00000000


[batch =  400] train_perplexity =    58.84, train_loss =  4.07, learning_rate = 0.00000000


[batch =  500] train_perplexity =    57.90, train_loss =  4.06, learning_rate = 0.00000000


[batch =  600] train_perplexity =    62.21, train_loss =  4.13, learning_rate = 0.00000000


[batch =  700] train_perplexity =    60.52, train_loss =  4.10, learning_rate = 0.00000000


[batch =  800] train_perplexity =    54.87, train_loss =  4.00, learning_rate = 0.00000000


[batch =  900] train_perplexity =    59.75, train_loss =  4.09, learning_rate = 0.00000000


[batch = 1000] train_perplexity =    62.79, train_loss =  4.14, learning_rate = 0.00000000


[batch = 1100] train_perplexity =    53.84, train_loss =  3.99, learning_rate = 0.00000000


[batch = 1200] train_perplexity =    54.38, train_loss =  4.00, learning_rate = 0.00000000


[batch = 1300] train_perplexity =    54.46, train_loss =  4.00, learning_rate = 0.00000000


[epoch =  79] validation_perplexity =    95.56, validation_loss =  4.56

[epoch =  79] annealing learning_rate = 0.00000000

[batch =  100] train_perplexity =    60.19, train_loss =  4.10, learning_rate = 0.00000000


[batch =  200] train_perplexity =    64.06, train_loss =  4.16, learning_rate = 0.00000000


[batch =  300] train_perplexity =    63.82, train_loss =  4.16, learning_rate = 0.00000000


[batch =  400] train_perplexity =    58.66, train_loss =  4.07, learning_rate = 0.00000000


[batch =  500] train_perplexity =    58.29, train_loss =  4.07, learning_rate = 0.00000000


[batch =  600] train_perplexity =    62.08, train_loss =  4.13, learning_rate = 0.00000000


[batch =  700] train_perplexity =    59.73, train_loss =  4.09, learning_rate = 0.00000000


[batch =  800] train_perplexity =    54.99, train_loss =  4.01, learning_rate = 0.00000000


[batch =  900] train_perplexity =    59.80, train_loss =  4.09, learning_rate = 0.00000000


[batch = 1000] train_perplexity =    63.06, train_loss =  4.14, learning_rate = 0.00000000


[batch = 1100] train_perplexity =    53.74, train_loss =  3.98, learning_rate = 0.00000000


[batch = 1200] train_perplexity =    54.07, train_loss =  3.99, learning_rate = 0.00000000


[batch = 1300] train_perplexity =    54.54, train_loss =  4.00, learning_rate = 0.00000000


[epoch =  80] validation_perplexity =    95.56, validation_loss =  4.56

[epoch =  80] annealing learning_rate = 0.00000000

[batch =  100] train_perplexity =    60.27, train_loss =  4.10, learning_rate = 0.00000000


[batch =  200] train_perplexity =    63.88, train_loss =  4.16, learning_rate = 0.00000000


[batch =  300] train_perplexity =    63.92, train_loss =  4.16, learning_rate = 0.00000000


[batch =  400] train_perplexity =    58.35, train_loss =  4.07, learning_rate = 0.00000000


[batch =  500] train_perplexity =    58.28, train_loss =  4.07, learning_rate = 0.00000000


[batch =  600] train_perplexity =    62.20, train_loss =  4.13, learning_rate = 0.00000000


[batch =  700] train_perplexity =    59.86, train_loss =  4.09, learning_rate = 0.00000000


[batch =  800] train_perplexity =    55.15, train_loss =  4.01, learning_rate = 0.00000000


[batch =  900] train_perplexity =    60.10, train_loss =  4.10, learning_rate = 0.00000000


[batch = 1000] train_perplexity =    62.92, train_loss =  4.14, learning_rate = 0.00000000


[batch = 1100] train_perplexity =    53.62, train_loss =  3.98, learning_rate = 0.00000000


[batch = 1200] train_perplexity =    54.09, train_loss =  3.99, learning_rate = 0.00000000


[batch = 1300] train_perplexity =    54.73, train_loss =  4.00, learning_rate = 0.00000000


[epoch =  81] validation_perplexity =    95.56, validation_loss =  4.56

[epoch =  81] annealing learning_rate = 0.00000000

[batch =  100] train_perplexity =    59.73, train_loss =  4.09, learning_rate = 0.00000000


[batch =  200] train_perplexity =    64.03, train_loss =  4.16, learning_rate = 0.00000000


[batch =  300] train_perplexity =    64.05, train_loss =  4.16, learning_rate = 0.00000000


[batch =  400] train_perplexity =    58.82, train_loss =  4.07, learning_rate = 0.00000000


[batch =  500] train_perplexity =    58.55, train_loss =  4.07, learning_rate = 0.00000000


[batch =  600] train_perplexity =    62.24, train_loss =  4.13, learning_rate = 0.00000000


[batch =  700] train_perplexity =    59.89, train_loss =  4.09, learning_rate = 0.00000000


[batch =  800] train_perplexity =    55.04, train_loss =  4.01, learning_rate = 0.00000000


[batch =  900] train_perplexity =    60.34, train_loss =  4.10, learning_rate = 0.00000000


[batch = 1000] train_perplexity =    62.74, train_loss =  4.14, learning_rate = 0.00000000


[batch = 1100] train_perplexity =    53.70, train_loss =  3.98, learning_rate = 0.00000000


[batch = 1200] train_perplexity =    54.48, train_loss =  4.00, learning_rate = 0.00000000


[batch = 1300] train_perplexity =    54.64, train_loss =  4.00, learning_rate = 0.00000000


[epoch =  82] validation_perplexity =    95.56, validation_loss =  4.56

[epoch =  82] annealing learning_rate = 0.00000000

[batch =  100] train_perplexity =    59.85, train_loss =  4.09, learning_rate = 0.00000000


[batch =  200] train_perplexity =    64.12, train_loss =  4.16, learning_rate = 0.00000000


[batch =  300] train_perplexity =    63.79, train_loss =  4.16, learning_rate = 0.00000000


[batch =  400] train_perplexity =    58.28, train_loss =  4.07, learning_rate = 0.00000000


[batch =  500] train_perplexity =    58.15, train_loss =  4.06, learning_rate = 0.00000000


[batch =  600] train_perplexity =    62.01, train_loss =  4.13, learning_rate = 0.00000000


[batch =  700] train_perplexity =    59.93, train_loss =  4.09, learning_rate = 0.00000000


[batch =  800] train_perplexity =    54.92, train_loss =  4.01, learning_rate = 0.00000000


[batch =  900] train_perplexity =    59.98, train_loss =  4.09, learning_rate = 0.00000000


[batch = 1000] train_perplexity =    63.00, train_loss =  4.14, learning_rate = 0.00000000


[batch = 1100] train_perplexity =    53.44, train_loss =  3.98, learning_rate = 0.00000000


[batch = 1200] train_perplexity =    54.33, train_loss =  4.00, learning_rate = 0.00000000


[batch = 1300] train_perplexity =    54.85, train_loss =  4.00, learning_rate = 0.00000000


[epoch =  83] validation_perplexity =    95.56, validation_loss =  4.56

[epoch =  83] annealing learning_rate = 0.00000000

[batch =  100] train_perplexity =    59.70, train_loss =  4.09, learning_rate = 0.00000000


[batch =  200] train_perplexity =    63.81, train_loss =  4.16, learning_rate = 0.00000000


[batch =  300] train_perplexity =    63.83, train_loss =  4.16, learning_rate = 0.00000000


[batch =  400] train_perplexity =    58.33, train_loss =  4.07, learning_rate = 0.00000000


[batch =  500] train_perplexity =    58.32, train_loss =  4.07, learning_rate = 0.00000000


[batch =  600] train_perplexity =    61.94, train_loss =  4.13, learning_rate = 0.00000000


[batch =  700] train_perplexity =    60.10, train_loss =  4.10, learning_rate = 0.00000000


[batch =  800] train_perplexity =    55.13, train_loss =  4.01, learning_rate = 0.00000000


[batch =  900] train_perplexity =    59.68, train_loss =  4.09, learning_rate = 0.00000000


[batch = 1000] train_perplexity =    62.72, train_loss =  4.14, learning_rate = 0.00000000


[batch = 1100] train_perplexity =    53.52, train_loss =  3.98, learning_rate = 0.00000000


[batch = 1200] train_perplexity =    54.40, train_loss =  4.00, learning_rate = 0.00000000


[batch = 1300] train_perplexity =    54.77, train_loss =  4.00, learning_rate = 0.00000000


[epoch =  84] validation_perplexity =    95.56, validation_loss =  4.56

[epoch =  84] annealing learning_rate = 0.00000000

[batch =  100] train_perplexity =    59.67, train_loss =  4.09, learning_rate = 0.00000000


[batch =  200] train_perplexity =    63.73, train_loss =  4.15, learning_rate = 0.00000000


[batch =  300] train_perplexity =    63.64, train_loss =  4.15, learning_rate = 0.00000000


[batch =  400] train_perplexity =    58.77, train_loss =  4.07, learning_rate = 0.00000000


[batch =  500] train_perplexity =    57.89, train_loss =  4.06, learning_rate = 0.00000000


[batch =  600] train_perplexity =    61.96, train_loss =  4.13, learning_rate = 0.00000000


[batch =  700] train_perplexity =    60.20, train_loss =  4.10, learning_rate = 0.00000000


[batch =  800] train_perplexity =    54.98, train_loss =  4.01, learning_rate = 0.00000000


[batch =  900] train_perplexity =    60.01, train_loss =  4.09, learning_rate = 0.00000000


[batch = 1000] train_perplexity =    62.80, train_loss =  4.14, learning_rate = 0.00000000


[batch = 1100] train_perplexity =    53.68, train_loss =  3.98, learning_rate = 0.00000000


[batch = 1200] train_perplexity =    54.45, train_loss =  4.00, learning_rate = 0.00000000


[batch = 1300] train_perplexity =    54.77, train_loss =  4.00, learning_rate = 0.00000000


[epoch =  85] validation_perplexity =    95.56, validation_loss =  4.56

[epoch =  85] annealing learning_rate = 0.00000000

[batch =  100] train_perplexity =    59.80, train_loss =  4.09, learning_rate = 0.00000000


[batch =  200] train_perplexity =    63.98, train_loss =  4.16, learning_rate = 0.00000000


[batch =  300] train_perplexity =    64.20, train_loss =  4.16, learning_rate = 0.00000000


[batch =  400] train_perplexity =    58.66, train_loss =  4.07, learning_rate = 0.00000000


[batch =  500] train_perplexity =    58.38, train_loss =  4.07, learning_rate = 0.00000000


[batch =  600] train_perplexity =    62.00, train_loss =  4.13, learning_rate = 0.00000000


[batch =  700] train_perplexity =    59.72, train_loss =  4.09, learning_rate = 0.00000000


[batch =  800] train_perplexity =    54.92, train_loss =  4.01, learning_rate = 0.00000000


[batch =  900] train_perplexity =    60.10, train_loss =  4.10, learning_rate = 0.00000000


[batch = 1000] train_perplexity =    62.85, train_loss =  4.14, learning_rate = 0.00000000


[batch = 1100] train_perplexity =    53.72, train_loss =  3.98, learning_rate = 0.00000000


[batch = 1200] train_perplexity =    54.49, train_loss =  4.00, learning_rate = 0.00000000


[batch = 1300] train_perplexity =    54.86, train_loss =  4.00, learning_rate = 0.00000000


[epoch =  86] validation_perplexity =    95.56, validation_loss =  4.56

[epoch =  86] annealing learning_rate = 0.00000000

[batch =  100] train_perplexity =    59.52, train_loss =  4.09, learning_rate = 0.00000000


[batch =  200] train_perplexity =    63.58, train_loss =  4.15, learning_rate = 0.00000000


[batch =  300] train_perplexity =    63.90, train_loss =  4.16, learning_rate = 0.00000000


[batch =  400] train_perplexity =    58.59, train_loss =  4.07, learning_rate = 0.00000000


[batch =  500] train_perplexity =    58.34, train_loss =  4.07, learning_rate = 0.00000000


[batch =  600] train_perplexity =    61.97, train_loss =  4.13, learning_rate = 0.00000000


[batch =  700] train_perplexity =    60.06, train_loss =  4.10, learning_rate = 0.00000000


[batch =  800] train_perplexity =    55.22, train_loss =  4.01, learning_rate = 0.00000000


[batch =  900] train_perplexity =    60.01, train_loss =  4.09, learning_rate = 0.00000000


[batch = 1000] train_perplexity =    62.86, train_loss =  4.14, learning_rate = 0.00000000


[batch = 1100] train_perplexity =    53.89, train_loss =  3.99, learning_rate = 0.00000000


[batch = 1200] train_perplexity =    54.32, train_loss =  3.99, learning_rate = 0.00000000


[batch = 1300] train_perplexity =    54.93, train_loss =  4.01, learning_rate = 0.00000000


[epoch =  87] validation_perplexity =    95.56, validation_loss =  4.56

[epoch =  87] annealing learning_rate = 0.00000000

[batch =  100] train_perplexity =    59.84, train_loss =  4.09, learning_rate = 0.00000000


[batch =  200] train_perplexity =    63.91, train_loss =  4.16, learning_rate = 0.00000000


[batch =  300] train_perplexity =    63.63, train_loss =  4.15, learning_rate = 0.00000000


[batch =  400] train_perplexity =    58.78, train_loss =  4.07, learning_rate = 0.00000000


[batch =  500] train_perplexity =    58.40, train_loss =  4.07, learning_rate = 0.00000000


[batch =  600] train_perplexity =    62.49, train_loss =  4.14, learning_rate = 0.00000000


[batch =  700] train_perplexity =    60.04, train_loss =  4.10, learning_rate = 0.00000000


[batch =  800] train_perplexity =    54.77, train_loss =  4.00, learning_rate = 0.00000000


[batch =  900] train_perplexity =    59.78, train_loss =  4.09, learning_rate = 0.00000000


[batch = 1000] train_perplexity =    62.99, train_loss =  4.14, learning_rate = 0.00000000


[batch = 1100] train_perplexity =    53.92, train_loss =  3.99, learning_rate = 0.00000000


[batch = 1200] train_perplexity =    54.20, train_loss =  3.99, learning_rate = 0.00000000


[batch = 1300] train_perplexity =    54.72, train_loss =  4.00, learning_rate = 0.00000000


[epoch =  88] validation_perplexity =    95.56, validation_loss =  4.56

[epoch =  88] annealing learning_rate = 0.00000000

[batch =  100] train_perplexity =    59.87, train_loss =  4.09, learning_rate = 0.00000000


[batch =  200] train_perplexity =    64.09, train_loss =  4.16, learning_rate = 0.00000000


[batch =  300] train_perplexity =    63.94, train_loss =  4.16, learning_rate = 0.00000000


[batch =  400] train_perplexity =    58.68, train_loss =  4.07, learning_rate = 0.00000000


[batch =  500] train_perplexity =    58.44, train_loss =  4.07, learning_rate = 0.00000000


[batch =  600] train_perplexity =    61.93, train_loss =  4.13, learning_rate = 0.00000000


[batch =  700] train_perplexity =    60.41, train_loss =  4.10, learning_rate = 0.00000000


[batch =  800] train_perplexity =    54.90, train_loss =  4.01, learning_rate = 0.00000000


[batch =  900] train_perplexity =    60.38, train_loss =  4.10, learning_rate = 0.00000000


[batch = 1000] train_perplexity =    62.89, train_loss =  4.14, learning_rate = 0.00000000


[batch = 1100] train_perplexity =    53.49, train_loss =  3.98, learning_rate = 0.00000000


[batch = 1200] train_perplexity =    54.38, train_loss =  4.00, learning_rate = 0.00000000


[batch = 1300] train_perplexity =    54.81, train_loss =  4.00, learning_rate = 0.00000000


[epoch =  89] validation_perplexity =    95.56, validation_loss =  4.56

[epoch =  89] annealing learning_rate = 0.00000000

[batch =  100] train_perplexity =    59.87, train_loss =  4.09, learning_rate = 0.00000000


[batch =  200] train_perplexity =    63.87, train_loss =  4.16, learning_rate = 0.00000000


[batch =  300] train_perplexity =    63.82, train_loss =  4.16, learning_rate = 0.00000000


[batch =  400] train_perplexity =    58.28, train_loss =  4.07, learning_rate = 0.00000000


[batch =  500] train_perplexity =    58.08, train_loss =  4.06, learning_rate = 0.00000000


[batch =  600] train_perplexity =    62.26, train_loss =  4.13, learning_rate = 0.00000000


[batch =  700] train_perplexity =    59.79, train_loss =  4.09, learning_rate = 0.00000000


[batch =  800] train_perplexity =    54.80, train_loss =  4.00, learning_rate = 0.00000000


[batch =  900] train_perplexity =    60.28, train_loss =  4.10, learning_rate = 0.00000000


[batch = 1000] train_perplexity =    63.15, train_loss =  4.15, learning_rate = 0.00000000


[batch = 1100] train_perplexity =    53.66, train_loss =  3.98, learning_rate = 0.00000000


[batch = 1200] train_perplexity =    54.11, train_loss =  3.99, learning_rate = 0.00000000


[batch = 1300] train_perplexity =    54.76, train_loss =  4.00, learning_rate = 0.00000000


[epoch =  90] validation_perplexity =    95.56, validation_loss =  4.56

[epoch =  90] annealing learning_rate = 0.00000000

[batch =  100] train_perplexity =    59.60, train_loss =  4.09, learning_rate = 0.00000000


[batch =  200] train_perplexity =    64.35, train_loss =  4.16, learning_rate = 0.00000000


[batch =  300] train_perplexity =    63.96, train_loss =  4.16, learning_rate = 0.00000000


[batch =  400] train_perplexity =    58.58, train_loss =  4.07, learning_rate = 0.00000000


[batch =  500] train_perplexity =    58.42, train_loss =  4.07, learning_rate = 0.00000000


[batch =  600] train_perplexity =    62.44, train_loss =  4.13, learning_rate = 0.00000000


[batch =  700] train_perplexity =    60.08, train_loss =  4.10, learning_rate = 0.00000000


[batch =  800] train_perplexity =    54.85, train_loss =  4.00, learning_rate = 0.00000000


[batch =  900] train_perplexity =    59.80, train_loss =  4.09, learning_rate = 0.00000000


[batch = 1000] train_perplexity =    63.02, train_loss =  4.14, learning_rate = 0.00000000


[batch = 1100] train_perplexity =    53.75, train_loss =  3.98, learning_rate = 0.00000000


[batch = 1200] train_perplexity =    54.14, train_loss =  3.99, learning_rate = 0.00000000


[batch = 1300] train_perplexity =    54.73, train_loss =  4.00, learning_rate = 0.00000000


[epoch =  91] validation_perplexity =    95.56, validation_loss =  4.56

[epoch =  91] annealing learning_rate = 0.00000000

[batch =  100] train_perplexity =    59.89, train_loss =  4.09, learning_rate = 0.00000000


[batch =  200] train_perplexity =    63.98, train_loss =  4.16, learning_rate = 0.00000000


[batch =  300] train_perplexity =    63.78, train_loss =  4.16, learning_rate = 0.00000000


[batch =  400] train_perplexity =    58.63, train_loss =  4.07, learning_rate = 0.00000000


[batch =  500] train_perplexity =    58.45, train_loss =  4.07, learning_rate = 0.00000000


[batch =  600] train_perplexity =    62.15, train_loss =  4.13, learning_rate = 0.00000000


[batch =  700] train_perplexity =    60.01, train_loss =  4.09, learning_rate = 0.00000000


[batch =  800] train_perplexity =    54.95, train_loss =  4.01, learning_rate = 0.00000000


[batch =  900] train_perplexity =    60.22, train_loss =  4.10, learning_rate = 0.00000000


[batch = 1000] train_perplexity =    62.91, train_loss =  4.14, learning_rate = 0.00000000


[batch = 1100] train_perplexity =    53.44, train_loss =  3.98, learning_rate = 0.00000000


[batch = 1200] train_perplexity =    54.46, train_loss =  4.00, learning_rate = 0.00000000


[batch = 1300] train_perplexity =    54.88, train_loss =  4.01, learning_rate = 0.00000000


[epoch =  92] validation_perplexity =    95.56, validation_loss =  4.56

[epoch =  92] annealing learning_rate = 0.00000000

[batch =  100] train_perplexity =    59.84, train_loss =  4.09, learning_rate = 0.00000000


[batch =  200] train_perplexity =    64.16, train_loss =  4.16, learning_rate = 0.00000000


[batch =  300] train_perplexity =    63.91, train_loss =  4.16, learning_rate = 0.00000000


[batch =  400] train_perplexity =    58.85, train_loss =  4.08, learning_rate = 0.00000000


[batch =  500] train_perplexity =    58.39, train_loss =  4.07, learning_rate = 0.00000000


[batch =  600] train_perplexity =    61.82, train_loss =  4.12, learning_rate = 0.00000000


[batch =  700] train_perplexity =    60.08, train_loss =  4.10, learning_rate = 0.00000000


[batch =  800] train_perplexity =    55.09, train_loss =  4.01, learning_rate = 0.00000000


[batch =  900] train_perplexity =    59.90, train_loss =  4.09, learning_rate = 0.00000000


[batch = 1000] train_perplexity =    62.90, train_loss =  4.14, learning_rate = 0.00000000


[batch = 1100] train_perplexity =    53.89, train_loss =  3.99, learning_rate = 0.00000000


[batch = 1200] train_perplexity =    54.25, train_loss =  3.99, learning_rate = 0.00000000


[batch = 1300] train_perplexity =    54.69, train_loss =  4.00, learning_rate = 0.00000000


[epoch =  93] validation_perplexity =    95.56, validation_loss =  4.56

[epoch =  93] annealing learning_rate = 0.00000000

[batch =  100] train_perplexity =    59.85, train_loss =  4.09, learning_rate = 0.00000000


[batch =  200] train_perplexity =    63.95, train_loss =  4.16, learning_rate = 0.00000000


[batch =  300] train_perplexity =    63.68, train_loss =  4.15, learning_rate = 0.00000000


[batch =  400] train_perplexity =    58.29, train_loss =  4.07, learning_rate = 0.00000000


[batch =  500] train_perplexity =    58.30, train_loss =  4.07, learning_rate = 0.00000000


[batch =  600] train_perplexity =    61.92, train_loss =  4.13, learning_rate = 0.00000000


[batch =  700] train_perplexity =    60.01, train_loss =  4.09, learning_rate = 0.00000000


[batch =  800] train_perplexity =    54.95, train_loss =  4.01, learning_rate = 0.00000000


[batch =  900] train_perplexity =    60.10, train_loss =  4.10, learning_rate = 0.00000000


[batch = 1000] train_perplexity =    62.77, train_loss =  4.14, learning_rate = 0.00000000


[batch = 1100] train_perplexity =    53.67, train_loss =  3.98, learning_rate = 0.00000000


[batch = 1200] train_perplexity =    54.11, train_loss =  3.99, learning_rate = 0.00000000


[batch = 1300] train_perplexity =    54.57, train_loss =  4.00, learning_rate = 0.00000000


[epoch =  94] validation_perplexity =    95.56, validation_loss =  4.56

[epoch =  94] annealing learning_rate = 0.00000000

[batch =  100] train_perplexity =    59.66, train_loss =  4.09, learning_rate = 0.00000000


[batch =  200] train_perplexity =    63.88, train_loss =  4.16, learning_rate = 0.00000000


[batch =  300] train_perplexity =    63.95, train_loss =  4.16, learning_rate = 0.00000000


[batch =  400] train_perplexity =    58.48, train_loss =  4.07, learning_rate = 0.00000000


[batch =  500] train_perplexity =    58.38, train_loss =  4.07, learning_rate = 0.00000000


[batch =  600] train_perplexity =    62.13, train_loss =  4.13, learning_rate = 0.00000000


[batch =  700] train_perplexity =    60.05, train_loss =  4.10, learning_rate = 0.00000000


[batch =  800] train_perplexity =    55.02, train_loss =  4.01, learning_rate = 0.00000000


[batch =  900] train_perplexity =    60.12, train_loss =  4.10, learning_rate = 0.00000000


[batch = 1000] train_perplexity =    63.32, train_loss =  4.15, learning_rate = 0.00000000


[batch = 1100] train_perplexity =    53.60, train_loss =  3.98, learning_rate = 0.00000000


[batch = 1200] train_perplexity =    54.29, train_loss =  3.99, learning_rate = 0.00000000


[batch = 1300] train_perplexity =    54.83, train_loss =  4.00, learning_rate = 0.00000000


[epoch =  95] validation_perplexity =    95.56, validation_loss =  4.56

[epoch =  95] annealing learning_rate = 0.00000000

[batch =  100] train_perplexity =    59.69, train_loss =  4.09, learning_rate = 0.00000000


[batch =  200] train_perplexity =    64.28, train_loss =  4.16, learning_rate = 0.00000000


[batch =  300] train_perplexity =    63.93, train_loss =  4.16, learning_rate = 0.00000000


[batch =  400] train_perplexity =    58.60, train_loss =  4.07, learning_rate = 0.00000000


[batch =  500] train_perplexity =    57.94, train_loss =  4.06, learning_rate = 0.00000000


[batch =  600] train_perplexity =    62.16, train_loss =  4.13, learning_rate = 0.00000000


[batch =  700] train_perplexity =    60.06, train_loss =  4.10, learning_rate = 0.00000000


[batch =  800] train_perplexity =    54.82, train_loss =  4.00, learning_rate = 0.00000000


[batch =  900] train_perplexity =    60.20, train_loss =  4.10, learning_rate = 0.00000000


[batch = 1000] train_perplexity =    63.00, train_loss =  4.14, learning_rate = 0.00000000


[batch = 1100] train_perplexity =    53.89, train_loss =  3.99, learning_rate = 0.00000000


[batch = 1200] train_perplexity =    54.60, train_loss =  4.00, learning_rate = 0.00000000


[batch = 1300] train_perplexity =    54.51, train_loss =  4.00, learning_rate = 0.00000000


[epoch =  96] validation_perplexity =    95.56, validation_loss =  4.56

[epoch =  96] annealing learning_rate = 0.00000000

[batch =  100] train_perplexity =    59.63, train_loss =  4.09, learning_rate = 0.00000000


[batch =  200] train_perplexity =    64.12, train_loss =  4.16, learning_rate = 0.00000000


[batch =  300] train_perplexity =    63.77, train_loss =  4.16, learning_rate = 0.00000000


[batch =  400] train_perplexity =    59.00, train_loss =  4.08, learning_rate = 0.00000000


[batch =  500] train_perplexity =    58.36, train_loss =  4.07, learning_rate = 0.00000000


[batch =  600] train_perplexity =    62.07, train_loss =  4.13, learning_rate = 0.00000000


[batch =  700] train_perplexity =    59.81, train_loss =  4.09, learning_rate = 0.00000000


[batch =  800] train_perplexity =    54.65, train_loss =  4.00, learning_rate = 0.00000000


[batch =  900] train_perplexity =    60.00, train_loss =  4.09, learning_rate = 0.00000000


[batch = 1000] train_perplexity =    62.68, train_loss =  4.14, learning_rate = 0.00000000


[batch = 1100] train_perplexity =    53.76, train_loss =  3.98, learning_rate = 0.00000000


[batch = 1200] train_perplexity =    54.74, train_loss =  4.00, learning_rate = 0.00000000


[batch = 1300] train_perplexity =    54.47, train_loss =  4.00, learning_rate = 0.00000000


[epoch =  97] validation_perplexity =    95.56, validation_loss =  4.56

[epoch =  97] annealing learning_rate = 0.00000000

[batch =  100] train_perplexity =    59.80, train_loss =  4.09, learning_rate = 0.00000000


[batch =  200] train_perplexity =    63.92, train_loss =  4.16, learning_rate = 0.00000000


[batch =  300] train_perplexity =    63.95, train_loss =  4.16, learning_rate = 0.00000000


[batch =  400] train_perplexity =    58.37, train_loss =  4.07, learning_rate = 0.00000000


[batch =  500] train_perplexity =    58.43, train_loss =  4.07, learning_rate = 0.00000000


[batch =  600] train_perplexity =    62.36, train_loss =  4.13, learning_rate = 0.00000000


[batch =  700] train_perplexity =    59.74, train_loss =  4.09, learning_rate = 0.00000000


[batch =  800] train_perplexity =    54.95, train_loss =  4.01, learning_rate = 0.00000000


[batch =  900] train_perplexity =    60.00, train_loss =  4.09, learning_rate = 0.00000000


[batch = 1000] train_perplexity =    63.29, train_loss =  4.15, learning_rate = 0.00000000


[batch = 1100] train_perplexity =    53.49, train_loss =  3.98, learning_rate = 0.00000000


[batch = 1200] train_perplexity =    54.52, train_loss =  4.00, learning_rate = 0.00000000


[batch = 1300] train_perplexity =    55.10, train_loss =  4.01, learning_rate = 0.00000000


[epoch =  98] validation_perplexity =    95.56, validation_loss =  4.56

[epoch =  98] annealing learning_rate = 0.00000000

[batch =  100] train_perplexity =    59.80, train_loss =  4.09, learning_rate = 0.00000000


[batch =  200] train_perplexity =    63.90, train_loss =  4.16, learning_rate = 0.00000000


[batch =  300] train_perplexity =    63.85, train_loss =  4.16, learning_rate = 0.00000000


[batch =  400] train_perplexity =    58.45, train_loss =  4.07, learning_rate = 0.00000000


[batch =  500] train_perplexity =    58.52, train_loss =  4.07, learning_rate = 0.00000000


[batch =  600] train_perplexity =    62.15, train_loss =  4.13, learning_rate = 0.00000000


[batch =  700] train_perplexity =    59.94, train_loss =  4.09, learning_rate = 0.00000000


[batch =  800] train_perplexity =    54.97, train_loss =  4.01, learning_rate = 0.00000000


[batch =  900] train_perplexity =    60.08, train_loss =  4.10, learning_rate = 0.00000000


[batch = 1000] train_perplexity =    62.80, train_loss =  4.14, learning_rate = 0.00000000


[batch = 1100] train_perplexity =    53.83, train_loss =  3.99, learning_rate = 0.00000000


[batch = 1200] train_perplexity =    54.48, train_loss =  4.00, learning_rate = 0.00000000


[batch = 1300] train_perplexity =    54.92, train_loss =  4.01, learning_rate = 0.00000000


[epoch =  99] validation_perplexity =    95.56, validation_loss =  4.56

[epoch =  99] annealing learning_rate = 0.00000000

[batch =  100] train_perplexity =    59.70, train_loss =  4.09, learning_rate = 0.00000000


[batch =  200] train_perplexity =    64.03, train_loss =  4.16, learning_rate = 0.00000000


[batch =  300] train_perplexity =    64.01, train_loss =  4.16, learning_rate = 0.00000000


[batch =  400] train_perplexity =    58.78, train_loss =  4.07, learning_rate = 0.00000000


[batch =  500] train_perplexity =    58.24, train_loss =  4.06, learning_rate = 0.00000000


[batch =  600] train_perplexity =    62.12, train_loss =  4.13, learning_rate = 0.00000000


[batch =  700] train_perplexity =    60.10, train_loss =  4.10, learning_rate = 0.00000000


[batch =  800] train_perplexity =    54.92, train_loss =  4.01, learning_rate = 0.00000000


[batch =  900] train_perplexity =    60.12, train_loss =  4.10, learning_rate = 0.00000000


[batch = 1000] train_perplexity =    62.94, train_loss =  4.14, learning_rate = 0.00000000


[batch = 1100] train_perplexity =    54.00, train_loss =  3.99, learning_rate = 0.00000000


[batch = 1200] train_perplexity =    54.42, train_loss =  4.00, learning_rate = 0.00000000


[batch = 1300] train_perplexity =    54.76, train_loss =  4.00, learning_rate = 0.00000000


[epoch = 100] validation_perplexity =    95.56, validation_loss =  4.56

[epoch = 100] annealing learning_rate = 0.00000000

[batch =  100] train_perplexity =    59.78, train_loss =  4.09, learning_rate = 0.00000000


[batch =  200] train_perplexity =    63.68, train_loss =  4.15, learning_rate = 0.00000000


[batch =  300] train_perplexity =    63.89, train_loss =  4.16, learning_rate = 0.00000000


[batch =  400] train_perplexity =    58.87, train_loss =  4.08, learning_rate = 0.00000000


[batch =  500] train_perplexity =    58.22, train_loss =  4.06, learning_rate = 0.00000000


[batch =  600] train_perplexity =    61.98, train_loss =  4.13, learning_rate = 0.00000000


[batch =  700] train_perplexity =    59.92, train_loss =  4.09, learning_rate = 0.00000000


[batch =  800] train_perplexity =    55.19, train_loss =  4.01, learning_rate = 0.00000000


[batch =  900] train_perplexity =    59.95, train_loss =  4.09, learning_rate = 0.00000000


[batch = 1000] train_perplexity =    63.19, train_loss =  4.15, learning_rate = 0.00000000


[batch = 1100] train_perplexity =    53.56, train_loss =  3.98, learning_rate = 0.00000000


[batch = 1200] train_perplexity =    54.34, train_loss =  4.00, learning_rate = 0.00000000


[batch = 1300] train_perplexity =    54.79, train_loss =  4.00, learning_rate = 0.00000000


[epoch = 101] validation_perplexity =    95.56, validation_loss =  4.56

[epoch = 101] annealing learning_rate = 0.00000000

[batch =  100] train_perplexity =    60.17, train_loss =  4.10, learning_rate = 0.00000000


[batch =  200] train_perplexity =    64.07, train_loss =  4.16, learning_rate = 0.00000000


[batch =  300] train_perplexity =    63.74, train_loss =  4.15, learning_rate = 0.00000000


[batch =  400] train_perplexity =    58.55, train_loss =  4.07, learning_rate = 0.00000000


[batch =  500] train_perplexity =    58.02, train_loss =  4.06, learning_rate = 0.00000000


[batch =  600] train_perplexity =    62.15, train_loss =  4.13, learning_rate = 0.00000000


[batch =  700] train_perplexity =    59.69, train_loss =  4.09, learning_rate = 0.00000000


[batch =  800] train_perplexity =    54.86, train_loss =  4.00, learning_rate = 0.00000000


[batch =  900] train_perplexity =    59.69, train_loss =  4.09, learning_rate = 0.00000000


[batch = 1000] train_perplexity =    62.86, train_loss =  4.14, learning_rate = 0.00000000


[batch = 1100] train_perplexity =    53.65, train_loss =  3.98, learning_rate = 0.00000000


[batch = 1200] train_perplexity =    54.43, train_loss =  4.00, learning_rate = 0.00000000


[batch = 1300] train_perplexity =    54.65, train_loss =  4.00, learning_rate = 0.00000000


[epoch = 102] validation_perplexity =    95.56, validation_loss =  4.56

[epoch = 102] annealing learning_rate = 0.00000000

[batch =  100] train_perplexity =    60.07, train_loss =  4.10, learning_rate = 0.00000000


[batch =  200] train_perplexity =    63.75, train_loss =  4.15, learning_rate = 0.00000000


[batch =  300] train_perplexity =    64.04, train_loss =  4.16, learning_rate = 0.00000000


[batch =  400] train_perplexity =    58.60, train_loss =  4.07, learning_rate = 0.00000000


[batch =  500] train_perplexity =    58.46, train_loss =  4.07, learning_rate = 0.00000000


[batch =  600] train_perplexity =    62.24, train_loss =  4.13, learning_rate = 0.00000000


[batch =  700] train_perplexity =    59.80, train_loss =  4.09, learning_rate = 0.00000000


[batch =  800] train_perplexity =    54.92, train_loss =  4.01, learning_rate = 0.00000000


[batch =  900] train_perplexity =    60.13, train_loss =  4.10, learning_rate = 0.00000000


[batch = 1000] train_perplexity =    62.88, train_loss =  4.14, learning_rate = 0.00000000


[batch = 1100] train_perplexity =    53.67, train_loss =  3.98, learning_rate = 0.00000000


[batch = 1200] train_perplexity =    54.22, train_loss =  3.99, learning_rate = 0.00000000


[batch = 1300] train_perplexity =    54.81, train_loss =  4.00, learning_rate = 0.00000000


[epoch = 103] validation_perplexity =    95.56, validation_loss =  4.56

[epoch = 103] annealing learning_rate = 0.00000000

[batch =  100] train_perplexity =    59.41, train_loss =  4.08, learning_rate = 0.00000000


[batch =  200] train_perplexity =    63.88, train_loss =  4.16, learning_rate = 0.00000000


[batch =  300] train_perplexity =    64.05, train_loss =  4.16, learning_rate = 0.00000000


[batch =  400] train_perplexity =    58.23, train_loss =  4.06, learning_rate = 0.00000000


[batch =  500] train_perplexity =    58.41, train_loss =  4.07, learning_rate = 0.00000000


[batch =  600] train_perplexity =    61.95, train_loss =  4.13, learning_rate = 0.00000000


[batch =  700] train_perplexity =    59.92, train_loss =  4.09, learning_rate = 0.00000000


[batch =  800] train_perplexity =    55.08, train_loss =  4.01, learning_rate = 0.00000000


[batch =  900] train_perplexity =    60.24, train_loss =  4.10, learning_rate = 0.00000000


[batch = 1000] train_perplexity =    62.71, train_loss =  4.14, learning_rate = 0.00000000


[batch = 1100] train_perplexity =    53.80, train_loss =  3.99, learning_rate = 0.00000000


[batch = 1200] train_perplexity =    54.28, train_loss =  3.99, learning_rate = 0.00000000


[batch = 1300] train_perplexity =    54.57, train_loss =  4.00, learning_rate = 0.00000000


[epoch = 104] validation_perplexity =    95.56, validation_loss =  4.56

[epoch = 104] annealing learning_rate = 0.00000000

[batch =  100] train_perplexity =    59.85, train_loss =  4.09, learning_rate = 0.00000000


[batch =  200] train_perplexity =    64.47, train_loss =  4.17, learning_rate = 0.00000000


[batch =  300] train_perplexity =    63.78, train_loss =  4.16, learning_rate = 0.00000000


[batch =  400] train_perplexity =    58.71, train_loss =  4.07, learning_rate = 0.00000000


[batch =  500] train_perplexity =    58.65, train_loss =  4.07, learning_rate = 0.00000000


[batch =  600] train_perplexity =    61.92, train_loss =  4.13, learning_rate = 0.00000000


[batch =  700] train_perplexity =    60.19, train_loss =  4.10, learning_rate = 0.00000000


[batch =  800] train_perplexity =    54.86, train_loss =  4.00, learning_rate = 0.00000000


[batch =  900] train_perplexity =    59.93, train_loss =  4.09, learning_rate = 0.00000000


[batch = 1000] train_perplexity =    62.92, train_loss =  4.14, learning_rate = 0.00000000


[batch = 1100] train_perplexity =    53.69, train_loss =  3.98, learning_rate = 0.00000000


[batch = 1200] train_perplexity =    54.21, train_loss =  3.99, learning_rate = 0.00000000


[batch = 1300] train_perplexity =    54.80, train_loss =  4.00, learning_rate = 0.00000000


[epoch = 105] validation_perplexity =    95.56, validation_loss =  4.56

[epoch = 105] annealing learning_rate = 0.00000000

[batch =  100] train_perplexity =    59.70, train_loss =  4.09, learning_rate = 0.00000000


[batch =  200] train_perplexity =    64.06, train_loss =  4.16, learning_rate = 0.00000000


[batch =  300] train_perplexity =    63.89, train_loss =  4.16, learning_rate = 0.00000000


[batch =  400] train_perplexity =    58.64, train_loss =  4.07, learning_rate = 0.00000000


[batch =  500] train_perplexity =    58.25, train_loss =  4.06, learning_rate = 0.00000000


[batch =  600] train_perplexity =    62.00, train_loss =  4.13, learning_rate = 0.00000000


[batch =  700] train_perplexity =    59.99, train_loss =  4.09, learning_rate = 0.00000000


[batch =  800] train_perplexity =    54.96, train_loss =  4.01, learning_rate = 0.00000000


[batch =  900] train_perplexity =    60.17, train_loss =  4.10, learning_rate = 0.00000000


[batch = 1000] train_perplexity =    62.74, train_loss =  4.14, learning_rate = 0.00000000


[batch = 1100] train_perplexity =    53.65, train_loss =  3.98, learning_rate = 0.00000000


[batch = 1200] train_perplexity =    54.13, train_loss =  3.99, learning_rate = 0.00000000


[batch = 1300] train_perplexity =    54.96, train_loss =  4.01, learning_rate = 0.00000000


[epoch = 106] validation_perplexity =    95.56, validation_loss =  4.56

[epoch = 106] annealing learning_rate = 0.00000000

[batch =  100] train_perplexity =    59.61, train_loss =  4.09, learning_rate = 0.00000000


[batch =  200] train_perplexity =    63.85, train_loss =  4.16, learning_rate = 0.00000000


[batch =  300] train_perplexity =    64.07, train_loss =  4.16, learning_rate = 0.00000000


[batch =  400] train_perplexity =    58.66, train_loss =  4.07, learning_rate = 0.00000000


[batch =  500] train_perplexity =    58.32, train_loss =  4.07, learning_rate = 0.00000000


[batch =  600] train_perplexity =    62.01, train_loss =  4.13, learning_rate = 0.00000000


[batch =  700] train_perplexity =    59.68, train_loss =  4.09, learning_rate = 0.00000000


[batch =  800] train_perplexity =    54.94, train_loss =  4.01, learning_rate = 0.00000000


[batch =  900] train_perplexity =    60.03, train_loss =  4.09, learning_rate = 0.00000000


[batch = 1000] train_perplexity =    62.70, train_loss =  4.14, learning_rate = 0.00000000


[batch = 1100] train_perplexity =    53.70, train_loss =  3.98, learning_rate = 0.00000000


[batch = 1200] train_perplexity =    54.40, train_loss =  4.00, learning_rate = 0.00000000


[batch = 1300] train_perplexity =    54.86, train_loss =  4.00, learning_rate = 0.00000000


[epoch = 107] validation_perplexity =    95.56, validation_loss =  4.56

[epoch = 107] annealing learning_rate = 0.00000000

[batch =  100] train_perplexity =    59.95, train_loss =  4.09, learning_rate = 0.00000000


[batch =  200] train_perplexity =    64.23, train_loss =  4.16, learning_rate = 0.00000000


[batch =  300] train_perplexity =    63.76, train_loss =  4.16, learning_rate = 0.00000000


[batch =  400] train_perplexity =    58.58, train_loss =  4.07, learning_rate = 0.00000000


[batch =  500] train_perplexity =    58.39, train_loss =  4.07, learning_rate = 0.00000000


[batch =  600] train_perplexity =    62.18, train_loss =  4.13, learning_rate = 0.00000000


[batch =  700] train_perplexity =    59.85, train_loss =  4.09, learning_rate = 0.00000000


[batch =  800] train_perplexity =    54.86, train_loss =  4.00, learning_rate = 0.00000000


[batch =  900] train_perplexity =    59.90, train_loss =  4.09, learning_rate = 0.00000000


[batch = 1000] train_perplexity =    63.02, train_loss =  4.14, learning_rate = 0.00000000


[batch = 1100] train_perplexity =    53.84, train_loss =  3.99, learning_rate = 0.00000000


[batch = 1200] train_perplexity =    54.38, train_loss =  4.00, learning_rate = 0.00000000


[batch = 1300] train_perplexity =    54.92, train_loss =  4.01, learning_rate = 0.00000000


[epoch = 108] validation_perplexity =    95.56, validation_loss =  4.56

[epoch = 108] annealing learning_rate = 0.00000000

[batch =  100] train_perplexity =    59.61, train_loss =  4.09, learning_rate = 0.00000000


[batch =  200] train_perplexity =    63.96, train_loss =  4.16, learning_rate = 0.00000000


[batch =  300] train_perplexity =    64.15, train_loss =  4.16, learning_rate = 0.00000000


[batch =  400] train_perplexity =    58.66, train_loss =  4.07, learning_rate = 0.00000000


[batch =  500] train_perplexity =    58.10, train_loss =  4.06, learning_rate = 0.00000000


[batch =  600] train_perplexity =    61.86, train_loss =  4.12, learning_rate = 0.00000000


[batch =  700] train_perplexity =    60.01, train_loss =  4.09, learning_rate = 0.00000000


[batch =  800] train_perplexity =    54.93, train_loss =  4.01, learning_rate = 0.00000000


[batch =  900] train_perplexity =    59.82, train_loss =  4.09, learning_rate = 0.00000000


[batch = 1000] train_perplexity =    62.99, train_loss =  4.14, learning_rate = 0.00000000


[batch = 1100] train_perplexity =    53.96, train_loss =  3.99, learning_rate = 0.00000000


[batch = 1200] train_perplexity =    54.29, train_loss =  3.99, learning_rate = 0.00000000


[batch = 1300] train_perplexity =    54.80, train_loss =  4.00, learning_rate = 0.00000000


[epoch = 109] validation_perplexity =    95.56, validation_loss =  4.56

[epoch = 109] annealing learning_rate = 0.00000000

[batch =  100] train_perplexity =    59.62, train_loss =  4.09, learning_rate = 0.00000000


[batch =  200] train_perplexity =    64.05, train_loss =  4.16, learning_rate = 0.00000000


[batch =  300] train_perplexity =    63.63, train_loss =  4.15, learning_rate = 0.00000000


[batch =  400] train_perplexity =    58.61, train_loss =  4.07, learning_rate = 0.00000000


[batch =  500] train_perplexity =    58.30, train_loss =  4.07, learning_rate = 0.00000000


[batch =  600] train_perplexity =    61.97, train_loss =  4.13, learning_rate = 0.00000000


[batch =  700] train_perplexity =    59.68, train_loss =  4.09, learning_rate = 0.00000000


[batch =  800] train_perplexity =    54.90, train_loss =  4.01, learning_rate = 0.00000000


[batch =  900] train_perplexity =    60.24, train_loss =  4.10, learning_rate = 0.00000000


[batch = 1000] train_perplexity =    63.00, train_loss =  4.14, learning_rate = 0.00000000


[batch = 1100] train_perplexity =    53.82, train_loss =  3.99, learning_rate = 0.00000000


[batch = 1200] train_perplexity =    54.35, train_loss =  4.00, learning_rate = 0.00000000


[batch = 1300] train_perplexity =    54.85, train_loss =  4.00, learning_rate = 0.00000000


[epoch = 110] validation_perplexity =    95.56, validation_loss =  4.56

[epoch = 110] annealing learning_rate = 0.00000000

[batch =  100] train_perplexity =    59.48, train_loss =  4.09, learning_rate = 0.00000000


[batch =  200] train_perplexity =    63.92, train_loss =  4.16, learning_rate = 0.00000000


[batch =  300] train_perplexity =    63.81, train_loss =  4.16, learning_rate = 0.00000000


[batch =  400] train_perplexity =    58.49, train_loss =  4.07, learning_rate = 0.00000000


[batch =  500] train_perplexity =    58.36, train_loss =  4.07, learning_rate = 0.00000000


[batch =  600] train_perplexity =    62.25, train_loss =  4.13, learning_rate = 0.00000000


[batch =  700] train_perplexity =    60.04, train_loss =  4.10, learning_rate = 0.00000000


[batch =  800] train_perplexity =    54.98, train_loss =  4.01, learning_rate = 0.00000000


[batch =  900] train_perplexity =    60.04, train_loss =  4.09, learning_rate = 0.00000000


[batch = 1000] train_perplexity =    63.03, train_loss =  4.14, learning_rate = 0.00000000


[batch = 1100] train_perplexity =    53.34, train_loss =  3.98, learning_rate = 0.00000000


[batch = 1200] train_perplexity =    54.37, train_loss =  4.00, learning_rate = 0.00000000


[batch = 1300] train_perplexity =    54.61, train_loss =  4.00, learning_rate = 0.00000000


[epoch = 111] validation_perplexity =    95.56, validation_loss =  4.56

[epoch = 111] annealing learning_rate = 0.00000000

[batch =  100] train_perplexity =    59.71, train_loss =  4.09, learning_rate = 0.00000000


[batch =  200] train_perplexity =    63.80, train_loss =  4.16, learning_rate = 0.00000000


[batch =  300] train_perplexity =    63.55, train_loss =  4.15, learning_rate = 0.00000000


[batch =  400] train_perplexity =    58.40, train_loss =  4.07, learning_rate = 0.00000000


[batch =  500] train_perplexity =    58.18, train_loss =  4.06, learning_rate = 0.00000000


[batch =  600] train_perplexity =    62.13, train_loss =  4.13, learning_rate = 0.00000000


[batch =  700] train_perplexity =    59.71, train_loss =  4.09, learning_rate = 0.00000000


[batch =  800] train_perplexity =    54.59, train_loss =  4.00, learning_rate = 0.00000000


[batch =  900] train_perplexity =    60.30, train_loss =  4.10, learning_rate = 0.00000000


[batch = 1000] train_perplexity =    62.62, train_loss =  4.14, learning_rate = 0.00000000


[batch = 1100] train_perplexity =    53.68, train_loss =  3.98, learning_rate = 0.00000000


[batch = 1200] train_perplexity =    54.58, train_loss =  4.00, learning_rate = 0.00000000


[batch = 1300] train_perplexity =    54.80, train_loss =  4.00, learning_rate = 0.00000000


[epoch = 112] validation_perplexity =    95.56, validation_loss =  4.56

[epoch = 112] annealing learning_rate = 0.00000000

[batch =  100] train_perplexity =    59.76, train_loss =  4.09, learning_rate = 0.00000000


[batch =  200] train_perplexity =    64.18, train_loss =  4.16, learning_rate = 0.00000000


[batch =  300] train_perplexity =    63.94, train_loss =  4.16, learning_rate = 0.00000000


[batch =  400] train_perplexity =    58.55, train_loss =  4.07, learning_rate = 0.00000000


[batch =  500] train_perplexity =    58.38, train_loss =  4.07, learning_rate = 0.00000000


[batch =  600] train_perplexity =    62.17, train_loss =  4.13, learning_rate = 0.00000000


[batch =  700] train_perplexity =    60.01, train_loss =  4.09, learning_rate = 0.00000000


[batch =  800] train_perplexity =    55.35, train_loss =  4.01, learning_rate = 0.00000000


[batch =  900] train_perplexity =    59.85, train_loss =  4.09, learning_rate = 0.00000000


[batch = 1000] train_perplexity =    62.79, train_loss =  4.14, learning_rate = 0.00000000


[batch = 1100] train_perplexity =    53.93, train_loss =  3.99, learning_rate = 0.00000000


[batch = 1200] train_perplexity =    54.30, train_loss =  3.99, learning_rate = 0.00000000


[batch = 1300] train_perplexity =    55.07, train_loss =  4.01, learning_rate = 0.00000000


[epoch = 113] validation_perplexity =    95.56, validation_loss =  4.56

[epoch = 113] annealing learning_rate = 0.00000000

[batch =  100] train_perplexity =    59.94, train_loss =  4.09, learning_rate = 0.00000000


[batch =  200] train_perplexity =    64.31, train_loss =  4.16, learning_rate = 0.00000000


[batch =  300] train_perplexity =    63.81, train_loss =  4.16, learning_rate = 0.00000000


[batch =  400] train_perplexity =    58.63, train_loss =  4.07, learning_rate = 0.00000000


[batch =  500] train_perplexity =    58.07, train_loss =  4.06, learning_rate = 0.00000000


[batch =  600] train_perplexity =    61.87, train_loss =  4.13, learning_rate = 0.00000000


[batch =  700] train_perplexity =    60.23, train_loss =  4.10, learning_rate = 0.00000000


[batch =  800] train_perplexity =    54.76, train_loss =  4.00, learning_rate = 0.00000000


[batch =  900] train_perplexity =    59.91, train_loss =  4.09, learning_rate = 0.00000000


[batch = 1000] train_perplexity =    62.82, train_loss =  4.14, learning_rate = 0.00000000


[batch = 1100] train_perplexity =    53.42, train_loss =  3.98, learning_rate = 0.00000000


[batch = 1200] train_perplexity =    54.35, train_loss =  4.00, learning_rate = 0.00000000


[batch = 1300] train_perplexity =    54.63, train_loss =  4.00, learning_rate = 0.00000000


[epoch = 114] validation_perplexity =    95.56, validation_loss =  4.56

[epoch = 114] annealing learning_rate = 0.00000000

[batch =  100] train_perplexity =    59.51, train_loss =  4.09, learning_rate = 0.00000000


[batch =  200] train_perplexity =    64.22, train_loss =  4.16, learning_rate = 0.00000000


[batch =  300] train_perplexity =    63.89, train_loss =  4.16, learning_rate = 0.00000000


[batch =  400] train_perplexity =    58.71, train_loss =  4.07, learning_rate = 0.00000000


[batch =  500] train_perplexity =    58.18, train_loss =  4.06, learning_rate = 0.00000000


[batch =  600] train_perplexity =    61.74, train_loss =  4.12, learning_rate = 0.00000000


[batch =  700] train_perplexity =    59.75, train_loss =  4.09, learning_rate = 0.00000000


[batch =  800] train_perplexity =    54.90, train_loss =  4.01, learning_rate = 0.00000000


[batch =  900] train_perplexity =    60.06, train_loss =  4.10, learning_rate = 0.00000000


[batch = 1000] train_perplexity =    62.76, train_loss =  4.14, learning_rate = 0.00000000


[batch = 1100] train_perplexity =    53.82, train_loss =  3.99, learning_rate = 0.00000000


[batch = 1200] train_perplexity =    54.55, train_loss =  4.00, learning_rate = 0.00000000


[batch = 1300] train_perplexity =    54.43, train_loss =  4.00, learning_rate = 0.00000000


[epoch = 115] validation_perplexity =    95.56, validation_loss =  4.56

[epoch = 115] annealing learning_rate = 0.00000000

[batch =  100] train_perplexity =    60.14, train_loss =  4.10, learning_rate = 0.00000000


[batch =  200] train_perplexity =    63.97, train_loss =  4.16, learning_rate = 0.00000000


[batch =  300] train_perplexity =    63.55, train_loss =  4.15, learning_rate = 0.00000000


[batch =  400] train_perplexity =    58.57, train_loss =  4.07, learning_rate = 0.00000000


[batch =  500] train_perplexity =    58.20, train_loss =  4.06, learning_rate = 0.00000000


[batch =  600] train_perplexity =    62.12, train_loss =  4.13, learning_rate = 0.00000000


[batch =  700] train_perplexity =    59.97, train_loss =  4.09, learning_rate = 0.00000000


[batch =  800] train_perplexity =    54.73, train_loss =  4.00, learning_rate = 0.00000000


[batch =  900] train_perplexity =    60.08, train_loss =  4.10, learning_rate = 0.00000000


[batch = 1000] train_perplexity =    62.88, train_loss =  4.14, learning_rate = 0.00000000


[batch = 1100] train_perplexity =    53.86, train_loss =  3.99, learning_rate = 0.00000000


[batch = 1200] train_perplexity =    54.33, train_loss =  4.00, learning_rate = 0.00000000


[batch = 1300] train_perplexity =    54.62, train_loss =  4.00, learning_rate = 0.00000000


[epoch = 116] validation_perplexity =    95.56, validation_loss =  4.56

[epoch = 116] annealing learning_rate = 0.00000000

[batch =  100] train_perplexity =    59.53, train_loss =  4.09, learning_rate = 0.00000000


[batch =  200] train_perplexity =    64.23, train_loss =  4.16, learning_rate = 0.00000000


[batch =  300] train_perplexity =    63.95, train_loss =  4.16, learning_rate = 0.00000000


[batch =  400] train_perplexity =    58.70, train_loss =  4.07, learning_rate = 0.00000000


[batch =  500] train_perplexity =    58.29, train_loss =  4.07, learning_rate = 0.00000000


[batch =  600] train_perplexity =    62.07, train_loss =  4.13, learning_rate = 0.00000000


[batch =  700] train_perplexity =    60.05, train_loss =  4.10, learning_rate = 0.00000000


[batch =  800] train_perplexity =    55.14, train_loss =  4.01, learning_rate = 0.00000000


[batch =  900] train_perplexity =    59.95, train_loss =  4.09, learning_rate = 0.00000000


[batch = 1000] train_perplexity =    62.79, train_loss =  4.14, learning_rate = 0.00000000


[batch = 1100] train_perplexity =    54.10, train_loss =  3.99, learning_rate = 0.00000000


[batch = 1200] train_perplexity =    54.56, train_loss =  4.00, learning_rate = 0.00000000


[batch = 1300] train_perplexity =    54.75, train_loss =  4.00, learning_rate = 0.00000000


[epoch = 117] validation_perplexity =    95.56, validation_loss =  4.56

[epoch = 117] annealing learning_rate = 0.00000000

[batch =  100] train_perplexity =    59.88, train_loss =  4.09, learning_rate = 0.00000000


[batch =  200] train_perplexity =    63.74, train_loss =  4.15, learning_rate = 0.00000000


[batch =  300] train_perplexity =    63.99, train_loss =  4.16, learning_rate = 0.00000000


[batch =  400] train_perplexity =    58.66, train_loss =  4.07, learning_rate = 0.00000000


[batch =  500] train_perplexity =    58.13, train_loss =  4.06, learning_rate = 0.00000000


[batch =  600] train_perplexity =    62.27, train_loss =  4.13, learning_rate = 0.00000000


[batch =  700] train_perplexity =    59.90, train_loss =  4.09, learning_rate = 0.00000000


[batch =  800] train_perplexity =    55.03, train_loss =  4.01, learning_rate = 0.00000000


[batch =  900] train_perplexity =    59.90, train_loss =  4.09, learning_rate = 0.00000000


[batch = 1000] train_perplexity =    62.96, train_loss =  4.14, learning_rate = 0.00000000


[batch = 1100] train_perplexity =    53.73, train_loss =  3.98, learning_rate = 0.00000000


[batch = 1200] train_perplexity =    54.44, train_loss =  4.00, learning_rate = 0.00000000


[batch = 1300] train_perplexity =    54.87, train_loss =  4.00, learning_rate = 0.00000000


[epoch = 118] validation_perplexity =    95.56, validation_loss =  4.56

[epoch = 118] annealing learning_rate = 0.00000000

[batch =  100] train_perplexity =    59.86, train_loss =  4.09, learning_rate = 0.00000000


[batch =  200] train_perplexity =    63.97, train_loss =  4.16, learning_rate = 0.00000000


[batch =  300] train_perplexity =    63.69, train_loss =  4.15, learning_rate = 0.00000000


[batch =  400] train_perplexity =    58.67, train_loss =  4.07, learning_rate = 0.00000000


[batch =  500] train_perplexity =    58.43, train_loss =  4.07, learning_rate = 0.00000000


[batch =  600] train_perplexity =    62.16, train_loss =  4.13, learning_rate = 0.00000000


[batch =  700] train_perplexity =    60.11, train_loss =  4.10, learning_rate = 0.00000000


[batch =  800] train_perplexity =    55.05, train_loss =  4.01, learning_rate = 0.00000000


[batch =  900] train_perplexity =    59.81, train_loss =  4.09, learning_rate = 0.00000000


[batch = 1000] train_perplexity =    62.98, train_loss =  4.14, learning_rate = 0.00000000


[batch = 1100] train_perplexity =    53.88, train_loss =  3.99, learning_rate = 0.00000000


[batch = 1200] train_perplexity =    54.03, train_loss =  3.99, learning_rate = 0.00000000


[batch = 1300] train_perplexity =    54.82, train_loss =  4.00, learning_rate = 0.00000000


[epoch = 119] validation_perplexity =    95.56, validation_loss =  4.56

[epoch = 119] annealing learning_rate = 0.00000000

[batch =  100] train_perplexity =    59.66, train_loss =  4.09, learning_rate = 0.00000000


[batch =  200] train_perplexity =    64.25, train_loss =  4.16, learning_rate = 0.00000000


[batch =  300] train_perplexity =    63.99, train_loss =  4.16, learning_rate = 0.00000000


[batch =  400] train_perplexity =    58.58, train_loss =  4.07, learning_rate = 0.00000000


[batch =  500] train_perplexity =    58.22, train_loss =  4.06, learning_rate = 0.00000000


[batch =  600] train_perplexity =    62.00, train_loss =  4.13, learning_rate = 0.00000000


[batch =  700] train_perplexity =    59.80, train_loss =  4.09, learning_rate = 0.00000000


[batch =  800] train_perplexity =    54.85, train_loss =  4.00, learning_rate = 0.00000000


[batch =  900] train_perplexity =    60.08, train_loss =  4.10, learning_rate = 0.00000000


[batch = 1000] train_perplexity =    62.88, train_loss =  4.14, learning_rate = 0.00000000


[batch = 1100] train_perplexity =    53.71, train_loss =  3.98, learning_rate = 0.00000000


[batch = 1200] train_perplexity =    54.39, train_loss =  4.00, learning_rate = 0.00000000


[batch = 1300] train_perplexity =    54.66, train_loss =  4.00, learning_rate = 0.00000000


[epoch = 120] validation_perplexity =    95.56, validation_loss =  4.56

[epoch = 120] annealing learning_rate = 0.00000000

[batch =  100] train_perplexity =    59.97, train_loss =  4.09, learning_rate = 0.00000000


[batch =  200] train_perplexity =    63.88, train_loss =  4.16, learning_rate = 0.00000000


[batch =  300] train_perplexity =    63.86, train_loss =  4.16, learning_rate = 0.00000000


[batch =  400] train_perplexity =    58.36, train_loss =  4.07, learning_rate = 0.00000000


[batch =  500] train_perplexity =    58.43, train_loss =  4.07, learning_rate = 0.00000000


[batch =  600] train_perplexity =    62.35, train_loss =  4.13, learning_rate = 0.00000000


[batch =  700] train_perplexity =    59.90, train_loss =  4.09, learning_rate = 0.00000000


[batch =  800] train_perplexity =    55.02, train_loss =  4.01, learning_rate = 0.00000000


[batch =  900] train_perplexity =    60.35, train_loss =  4.10, learning_rate = 0.00000000


[batch = 1000] train_perplexity =    62.95, train_loss =  4.14, learning_rate = 0.00000000


[batch = 1100] train_perplexity =    53.47, train_loss =  3.98, learning_rate = 0.00000000


[batch = 1200] train_perplexity =    54.50, train_loss =  4.00, learning_rate = 0.00000000


[batch = 1300] train_perplexity =    54.76, train_loss =  4.00, learning_rate = 0.00000000


[epoch = 121] validation_perplexity =    95.56, validation_loss =  4.56

[epoch = 121] annealing learning_rate = 0.00000000

[batch =  100] train_perplexity =    59.78, train_loss =  4.09, learning_rate = 0.00000000


[batch =  200] train_perplexity =    64.13, train_loss =  4.16, learning_rate = 0.00000000


[batch =  300] train_perplexity =    63.51, train_loss =  4.15, learning_rate = 0.00000000


[batch =  400] train_perplexity =    58.82, train_loss =  4.07, learning_rate = 0.00000000


[batch =  500] train_perplexity =    58.51, train_loss =  4.07, learning_rate = 0.00000000


[batch =  600] train_perplexity =    62.09, train_loss =  4.13, learning_rate = 0.00000000


[batch =  700] train_perplexity =    59.82, train_loss =  4.09, learning_rate = 0.00000000


[batch =  800] train_perplexity =    54.97, train_loss =  4.01, learning_rate = 0.00000000


[batch =  900] train_perplexity =    59.89, train_loss =  4.09, learning_rate = 0.00000000


[batch = 1000] train_perplexity =    62.95, train_loss =  4.14, learning_rate = 0.00000000


[batch = 1100] train_perplexity =    53.75, train_loss =  3.98, learning_rate = 0.00000000


[batch = 1200] train_perplexity =    54.35, train_loss =  4.00, learning_rate = 0.00000000


[batch = 1300] train_perplexity =    54.90, train_loss =  4.01, learning_rate = 0.00000000


[epoch = 122] validation_perplexity =    95.56, validation_loss =  4.56

[epoch = 122] annealing learning_rate = 0.00000000

[batch =  100] train_perplexity =    59.75, train_loss =  4.09, learning_rate = 0.00000000


[batch =  200] train_perplexity =    63.81, train_loss =  4.16, learning_rate = 0.00000000


[batch =  300] train_perplexity =    64.05, train_loss =  4.16, learning_rate = 0.00000000


[batch =  400] train_perplexity =    58.48, train_loss =  4.07, learning_rate = 0.00000000


[batch =  500] train_perplexity =    58.43, train_loss =  4.07, learning_rate = 0.00000000


[batch =  600] train_perplexity =    62.06, train_loss =  4.13, learning_rate = 0.00000000


[batch =  700] train_perplexity =    59.61, train_loss =  4.09, learning_rate = 0.00000000


[batch =  800] train_perplexity =    55.06, train_loss =  4.01, learning_rate = 0.00000000


[batch =  900] train_perplexity =    60.14, train_loss =  4.10, learning_rate = 0.00000000


[batch = 1000] train_perplexity =    62.71, train_loss =  4.14, learning_rate = 0.00000000


[batch = 1100] train_perplexity =    53.76, train_loss =  3.98, learning_rate = 0.00000000


[batch = 1200] train_perplexity =    54.29, train_loss =  3.99, learning_rate = 0.00000000


[batch = 1300] train_perplexity =    54.69, train_loss =  4.00, learning_rate = 0.00000000


[epoch = 123] validation_perplexity =    95.56, validation_loss =  4.56

[epoch = 123] annealing learning_rate = 0.00000000

[batch =  100] train_perplexity =    59.55, train_loss =  4.09, learning_rate = 0.00000000


[batch =  200] train_perplexity =    63.93, train_loss =  4.16, learning_rate = 0.00000000


[batch =  300] train_perplexity =    63.46, train_loss =  4.15, learning_rate = 0.00000000


[batch =  400] train_perplexity =    58.62, train_loss =  4.07, learning_rate = 0.00000000


[batch =  500] train_perplexity =    58.31, train_loss =  4.07, learning_rate = 0.00000000


[batch =  600] train_perplexity =    61.87, train_loss =  4.13, learning_rate = 0.00000000


[batch =  700] train_perplexity =    59.62, train_loss =  4.09, learning_rate = 0.00000000


[batch =  800] train_perplexity =    55.04, train_loss =  4.01, learning_rate = 0.00000000


[batch =  900] train_perplexity =    59.93, train_loss =  4.09, learning_rate = 0.00000000


[batch = 1000] train_perplexity =    62.81, train_loss =  4.14, learning_rate = 0.00000000


[batch = 1100] train_perplexity =    53.72, train_loss =  3.98, learning_rate = 0.00000000


[batch = 1200] train_perplexity =    54.38, train_loss =  4.00, learning_rate = 0.00000000


[batch = 1300] train_perplexity =    54.79, train_loss =  4.00, learning_rate = 0.00000000


[epoch = 124] validation_perplexity =    95.56, validation_loss =  4.56

[epoch = 124] annealing learning_rate = 0.00000000

[batch =  100] train_perplexity =    59.77, train_loss =  4.09, learning_rate = 0.00000000


[batch =  200] train_perplexity =    63.94, train_loss =  4.16, learning_rate = 0.00000000


[batch =  300] train_perplexity =    64.20, train_loss =  4.16, learning_rate = 0.00000000


[batch =  400] train_perplexity =    58.73, train_loss =  4.07, learning_rate = 0.00000000


[batch =  500] train_perplexity =    58.45, train_loss =  4.07, learning_rate = 0.00000000


[batch =  600] train_perplexity =    62.40, train_loss =  4.13, learning_rate = 0.00000000


[batch =  700] train_perplexity =    59.55, train_loss =  4.09, learning_rate = 0.00000000


[batch =  800] train_perplexity =    55.03, train_loss =  4.01, learning_rate = 0.00000000


[batch =  900] train_perplexity =    60.08, train_loss =  4.10, learning_rate = 0.00000000


[batch = 1000] train_perplexity =    63.05, train_loss =  4.14, learning_rate = 0.00000000


[batch = 1100] train_perplexity =    53.35, train_loss =  3.98, learning_rate = 0.00000000


[batch = 1200] train_perplexity =    54.36, train_loss =  4.00, learning_rate = 0.00000000


[batch = 1300] train_perplexity =    55.09, train_loss =  4.01, learning_rate = 0.00000000


[epoch = 125] validation_perplexity =    95.56, validation_loss =  4.56

[epoch = 125] annealing learning_rate = 0.00000000

[batch =  100] train_perplexity =    59.92, train_loss =  4.09, learning_rate = 0.00000000


[batch =  200] train_perplexity =    64.02, train_loss =  4.16, learning_rate = 0.00000000


[batch =  300] train_perplexity =    63.72, train_loss =  4.15, learning_rate = 0.00000000


[batch =  400] train_perplexity =    58.43, train_loss =  4.07, learning_rate = 0.00000000


[batch =  500] train_perplexity =    58.09, train_loss =  4.06, learning_rate = 0.00000000


[batch =  600] train_perplexity =    62.09, train_loss =  4.13, learning_rate = 0.00000000


[batch =  700] train_perplexity =    59.98, train_loss =  4.09, learning_rate = 0.00000000


[batch =  800] train_perplexity =    54.95, train_loss =  4.01, learning_rate = 0.00000000


[batch =  900] train_perplexity =    60.21, train_loss =  4.10, learning_rate = 0.00000000


[batch = 1000] train_perplexity =    62.62, train_loss =  4.14, learning_rate = 0.00000000


[batch = 1100] train_perplexity =    54.01, train_loss =  3.99, learning_rate = 0.00000000


[batch = 1200] train_perplexity =    54.24, train_loss =  3.99, learning_rate = 0.00000000


[batch = 1300] train_perplexity =    54.74, train_loss =  4.00, learning_rate = 0.00000000


[epoch = 126] validation_perplexity =    95.56, validation_loss =  4.56

[epoch = 126] annealing learning_rate = 0.00000000

[batch =  100] train_perplexity =    59.73, train_loss =  4.09, learning_rate = 0.00000000


[batch =  200] train_perplexity =    64.39, train_loss =  4.16, learning_rate = 0.00000000


[batch =  300] train_perplexity =    63.61, train_loss =  4.15, learning_rate = 0.00000000


[batch =  400] train_perplexity =    58.73, train_loss =  4.07, learning_rate = 0.00000000


[batch =  500] train_perplexity =    58.19, train_loss =  4.06, learning_rate = 0.00000000


[batch =  600] train_perplexity =    61.95, train_loss =  4.13, learning_rate = 0.00000000


[batch =  700] train_perplexity =    60.00, train_loss =  4.09, learning_rate = 0.00000000


[batch =  800] train_perplexity =    55.03, train_loss =  4.01, learning_rate = 0.00000000


[batch =  900] train_perplexity =    60.17, train_loss =  4.10, learning_rate = 0.00000000


[batch = 1000] train_perplexity =    62.94, train_loss =  4.14, learning_rate = 0.00000000


[batch = 1100] train_perplexity =    53.84, train_loss =  3.99, learning_rate = 0.00000000


[batch = 1200] train_perplexity =    54.70, train_loss =  4.00, learning_rate = 0.00000000


[batch = 1300] train_perplexity =    54.99, train_loss =  4.01, learning_rate = 0.00000000


[epoch = 127] validation_perplexity =    95.56, validation_loss =  4.56

[epoch = 127] annealing learning_rate = 0.00000000

[batch =  100] train_perplexity =    59.63, train_loss =  4.09, learning_rate = 0.00000000


[batch =  200] train_perplexity =    64.09, train_loss =  4.16, learning_rate = 0.00000000


[batch =  300] train_perplexity =    64.00, train_loss =  4.16, learning_rate = 0.00000000


[batch =  400] train_perplexity =    58.26, train_loss =  4.06, learning_rate = 0.00000000


[batch =  500] train_perplexity =    58.44, train_loss =  4.07, learning_rate = 0.00000000


[batch =  600] train_perplexity =    61.79, train_loss =  4.12, learning_rate = 0.00000000


[batch =  700] train_perplexity =    59.70, train_loss =  4.09, learning_rate = 0.00000000


[batch =  800] train_perplexity =    54.95, train_loss =  4.01, learning_rate = 0.00000000


[batch =  900] train_perplexity =    60.08, train_loss =  4.10, learning_rate = 0.00000000


[batch = 1000] train_perplexity =    63.03, train_loss =  4.14, learning_rate = 0.00000000


[batch = 1100] train_perplexity =    53.87, train_loss =  3.99, learning_rate = 0.00000000


[batch = 1200] train_perplexity =    54.45, train_loss =  4.00, learning_rate = 0.00000000


[batch = 1300] train_perplexity =    54.70, train_loss =  4.00, learning_rate = 0.00000000


[epoch = 128] validation_perplexity =    95.56, validation_loss =  4.56

[epoch = 128] annealing learning_rate = 0.00000000

[batch =  100] train_perplexity =    59.66, train_loss =  4.09, learning_rate = 0.00000000


[batch =  200] train_perplexity =    64.17, train_loss =  4.16, learning_rate = 0.00000000


[batch =  300] train_perplexity =    64.08, train_loss =  4.16, learning_rate = 0.00000000


[batch =  400] train_perplexity =    58.44, train_loss =  4.07, learning_rate = 0.00000000


[batch =  500] train_perplexity =    58.24, train_loss =  4.06, learning_rate = 0.00000000


[batch =  600] train_perplexity =    62.13, train_loss =  4.13, learning_rate = 0.00000000


[batch =  700] train_perplexity =    59.74, train_loss =  4.09, learning_rate = 0.00000000


[batch =  800] train_perplexity =    55.23, train_loss =  4.01, learning_rate = 0.00000000


[batch =  900] train_perplexity =    59.96, train_loss =  4.09, learning_rate = 0.00000000


[batch = 1000] train_perplexity =    63.16, train_loss =  4.15, learning_rate = 0.00000000


[batch = 1100] train_perplexity =    53.91, train_loss =  3.99, learning_rate = 0.00000000


[batch = 1200] train_perplexity =    54.69, train_loss =  4.00, learning_rate = 0.00000000


[batch = 1300] train_perplexity =    54.77, train_loss =  4.00, learning_rate = 0.00000000


[epoch = 129] validation_perplexity =    95.56, validation_loss =  4.56

[epoch = 129] annealing learning_rate = 0.00000000

[batch =  100] train_perplexity =    59.63, train_loss =  4.09, learning_rate = 0.00000000


[batch =  200] train_perplexity =    64.21, train_loss =  4.16, learning_rate = 0.00000000


[batch =  300] train_perplexity =    64.35, train_loss =  4.16, learning_rate = 0.00000000


[batch =  400] train_perplexity =    58.52, train_loss =  4.07, learning_rate = 0.00000000


[batch =  500] train_perplexity =    58.16, train_loss =  4.06, learning_rate = 0.00000000


[batch =  600] train_perplexity =    61.98, train_loss =  4.13, learning_rate = 0.00000000


[batch =  700] train_perplexity =    59.82, train_loss =  4.09, learning_rate = 0.00000000


[batch =  800] train_perplexity =    54.99, train_loss =  4.01, learning_rate = 0.00000000


[batch =  900] train_perplexity =    60.22, train_loss =  4.10, learning_rate = 0.00000000


[batch = 1000] train_perplexity =    62.68, train_loss =  4.14, learning_rate = 0.00000000


[batch = 1100] train_perplexity =    53.64, train_loss =  3.98, learning_rate = 0.00000000


[batch = 1200] train_perplexity =    54.31, train_loss =  3.99, learning_rate = 0.00000000


[batch = 1300] train_perplexity =    54.64, train_loss =  4.00, learning_rate = 0.00000000


[epoch = 130] validation_perplexity =    95.56, validation_loss =  4.56

[epoch = 130] annealing learning_rate = 0.00000000

[batch =  100] train_perplexity =    59.79, train_loss =  4.09, learning_rate = 0.00000000


[batch =  200] train_perplexity =    63.94, train_loss =  4.16, learning_rate = 0.00000000


[batch =  300] train_perplexity =    63.96, train_loss =  4.16, learning_rate = 0.00000000


[batch =  400] train_perplexity =    58.74, train_loss =  4.07, learning_rate = 0.00000000


[batch =  500] train_perplexity =    58.21, train_loss =  4.06, learning_rate = 0.00000000


[batch =  600] train_perplexity =    62.30, train_loss =  4.13, learning_rate = 0.00000000


[batch =  700] train_perplexity =    59.76, train_loss =  4.09, learning_rate = 0.00000000


[batch =  800] train_perplexity =    54.94, train_loss =  4.01, learning_rate = 0.00000000


[batch =  900] train_perplexity =    60.06, train_loss =  4.10, learning_rate = 0.00000000


[batch = 1000] train_perplexity =    63.13, train_loss =  4.15, learning_rate = 0.00000000


[batch = 1100] train_perplexity =    53.77, train_loss =  3.98, learning_rate = 0.00000000


[batch = 1200] train_perplexity =    54.22, train_loss =  3.99, learning_rate = 0.00000000


[batch = 1300] train_perplexity =    54.89, train_loss =  4.01, learning_rate = 0.00000000


[epoch = 131] validation_perplexity =    95.56, validation_loss =  4.56

[epoch = 131] annealing learning_rate = 0.00000000

[batch =  100] train_perplexity =    59.88, train_loss =  4.09, learning_rate = 0.00000000


[batch =  200] train_perplexity =    64.14, train_loss =  4.16, learning_rate = 0.00000000


[batch =  300] train_perplexity =    64.09, train_loss =  4.16, learning_rate = 0.00000000


[batch =  400] train_perplexity =    58.55, train_loss =  4.07, learning_rate = 0.00000000


[batch =  500] train_perplexity =    58.13, train_loss =  4.06, learning_rate = 0.00000000


[batch =  600] train_perplexity =    61.82, train_loss =  4.12, learning_rate = 0.00000000


[batch =  700] train_perplexity =    59.70, train_loss =  4.09, learning_rate = 0.00000000


[batch =  800] train_perplexity =    54.84, train_loss =  4.00, learning_rate = 0.00000000


[batch =  900] train_perplexity =    59.87, train_loss =  4.09, learning_rate = 0.00000000


[batch = 1000] train_perplexity =    62.76, train_loss =  4.14, learning_rate = 0.00000000


[batch = 1100] train_perplexity =    53.74, train_loss =  3.98, learning_rate = 0.00000000


[batch = 1200] train_perplexity =    54.39, train_loss =  4.00, learning_rate = 0.00000000


[batch = 1300] train_perplexity =    54.52, train_loss =  4.00, learning_rate = 0.00000000


[epoch = 132] validation_perplexity =    95.56, validation_loss =  4.56

[epoch = 132] annealing learning_rate = 0.00000000

[batch =  100] train_perplexity =    60.00, train_loss =  4.09, learning_rate = 0.00000000


[batch =  200] train_perplexity =    63.82, train_loss =  4.16, learning_rate = 0.00000000


[batch =  300] train_perplexity =    64.15, train_loss =  4.16, learning_rate = 0.00000000


[batch =  400] train_perplexity =    58.80, train_loss =  4.07, learning_rate = 0.00000000


[batch =  500] train_perplexity =    58.26, train_loss =  4.06, learning_rate = 0.00000000


[batch =  600] train_perplexity =    62.18, train_loss =  4.13, learning_rate = 0.00000000


[batch =  700] train_perplexity =    59.92, train_loss =  4.09, learning_rate = 0.00000000


[batch =  800] train_perplexity =    55.00, train_loss =  4.01, learning_rate = 0.00000000


[batch =  900] train_perplexity =    60.05, train_loss =  4.10, learning_rate = 0.00000000


[batch = 1000] train_perplexity =    63.15, train_loss =  4.15, learning_rate = 0.00000000


[batch = 1100] train_perplexity =    53.71, train_loss =  3.98, learning_rate = 0.00000000


[batch = 1200] train_perplexity =    54.35, train_loss =  4.00, learning_rate = 0.00000000


[batch = 1300] train_perplexity =    54.97, train_loss =  4.01, learning_rate = 0.00000000


[epoch = 133] validation_perplexity =    95.56, validation_loss =  4.56

[epoch = 133] annealing learning_rate = 0.00000000

[batch =  100] train_perplexity =    59.69, train_loss =  4.09, learning_rate = 0.00000000


[batch =  200] train_perplexity =    64.11, train_loss =  4.16, learning_rate = 0.00000000


[batch =  300] train_perplexity =    64.25, train_loss =  4.16, learning_rate = 0.00000000


[batch =  400] train_perplexity =    58.71, train_loss =  4.07, learning_rate = 0.00000000


[batch =  500] train_perplexity =    58.43, train_loss =  4.07, learning_rate = 0.00000000


[batch =  600] train_perplexity =    61.93, train_loss =  4.13, learning_rate = 0.00000000


[batch =  700] train_perplexity =    59.88, train_loss =  4.09, learning_rate = 0.00000000


[batch =  800] train_perplexity =    55.10, train_loss =  4.01, learning_rate = 0.00000000


[batch =  900] train_perplexity =    60.00, train_loss =  4.09, learning_rate = 0.00000000


[batch = 1000] train_perplexity =    62.99, train_loss =  4.14, learning_rate = 0.00000000


[batch = 1100] train_perplexity =    53.98, train_loss =  3.99, learning_rate = 0.00000000


[batch = 1200] train_perplexity =    54.71, train_loss =  4.00, learning_rate = 0.00000000


[batch = 1300] train_perplexity =    54.94, train_loss =  4.01, learning_rate = 0.00000000


[epoch = 134] validation_perplexity =    95.56, validation_loss =  4.56

[epoch = 134] annealing learning_rate = 0.00000000

[batch =  100] train_perplexity =    59.50, train_loss =  4.09, learning_rate = 0.00000000


[batch =  200] train_perplexity =    64.01, train_loss =  4.16, learning_rate = 0.00000000


[batch =  300] train_perplexity =    63.82, train_loss =  4.16, learning_rate = 0.00000000


[batch =  400] train_perplexity =    58.54, train_loss =  4.07, learning_rate = 0.00000000


[batch =  500] train_perplexity =    58.40, train_loss =  4.07, learning_rate = 0.00000000


[batch =  600] train_perplexity =    62.08, train_loss =  4.13, learning_rate = 0.00000000


[batch =  700] train_perplexity =    59.70, train_loss =  4.09, learning_rate = 0.00000000


[batch =  800] train_perplexity =    55.05, train_loss =  4.01, learning_rate = 0.00000000


[batch =  900] train_perplexity =    60.06, train_loss =  4.10, learning_rate = 0.00000000


[batch = 1000] train_perplexity =    62.83, train_loss =  4.14, learning_rate = 0.00000000


[batch = 1100] train_perplexity =    53.56, train_loss =  3.98, learning_rate = 0.00000000


[batch = 1200] train_perplexity =    54.51, train_loss =  4.00, learning_rate = 0.00000000


[batch = 1300] train_perplexity =    54.84, train_loss =  4.00, learning_rate = 0.00000000


[epoch = 135] validation_perplexity =    95.56, validation_loss =  4.56

[epoch = 135] annealing learning_rate = 0.00000000

[batch =  100] train_perplexity =    59.61, train_loss =  4.09, learning_rate = 0.00000000


[batch =  200] train_perplexity =    64.32, train_loss =  4.16, learning_rate = 0.00000000


[batch =  300] train_perplexity =    63.69, train_loss =  4.15, learning_rate = 0.00000000


[batch =  400] train_perplexity =    58.65, train_loss =  4.07, learning_rate = 0.00000000


[batch =  500] train_perplexity =    58.16, train_loss =  4.06, learning_rate = 0.00000000


[batch =  600] train_perplexity =    62.21, train_loss =  4.13, learning_rate = 0.00000000


[batch =  700] train_perplexity =    60.00, train_loss =  4.09, learning_rate = 0.00000000


[batch =  800] train_perplexity =    54.73, train_loss =  4.00, learning_rate = 0.00000000


[batch =  900] train_perplexity =    59.97, train_loss =  4.09, learning_rate = 0.00000000


[batch = 1000] train_perplexity =    62.98, train_loss =  4.14, learning_rate = 0.00000000


[batch = 1100] train_perplexity =    53.88, train_loss =  3.99, learning_rate = 0.00000000


[batch = 1200] train_perplexity =    54.58, train_loss =  4.00, learning_rate = 0.00000000


[batch = 1300] train_perplexity =    54.87, train_loss =  4.00, learning_rate = 0.00000000


[epoch = 136] validation_perplexity =    95.56, validation_loss =  4.56

[epoch = 136] annealing learning_rate = 0.00000000

[batch =  100] train_perplexity =    59.76, train_loss =  4.09, learning_rate = 0.00000000


[batch =  200] train_perplexity =    63.91, train_loss =  4.16, learning_rate = 0.00000000


[batch =  300] train_perplexity =    63.82, train_loss =  4.16, learning_rate = 0.00000000


[batch =  400] train_perplexity =    58.64, train_loss =  4.07, learning_rate = 0.00000000


[batch =  500] train_perplexity =    58.30, train_loss =  4.07, learning_rate = 0.00000000


[batch =  600] train_perplexity =    62.36, train_loss =  4.13, learning_rate = 0.00000000


[batch =  700] train_perplexity =    60.08, train_loss =  4.10, learning_rate = 0.00000000


[batch =  800] train_perplexity =    55.20, train_loss =  4.01, learning_rate = 0.00000000


[batch =  900] train_perplexity =    59.93, train_loss =  4.09, learning_rate = 0.00000000


[batch = 1000] train_perplexity =    63.04, train_loss =  4.14, learning_rate = 0.00000000


[batch = 1100] train_perplexity =    53.84, train_loss =  3.99, learning_rate = 0.00000000


[batch = 1200] train_perplexity =    54.28, train_loss =  3.99, learning_rate = 0.00000000


[batch = 1300] train_perplexity =    54.64, train_loss =  4.00, learning_rate = 0.00000000


[epoch = 137] validation_perplexity =    95.56, validation_loss =  4.56

[epoch = 137] annealing learning_rate = 0.00000000

[batch =  100] train_perplexity =    59.94, train_loss =  4.09, learning_rate = 0.00000000


[batch =  200] train_perplexity =    63.77, train_loss =  4.16, learning_rate = 0.00000000


[batch =  300] train_perplexity =    63.73, train_loss =  4.15, learning_rate = 0.00000000


[batch =  400] train_perplexity =    58.43, train_loss =  4.07, learning_rate = 0.00000000


[batch =  500] train_perplexity =    58.30, train_loss =  4.07, learning_rate = 0.00000000


[batch =  600] train_perplexity =    62.44, train_loss =  4.13, learning_rate = 0.00000000


[batch =  700] train_perplexity =    59.95, train_loss =  4.09, learning_rate = 0.00000000


[batch =  800] train_perplexity =    55.09, train_loss =  4.01, learning_rate = 0.00000000


[batch =  900] train_perplexity =    59.85, train_loss =  4.09, learning_rate = 0.00000000


[batch = 1000] train_perplexity =    63.00, train_loss =  4.14, learning_rate = 0.00000000


[batch = 1100] train_perplexity =    53.65, train_loss =  3.98, learning_rate = 0.00000000


[batch = 1200] train_perplexity =    54.61, train_loss =  4.00, learning_rate = 0.00000000


[batch = 1300] train_perplexity =    54.82, train_loss =  4.00, learning_rate = 0.00000000


[epoch = 138] validation_perplexity =    95.56, validation_loss =  4.56

[epoch = 138] annealing learning_rate = 0.00000000

[batch =  100] train_perplexity =    59.81, train_loss =  4.09, learning_rate = 0.00000000


[batch =  200] train_perplexity =    64.03, train_loss =  4.16, learning_rate = 0.00000000


[batch =  300] train_perplexity =    63.88, train_loss =  4.16, learning_rate = 0.00000000


[batch =  400] train_perplexity =    58.85, train_loss =  4.08, learning_rate = 0.00000000


[batch =  500] train_perplexity =    58.70, train_loss =  4.07, learning_rate = 0.00000000


[batch =  600] train_perplexity =    62.37, train_loss =  4.13, learning_rate = 0.00000000


[batch =  700] train_perplexity =    59.99, train_loss =  4.09, learning_rate = 0.00000000


[batch =  800] train_perplexity =    54.75, train_loss =  4.00, learning_rate = 0.00000000


[batch =  900] train_perplexity =    59.93, train_loss =  4.09, learning_rate = 0.00000000


[batch = 1000] train_perplexity =    63.25, train_loss =  4.15, learning_rate = 0.00000000


[batch = 1100] train_perplexity =    53.38, train_loss =  3.98, learning_rate = 0.00000000


[batch = 1200] train_perplexity =    54.43, train_loss =  4.00, learning_rate = 0.00000000


[batch = 1300] train_perplexity =    54.76, train_loss =  4.00, learning_rate = 0.00000000


[epoch = 139] validation_perplexity =    95.56, validation_loss =  4.56

[epoch = 139] annealing learning_rate = 0.00000000

[batch =  100] train_perplexity =    59.70, train_loss =  4.09, learning_rate = 0.00000000


[batch =  200] train_perplexity =    63.51, train_loss =  4.15, learning_rate = 0.00000000


[batch =  300] train_perplexity =    63.79, train_loss =  4.16, learning_rate = 0.00000000


[batch =  400] train_perplexity =    58.66, train_loss =  4.07, learning_rate = 0.00000000


[batch =  500] train_perplexity =    58.41, train_loss =  4.07, learning_rate = 0.00000000


[batch =  600] train_perplexity =    62.01, train_loss =  4.13, learning_rate = 0.00000000


[batch =  700] train_perplexity =    60.14, train_loss =  4.10, learning_rate = 0.00000000


[batch =  800] train_perplexity =    55.26, train_loss =  4.01, learning_rate = 0.00000000


[batch =  900] train_perplexity =    60.21, train_loss =  4.10, learning_rate = 0.00000000


[batch = 1000] train_perplexity =    62.62, train_loss =  4.14, learning_rate = 0.00000000


[batch = 1100] train_perplexity =    53.71, train_loss =  3.98, learning_rate = 0.00000000


[batch = 1200] train_perplexity =    54.23, train_loss =  3.99, learning_rate = 0.00000000


[batch = 1300] train_perplexity =    54.76, train_loss =  4.00, learning_rate = 0.00000000


[epoch = 140] validation_perplexity =    95.56, validation_loss =  4.56

[epoch = 140] annealing learning_rate = 0.00000000

[batch =  100] train_perplexity =    59.97, train_loss =  4.09, learning_rate = 0.00000000


[batch =  200] train_perplexity =    63.86, train_loss =  4.16, learning_rate = 0.00000000


[batch =  300] train_perplexity =    63.86, train_loss =  4.16, learning_rate = 0.00000000


[batch =  400] train_perplexity =    58.31, train_loss =  4.07, learning_rate = 0.00000000


[batch =  500] train_perplexity =    58.27, train_loss =  4.07, learning_rate = 0.00000000


[batch =  600] train_perplexity =    61.95, train_loss =  4.13, learning_rate = 0.00000000


[batch =  700] train_perplexity =    60.00, train_loss =  4.09, learning_rate = 0.00000000


[batch =  800] train_perplexity =    54.94, train_loss =  4.01, learning_rate = 0.00000000


[batch =  900] train_perplexity =    59.83, train_loss =  4.09, learning_rate = 0.00000000


[batch = 1000] train_perplexity =    63.16, train_loss =  4.15, learning_rate = 0.00000000


[batch = 1100] train_perplexity =    53.65, train_loss =  3.98, learning_rate = 0.00000000


[batch = 1200] train_perplexity =    54.51, train_loss =  4.00, learning_rate = 0.00000000


[batch = 1300] train_perplexity =    54.96, train_loss =  4.01, learning_rate = 0.00000000


[epoch = 141] validation_perplexity =    95.56, validation_loss =  4.56

[epoch = 141] annealing learning_rate = 0.00000000

[batch =  100] train_perplexity =    59.80, train_loss =  4.09, learning_rate = 0.00000000


[batch =  200] train_perplexity =    64.10, train_loss =  4.16, learning_rate = 0.00000000


[batch =  300] train_perplexity =    63.82, train_loss =  4.16, learning_rate = 0.00000000


[batch =  400] train_perplexity =    58.93, train_loss =  4.08, learning_rate = 0.00000000


[batch =  500] train_perplexity =    58.17, train_loss =  4.06, learning_rate = 0.00000000


[batch =  600] train_perplexity =    62.01, train_loss =  4.13, learning_rate = 0.00000000


[batch =  700] train_perplexity =    59.78, train_loss =  4.09, learning_rate = 0.00000000


[batch =  800] train_perplexity =    55.15, train_loss =  4.01, learning_rate = 0.00000000


[batch =  900] train_perplexity =    60.03, train_loss =  4.09, learning_rate = 0.00000000


[batch = 1000] train_perplexity =    62.67, train_loss =  4.14, learning_rate = 0.00000000


[batch = 1100] train_perplexity =    53.87, train_loss =  3.99, learning_rate = 0.00000000


[batch = 1200] train_perplexity =    54.59, train_loss =  4.00, learning_rate = 0.00000000


[batch = 1300] train_perplexity =    54.74, train_loss =  4.00, learning_rate = 0.00000000


[epoch = 142] validation_perplexity =    95.56, validation_loss =  4.56

[epoch = 142] annealing learning_rate = 0.00000000

[batch =  100] train_perplexity =    59.73, train_loss =  4.09, learning_rate = 0.00000000


[batch =  200] train_perplexity =    64.03, train_loss =  4.16, learning_rate = 0.00000000


[batch =  300] train_perplexity =    64.02, train_loss =  4.16, learning_rate = 0.00000000


[batch =  400] train_perplexity =    58.53, train_loss =  4.07, learning_rate = 0.00000000


[batch =  500] train_perplexity =    58.10, train_loss =  4.06, learning_rate = 0.00000000


[batch =  600] train_perplexity =    61.94, train_loss =  4.13, learning_rate = 0.00000000


[batch =  700] train_perplexity =    59.75, train_loss =  4.09, learning_rate = 0.00000000


[batch =  800] train_perplexity =    54.99, train_loss =  4.01, learning_rate = 0.00000000


[batch =  900] train_perplexity =    59.74, train_loss =  4.09, learning_rate = 0.00000000


[batch = 1000] train_perplexity =    62.98, train_loss =  4.14, learning_rate = 0.00000000


[batch = 1100] train_perplexity =    54.09, train_loss =  3.99, learning_rate = 0.00000000


[batch = 1200] train_perplexity =    54.38, train_loss =  4.00, learning_rate = 0.00000000


[batch = 1300] train_perplexity =    54.96, train_loss =  4.01, learning_rate = 0.00000000


[epoch = 143] validation_perplexity =    95.56, validation_loss =  4.56

[epoch = 143] annealing learning_rate = 0.00000000

[batch =  100] train_perplexity =    59.83, train_loss =  4.09, learning_rate = 0.00000000


[batch =  200] train_perplexity =    63.70, train_loss =  4.15, learning_rate = 0.00000000


[batch =  300] train_perplexity =    63.96, train_loss =  4.16, learning_rate = 0.00000000


[batch =  400] train_perplexity =    58.51, train_loss =  4.07, learning_rate = 0.00000000


[batch =  500] train_perplexity =    58.22, train_loss =  4.06, learning_rate = 0.00000000


[batch =  600] train_perplexity =    62.01, train_loss =  4.13, learning_rate = 0.00000000


[batch =  700] train_perplexity =    59.75, train_loss =  4.09, learning_rate = 0.00000000


[batch =  800] train_perplexity =    54.99, train_loss =  4.01, learning_rate = 0.00000000


[batch =  900] train_perplexity =    60.08, train_loss =  4.10, learning_rate = 0.00000000


[batch = 1000] train_perplexity =    62.83, train_loss =  4.14, learning_rate = 0.00000000


[batch = 1100] train_perplexity =    53.74, train_loss =  3.98, learning_rate = 0.00000000


[batch = 1200] train_perplexity =    54.36, train_loss =  4.00, learning_rate = 0.00000000


[batch = 1300] train_perplexity =    54.79, train_loss =  4.00, learning_rate = 0.00000000


[epoch = 144] validation_perplexity =    95.56, validation_loss =  4.56

[epoch = 144] annealing learning_rate = 0.00000000

[batch =  100] train_perplexity =    59.96, train_loss =  4.09, learning_rate = 0.00000000


[batch =  200] train_perplexity =    64.34, train_loss =  4.16, learning_rate = 0.00000000


[batch =  300] train_perplexity =    63.95, train_loss =  4.16, learning_rate = 0.00000000


[batch =  400] train_perplexity =    58.72, train_loss =  4.07, learning_rate = 0.00000000


[batch =  500] train_perplexity =    58.38, train_loss =  4.07, learning_rate = 0.00000000


[batch =  600] train_perplexity =    62.22, train_loss =  4.13, learning_rate = 0.00000000


[batch =  700] train_perplexity =    60.12, train_loss =  4.10, learning_rate = 0.00000000


[batch =  800] train_perplexity =    54.79, train_loss =  4.00, learning_rate = 0.00000000


[batch =  900] train_perplexity =    60.02, train_loss =  4.09, learning_rate = 0.00000000


[batch = 1000] train_perplexity =    62.84, train_loss =  4.14, learning_rate = 0.00000000


[batch = 1100] train_perplexity =    53.93, train_loss =  3.99, learning_rate = 0.00000000


[batch = 1200] train_perplexity =    54.22, train_loss =  3.99, learning_rate = 0.00000000


[batch = 1300] train_perplexity =    54.96, train_loss =  4.01, learning_rate = 0.00000000


[epoch = 145] validation_perplexity =    95.56, validation_loss =  4.56

[epoch = 145] annealing learning_rate = 0.00000000

[batch =  100] train_perplexity =    59.93, train_loss =  4.09, learning_rate = 0.00000000


[batch =  200] train_perplexity =    63.99, train_loss =  4.16, learning_rate = 0.00000000


[batch =  300] train_perplexity =    64.06, train_loss =  4.16, learning_rate = 0.00000000


[batch =  400] train_perplexity =    58.74, train_loss =  4.07, learning_rate = 0.00000000


[batch =  500] train_perplexity =    58.31, train_loss =  4.07, learning_rate = 0.00000000


[batch =  600] train_perplexity =    62.18, train_loss =  4.13, learning_rate = 0.00000000


[batch =  700] train_perplexity =    59.85, train_loss =  4.09, learning_rate = 0.00000000


[batch =  800] train_perplexity =    54.79, train_loss =  4.00, learning_rate = 0.00000000


[batch =  900] train_perplexity =    60.17, train_loss =  4.10, learning_rate = 0.00000000


[batch = 1000] train_perplexity =    62.91, train_loss =  4.14, learning_rate = 0.00000000


[batch = 1100] train_perplexity =    53.65, train_loss =  3.98, learning_rate = 0.00000000


[batch = 1200] train_perplexity =    54.23, train_loss =  3.99, learning_rate = 0.00000000


[batch = 1300] train_perplexity =    54.76, train_loss =  4.00, learning_rate = 0.00000000


[epoch = 146] validation_perplexity =    95.56, validation_loss =  4.56

[epoch = 146] annealing learning_rate = 0.00000000

[batch =  100] train_perplexity =    59.88, train_loss =  4.09, learning_rate = 0.00000000


[batch =  200] train_perplexity =    64.01, train_loss =  4.16, learning_rate = 0.00000000


[batch =  300] train_perplexity =    64.22, train_loss =  4.16, learning_rate = 0.00000000


[batch =  400] train_perplexity =    58.67, train_loss =  4.07, learning_rate = 0.00000000


[batch =  500] train_perplexity =    58.11, train_loss =  4.06, learning_rate = 0.00000000


[batch =  600] train_perplexity =    61.98, train_loss =  4.13, learning_rate = 0.00000000


[batch =  700] train_perplexity =    59.70, train_loss =  4.09, learning_rate = 0.00000000


[batch =  800] train_perplexity =    55.01, train_loss =  4.01, learning_rate = 0.00000000


[batch =  900] train_perplexity =    60.05, train_loss =  4.10, learning_rate = 0.00000000


[batch = 1000] train_perplexity =    62.96, train_loss =  4.14, learning_rate = 0.00000000


[batch = 1100] train_perplexity =    53.96, train_loss =  3.99, learning_rate = 0.00000000


[batch = 1200] train_perplexity =    54.31, train_loss =  3.99, learning_rate = 0.00000000


[batch = 1300] train_perplexity =    54.59, train_loss =  4.00, learning_rate = 0.00000000


[epoch = 147] validation_perplexity =    95.56, validation_loss =  4.56

[epoch = 147] annealing learning_rate = 0.00000000

[batch =  100] train_perplexity =    59.87, train_loss =  4.09, learning_rate = 0.00000000


[batch =  200] train_perplexity =    63.96, train_loss =  4.16, learning_rate = 0.00000000


[batch =  300] train_perplexity =    63.99, train_loss =  4.16, learning_rate = 0.00000000


[batch =  400] train_perplexity =    58.65, train_loss =  4.07, learning_rate = 0.00000000


[batch =  500] train_perplexity =    57.99, train_loss =  4.06, learning_rate = 0.00000000


[batch =  600] train_perplexity =    62.21, train_loss =  4.13, learning_rate = 0.00000000


[batch =  700] train_perplexity =    59.91, train_loss =  4.09, learning_rate = 0.00000000


[batch =  800] train_perplexity =    54.97, train_loss =  4.01, learning_rate = 0.00000000


[batch =  900] train_perplexity =    60.05, train_loss =  4.10, learning_rate = 0.00000000


[batch = 1000] train_perplexity =    62.76, train_loss =  4.14, learning_rate = 0.00000000


[batch = 1100] train_perplexity =    53.75, train_loss =  3.98, learning_rate = 0.00000000


[batch = 1200] train_perplexity =    54.41, train_loss =  4.00, learning_rate = 0.00000000


[batch = 1300] train_perplexity =    54.90, train_loss =  4.01, learning_rate = 0.00000000


[epoch = 148] validation_perplexity =    95.56, validation_loss =  4.56

[epoch = 148] annealing learning_rate = 0.00000000

[batch =  100] train_perplexity =    59.84, train_loss =  4.09, learning_rate = 0.00000000


[batch =  200] train_perplexity =    64.46, train_loss =  4.17, learning_rate = 0.00000000


[batch =  300] train_perplexity =    64.00, train_loss =  4.16, learning_rate = 0.00000000


[batch =  400] train_perplexity =    58.54, train_loss =  4.07, learning_rate = 0.00000000


[batch =  500] train_perplexity =    58.27, train_loss =  4.07, learning_rate = 0.00000000


[batch =  600] train_perplexity =    62.02, train_loss =  4.13, learning_rate = 0.00000000


[batch =  700] train_perplexity =    59.97, train_loss =  4.09, learning_rate = 0.00000000


[batch =  800] train_perplexity =    55.26, train_loss =  4.01, learning_rate = 0.00000000


[batch =  900] train_perplexity =    59.96, train_loss =  4.09, learning_rate = 0.00000000


[batch = 1000] train_perplexity =    63.23, train_loss =  4.15, learning_rate = 0.00000000


[batch = 1100] train_perplexity =    53.99, train_loss =  3.99, learning_rate = 0.00000000


[batch = 1200] train_perplexity =    54.42, train_loss =  4.00, learning_rate = 0.00000000


[batch = 1300] train_perplexity =    54.76, train_loss =  4.00, learning_rate = 0.00000000


[epoch = 149] validation_perplexity =    95.56, validation_loss =  4.56

[epoch = 149] annealing learning_rate = 0.00000000

[batch =  100] train_perplexity =    59.77, train_loss =  4.09, learning_rate = 0.00000000


[batch =  200] train_perplexity =    64.06, train_loss =  4.16, learning_rate = 0.00000000


[batch =  300] train_perplexity =    63.83, train_loss =  4.16, learning_rate = 0.00000000


[batch =  400] train_perplexity =    58.50, train_loss =  4.07, learning_rate = 0.00000000


[batch =  500] train_perplexity =    58.34, train_loss =  4.07, learning_rate = 0.00000000


[batch =  600] train_perplexity =    62.20, train_loss =  4.13, learning_rate = 0.00000000


[batch =  700] train_perplexity =    59.69, train_loss =  4.09, learning_rate = 0.00000000


[batch =  800] train_perplexity =    55.05, train_loss =  4.01, learning_rate = 0.00000000


[batch =  900] train_perplexity =    59.63, train_loss =  4.09, learning_rate = 0.00000000


[batch = 1000] train_perplexity =    62.89, train_loss =  4.14, learning_rate = 0.00000000


[batch = 1100] train_perplexity =    53.59, train_loss =  3.98, learning_rate = 0.00000000


[batch = 1200] train_perplexity =    54.19, train_loss =  3.99, learning_rate = 0.00000000


[batch = 1300] train_perplexity =    55.12, train_loss =  4.01, learning_rate = 0.00000000


[epoch = 150] validation_perplexity =    95.56, validation_loss =  4.56

[epoch = 150] annealing learning_rate = 0.00000000

[batch =  100] train_perplexity =    59.89, train_loss =  4.09, learning_rate = 0.00000000


[batch =  200] train_perplexity =    63.67, train_loss =  4.15, learning_rate = 0.00000000


[batch =  300] train_perplexity =    63.91, train_loss =  4.16, learning_rate = 0.00000000


[batch =  400] train_perplexity =    58.36, train_loss =  4.07, learning_rate = 0.00000000


[batch =  500] train_perplexity =    58.42, train_loss =  4.07, learning_rate = 0.00000000


[batch =  600] train_perplexity =    62.11, train_loss =  4.13, learning_rate = 0.00000000


[batch =  700] train_perplexity =    59.84, train_loss =  4.09, learning_rate = 0.00000000


[batch =  800] train_perplexity =    55.08, train_loss =  4.01, learning_rate = 0.00000000


[batch =  900] train_perplexity =    60.25, train_loss =  4.10, learning_rate = 0.00000000


[batch = 1000] train_perplexity =    62.72, train_loss =  4.14, learning_rate = 0.00000000


[batch = 1100] train_perplexity =    53.75, train_loss =  3.98, learning_rate = 0.00000000


[batch = 1200] train_perplexity =    54.25, train_loss =  3.99, learning_rate = 0.00000000


[batch = 1300] train_perplexity =    54.68, train_loss =  4.00, learning_rate = 0.00000000


[epoch = 151] validation_perplexity =    95.56, validation_loss =  4.56

[epoch = 151] annealing learning_rate = 0.00000000

[batch =  100] train_perplexity =    59.76, train_loss =  4.09, learning_rate = 0.00000000


[batch =  200] train_perplexity =    64.21, train_loss =  4.16, learning_rate = 0.00000000


[batch =  300] train_perplexity =    63.85, train_loss =  4.16, learning_rate = 0.00000000


[batch =  400] train_perplexity =    58.56, train_loss =  4.07, learning_rate = 0.00000000


[batch =  500] train_perplexity =    58.65, train_loss =  4.07, learning_rate = 0.00000000


[batch =  600] train_perplexity =    62.12, train_loss =  4.13, learning_rate = 0.00000000


[batch =  700] train_perplexity =    59.85, train_loss =  4.09, learning_rate = 0.00000000


[batch =  800] train_perplexity =    54.85, train_loss =  4.00, learning_rate = 0.00000000


[batch =  900] train_perplexity =    60.01, train_loss =  4.09, learning_rate = 0.00000000


[batch = 1000] train_perplexity =    62.77, train_loss =  4.14, learning_rate = 0.00000000


[batch = 1100] train_perplexity =    53.81, train_loss =  3.99, learning_rate = 0.00000000


[batch = 1200] train_perplexity =    54.39, train_loss =  4.00, learning_rate = 0.00000000


[batch = 1300] train_perplexity =    54.72, train_loss =  4.00, learning_rate = 0.00000000


[epoch = 152] validation_perplexity =    95.56, validation_loss =  4.56

[epoch = 152] annealing learning_rate = 0.00000000

[batch =  100] train_perplexity =    59.57, train_loss =  4.09, learning_rate = 0.00000000


[batch =  200] train_perplexity =    64.23, train_loss =  4.16, learning_rate = 0.00000000


[batch =  300] train_perplexity =    63.80, train_loss =  4.16, learning_rate = 0.00000000


[batch =  400] train_perplexity =    58.68, train_loss =  4.07, learning_rate = 0.00000000


[batch =  500] train_perplexity =    58.70, train_loss =  4.07, learning_rate = 0.00000000


[batch =  600] train_perplexity =    62.24, train_loss =  4.13, learning_rate = 0.00000000


[batch =  700] train_perplexity =    59.84, train_loss =  4.09, learning_rate = 0.00000000


[batch =  800] train_perplexity =    54.87, train_loss =  4.00, learning_rate = 0.00000000


[batch =  900] train_perplexity =    60.39, train_loss =  4.10, learning_rate = 0.00000000


[batch = 1000] train_perplexity =    62.99, train_loss =  4.14, learning_rate = 0.00000000


[batch = 1100] train_perplexity =    53.57, train_loss =  3.98, learning_rate = 0.00000000


[batch = 1200] train_perplexity =    54.37, train_loss =  4.00, learning_rate = 0.00000000


[batch = 1300] train_perplexity =    54.80, train_loss =  4.00, learning_rate = 0.00000000


[epoch = 153] validation_perplexity =    95.56, validation_loss =  4.56

[epoch = 153] annealing learning_rate = 0.00000000

[batch =  100] train_perplexity =    59.64, train_loss =  4.09, learning_rate = 0.00000000


[batch =  200] train_perplexity =    64.26, train_loss =  4.16, learning_rate = 0.00000000


[batch =  300] train_perplexity =    64.07, train_loss =  4.16, learning_rate = 0.00000000


[batch =  400] train_perplexity =    58.72, train_loss =  4.07, learning_rate = 0.00000000


[batch =  500] train_perplexity =    58.29, train_loss =  4.07, learning_rate = 0.00000000


[batch =  600] train_perplexity =    61.80, train_loss =  4.12, learning_rate = 0.00000000


[batch =  700] train_perplexity =    59.86, train_loss =  4.09, learning_rate = 0.00000000


[batch =  800] train_perplexity =    55.04, train_loss =  4.01, learning_rate = 0.00000000


[batch =  900] train_perplexity =    60.08, train_loss =  4.10, learning_rate = 0.00000000


[batch = 1000] train_perplexity =    63.18, train_loss =  4.15, learning_rate = 0.00000000


[batch = 1100] train_perplexity =    53.72, train_loss =  3.98, learning_rate = 0.00000000


[batch = 1200] train_perplexity =    54.37, train_loss =  4.00, learning_rate = 0.00000000


[batch = 1300] train_perplexity =    54.85, train_loss =  4.00, learning_rate = 0.00000000


[epoch = 154] validation_perplexity =    95.56, validation_loss =  4.56

[epoch = 154] annealing learning_rate = 0.00000000

[batch =  100] train_perplexity =    59.59, train_loss =  4.09, learning_rate = 0.00000000


[batch =  200] train_perplexity =    64.12, train_loss =  4.16, learning_rate = 0.00000000


[batch =  300] train_perplexity =    64.24, train_loss =  4.16, learning_rate = 0.00000000


[batch =  400] train_perplexity =    58.72, train_loss =  4.07, learning_rate = 0.00000000


[batch =  500] train_perplexity =    58.23, train_loss =  4.06, learning_rate = 0.00000000


[batch =  600] train_perplexity =    62.08, train_loss =  4.13, learning_rate = 0.00000000


[batch =  700] train_perplexity =    59.81, train_loss =  4.09, learning_rate = 0.00000000


[batch =  800] train_perplexity =    54.98, train_loss =  4.01, learning_rate = 0.00000000


[batch =  900] train_perplexity =    60.15, train_loss =  4.10, learning_rate = 0.00000000


[batch = 1000] train_perplexity =    63.19, train_loss =  4.15, learning_rate = 0.00000000


[batch = 1100] train_perplexity =    53.81, train_loss =  3.99, learning_rate = 0.00000000


[batch = 1200] train_perplexity =    54.35, train_loss =  4.00, learning_rate = 0.00000000


[batch = 1300] train_perplexity =    54.80, train_loss =  4.00, learning_rate = 0.00000000


[epoch = 155] validation_perplexity =    95.56, validation_loss =  4.56

[epoch = 155] annealing learning_rate = 0.00000000

[batch =  100] train_perplexity =    59.86, train_loss =  4.09, learning_rate = 0.00000000


[batch =  200] train_perplexity =    64.05, train_loss =  4.16, learning_rate = 0.00000000


[batch =  300] train_perplexity =    63.64, train_loss =  4.15, learning_rate = 0.00000000


[batch =  400] train_perplexity =    58.71, train_loss =  4.07, learning_rate = 0.00000000


[batch =  500] train_perplexity =    58.26, train_loss =  4.06, learning_rate = 0.00000000


[batch =  600] train_perplexity =    61.94, train_loss =  4.13, learning_rate = 0.00000000


[batch =  700] train_perplexity =    59.77, train_loss =  4.09, learning_rate = 0.00000000


[batch =  800] train_perplexity =    55.02, train_loss =  4.01, learning_rate = 0.00000000


[batch =  900] train_perplexity =    59.82, train_loss =  4.09, learning_rate = 0.00000000


[batch = 1000] train_perplexity =    63.05, train_loss =  4.14, learning_rate = 0.00000000


[batch = 1100] train_perplexity =    53.89, train_loss =  3.99, learning_rate = 0.00000000


[batch = 1200] train_perplexity =    54.41, train_loss =  4.00, learning_rate = 0.00000000


[batch = 1300] train_perplexity =    54.85, train_loss =  4.00, learning_rate = 0.00000000


[epoch = 156] validation_perplexity =    95.56, validation_loss =  4.56

[epoch = 156] annealing learning_rate = 0.00000000

[batch =  100] train_perplexity =    59.82, train_loss =  4.09, learning_rate = 0.00000000


[batch =  200] train_perplexity =    64.09, train_loss =  4.16, learning_rate = 0.00000000


[batch =  300] train_perplexity =    63.76, train_loss =  4.16, learning_rate = 0.00000000


[batch =  400] train_perplexity =    58.85, train_loss =  4.08, learning_rate = 0.00000000


[batch =  500] train_perplexity =    58.33, train_loss =  4.07, learning_rate = 0.00000000


[batch =  600] train_perplexity =    62.24, train_loss =  4.13, learning_rate = 0.00000000


[batch =  700] train_perplexity =    59.92, train_loss =  4.09, learning_rate = 0.00000000


[batch =  800] train_perplexity =    55.00, train_loss =  4.01, learning_rate = 0.00000000


[batch =  900] train_perplexity =    59.99, train_loss =  4.09, learning_rate = 0.00000000


[batch = 1000] train_perplexity =    63.02, train_loss =  4.14, learning_rate = 0.00000000


[batch = 1100] train_perplexity =    53.74, train_loss =  3.98, learning_rate = 0.00000000


[batch = 1200] train_perplexity =    54.37, train_loss =  4.00, learning_rate = 0.00000000


[batch = 1300] train_perplexity =    54.51, train_loss =  4.00, learning_rate = 0.00000000


[epoch = 157] validation_perplexity =    95.56, validation_loss =  4.56

[epoch = 157] annealing learning_rate = 0.00000000

[batch =  100] train_perplexity =    59.47, train_loss =  4.09, learning_rate = 0.00000000


[batch =  200] train_perplexity =    63.95, train_loss =  4.16, learning_rate = 0.00000000


[batch =  300] train_perplexity =    64.05, train_loss =  4.16, learning_rate = 0.00000000


[batch =  400] train_perplexity =    58.80, train_loss =  4.07, learning_rate = 0.00000000


[batch =  500] train_perplexity =    58.23, train_loss =  4.06, learning_rate = 0.00000000


[batch =  600] train_perplexity =    62.12, train_loss =  4.13, learning_rate = 0.00000000


[batch =  700] train_perplexity =    59.90, train_loss =  4.09, learning_rate = 0.00000000


[batch =  800] train_perplexity =    55.09, train_loss =  4.01, learning_rate = 0.00000000


[batch =  900] train_perplexity =    59.90, train_loss =  4.09, learning_rate = 0.00000000


[batch = 1000] train_perplexity =    63.16, train_loss =  4.15, learning_rate = 0.00000000


[batch = 1100] train_perplexity =    53.71, train_loss =  3.98, learning_rate = 0.00000000


[batch = 1200] train_perplexity =    54.42, train_loss =  4.00, learning_rate = 0.00000000


[batch = 1300] train_perplexity =    54.93, train_loss =  4.01, learning_rate = 0.00000000


[epoch = 158] validation_perplexity =    95.56, validation_loss =  4.56

[epoch = 158] annealing learning_rate = 0.00000000

[batch =  100] train_perplexity =    59.79, train_loss =  4.09, learning_rate = 0.00000000


[batch =  200] train_perplexity =    64.12, train_loss =  4.16, learning_rate = 0.00000000


[batch =  300] train_perplexity =    64.05, train_loss =  4.16, learning_rate = 0.00000000


[batch =  400] train_perplexity =    58.69, train_loss =  4.07, learning_rate = 0.00000000


[batch =  500] train_perplexity =    58.33, train_loss =  4.07, learning_rate = 0.00000000


[batch =  600] train_perplexity =    61.88, train_loss =  4.13, learning_rate = 0.00000000


[batch =  700] train_perplexity =    60.00, train_loss =  4.09, learning_rate = 0.00000000


[batch =  800] train_perplexity =    54.84, train_loss =  4.00, learning_rate = 0.00000000


[batch =  900] train_perplexity =    59.90, train_loss =  4.09, learning_rate = 0.00000000


[batch = 1000] train_perplexity =    63.01, train_loss =  4.14, learning_rate = 0.00000000


[batch = 1100] train_perplexity =    53.61, train_loss =  3.98, learning_rate = 0.00000000


[batch = 1200] train_perplexity =    54.31, train_loss =  3.99, learning_rate = 0.00000000


[batch = 1300] train_perplexity =    54.69, train_loss =  4.00, learning_rate = 0.00000000


[epoch = 159] validation_perplexity =    95.56, validation_loss =  4.56

[epoch = 159] annealing learning_rate = 0.00000000

[batch =  100] train_perplexity =    59.66, train_loss =  4.09, learning_rate = 0.00000000


[batch =  200] train_perplexity =    64.02, train_loss =  4.16, learning_rate = 0.00000000


[batch =  300] train_perplexity =    63.90, train_loss =  4.16, learning_rate = 0.00000000


[batch =  400] train_perplexity =    58.80, train_loss =  4.07, learning_rate = 0.00000000


[batch =  500] train_perplexity =    58.23, train_loss =  4.06, learning_rate = 0.00000000


[batch =  600] train_perplexity =    62.23, train_loss =  4.13, learning_rate = 0.00000000


[batch =  700] train_perplexity =    60.14, train_loss =  4.10, learning_rate = 0.00000000


[batch =  800] train_perplexity =    55.09, train_loss =  4.01, learning_rate = 0.00000000


[batch =  900] train_perplexity =    60.10, train_loss =  4.10, learning_rate = 0.00000000


[batch = 1000] train_perplexity =    62.67, train_loss =  4.14, learning_rate = 0.00000000


[batch = 1100] train_perplexity =    53.81, train_loss =  3.99, learning_rate = 0.00000000


[batch = 1200] train_perplexity =    54.29, train_loss =  3.99, learning_rate = 0.00000000


[batch = 1300] train_perplexity =    54.69, train_loss =  4.00, learning_rate = 0.00000000


[epoch = 160] validation_perplexity =    95.56, validation_loss =  4.56

[epoch = 160] annealing learning_rate = 0.00000000

[batch =  100] train_perplexity =    59.60, train_loss =  4.09, learning_rate = 0.00000000


[batch =  200] train_perplexity =    63.94, train_loss =  4.16, learning_rate = 0.00000000


[batch =  300] train_perplexity =    64.13, train_loss =  4.16, learning_rate = 0.00000000


[batch =  400] train_perplexity =    58.77, train_loss =  4.07, learning_rate = 0.00000000


[batch =  500] train_perplexity =    58.25, train_loss =  4.06, learning_rate = 0.00000000


[batch =  600] train_perplexity =    61.84, train_loss =  4.12, learning_rate = 0.00000000


[batch =  700] train_perplexity =    59.79, train_loss =  4.09, learning_rate = 0.00000000


[batch =  800] train_perplexity =    54.81, train_loss =  4.00, learning_rate = 0.00000000


[batch =  900] train_perplexity =    59.81, train_loss =  4.09, learning_rate = 0.00000000


[batch = 1000] train_perplexity =    62.79, train_loss =  4.14, learning_rate = 0.00000000


[batch = 1100] train_perplexity =    53.75, train_loss =  3.98, learning_rate = 0.00000000


[batch = 1200] train_perplexity =    54.56, train_loss =  4.00, learning_rate = 0.00000000


[batch = 1300] train_perplexity =    54.72, train_loss =  4.00, learning_rate = 0.00000000


[epoch = 161] validation_perplexity =    95.56, validation_loss =  4.56

[epoch = 161] annealing learning_rate = 0.00000000

[batch =  100] train_perplexity =    59.87, train_loss =  4.09, learning_rate = 0.00000000


[batch =  200] train_perplexity =    63.85, train_loss =  4.16, learning_rate = 0.00000000


[batch =  300] train_perplexity =    64.04, train_loss =  4.16, learning_rate = 0.00000000


[batch =  400] train_perplexity =    58.39, train_loss =  4.07, learning_rate = 0.00000000


[batch =  500] train_perplexity =    58.56, train_loss =  4.07, learning_rate = 0.00000000


[batch =  600] train_perplexity =    61.97, train_loss =  4.13, learning_rate = 0.00000000


[batch =  700] train_perplexity =    59.96, train_loss =  4.09, learning_rate = 0.00000000


[batch =  800] train_perplexity =    55.11, train_loss =  4.01, learning_rate = 0.00000000


[batch =  900] train_perplexity =    60.04, train_loss =  4.09, learning_rate = 0.00000000


[batch = 1000] train_perplexity =    62.98, train_loss =  4.14, learning_rate = 0.00000000


[batch = 1100] train_perplexity =    53.85, train_loss =  3.99, learning_rate = 0.00000000


[batch = 1200] train_perplexity =    54.51, train_loss =  4.00, learning_rate = 0.00000000


[batch = 1300] train_perplexity =    54.72, train_loss =  4.00, learning_rate = 0.00000000


[epoch = 162] validation_perplexity =    95.56, validation_loss =  4.56

[epoch = 162] annealing learning_rate = 0.00000000

[batch =  100] train_perplexity =    59.71, train_loss =  4.09, learning_rate = 0.00000000


[batch =  200] train_perplexity =    64.25, train_loss =  4.16, learning_rate = 0.00000000


[batch =  300] train_perplexity =    63.73, train_loss =  4.15, learning_rate = 0.00000000


[batch =  400] train_perplexity =    58.65, train_loss =  4.07, learning_rate = 0.00000000


[batch =  500] train_perplexity =    58.32, train_loss =  4.07, learning_rate = 0.00000000


[batch =  600] train_perplexity =    62.51, train_loss =  4.14, learning_rate = 0.00000000


[batch =  700] train_perplexity =    59.58, train_loss =  4.09, learning_rate = 0.00000000


[batch =  800] train_perplexity =    55.00, train_loss =  4.01, learning_rate = 0.00000000


[batch =  900] train_perplexity =    59.73, train_loss =  4.09, learning_rate = 0.00000000


[batch = 1000] train_perplexity =    62.94, train_loss =  4.14, learning_rate = 0.00000000


[batch = 1100] train_perplexity =    53.79, train_loss =  3.99, learning_rate = 0.00000000


[batch = 1200] train_perplexity =    54.40, train_loss =  4.00, learning_rate = 0.00000000


[batch = 1300] train_perplexity =    54.90, train_loss =  4.01, learning_rate = 0.00000000


[epoch = 163] validation_perplexity =    95.56, validation_loss =  4.56

[epoch = 163] annealing learning_rate = 0.00000000

[batch =  100] train_perplexity =    59.81, train_loss =  4.09, learning_rate = 0.00000000


[batch =  200] train_perplexity =    63.82, train_loss =  4.16, learning_rate = 0.00000000


[batch =  300] train_perplexity =    64.10, train_loss =  4.16, learning_rate = 0.00000000


[batch =  400] train_perplexity =    58.78, train_loss =  4.07, learning_rate = 0.00000000


[batch =  500] train_perplexity =    58.35, train_loss =  4.07, learning_rate = 0.00000000


[batch =  600] train_perplexity =    62.00, train_loss =  4.13, learning_rate = 0.00000000


[batch =  700] train_perplexity =    59.59, train_loss =  4.09, learning_rate = 0.00000000


[batch =  800] train_perplexity =    55.11, train_loss =  4.01, learning_rate = 0.00000000


[batch =  900] train_perplexity =    59.69, train_loss =  4.09, learning_rate = 0.00000000


[batch = 1000] train_perplexity =    62.90, train_loss =  4.14, learning_rate = 0.00000000


[batch = 1100] train_perplexity =    53.89, train_loss =  3.99, learning_rate = 0.00000000


[batch = 1200] train_perplexity =    54.42, train_loss =  4.00, learning_rate = 0.00000000


[batch = 1300] train_perplexity =    55.28, train_loss =  4.01, learning_rate = 0.00000000


[epoch = 164] validation_perplexity =    95.56, validation_loss =  4.56

[epoch = 164] annealing learning_rate = 0.00000000

[batch =  100] train_perplexity =    59.73, train_loss =  4.09, learning_rate = 0.00000000


[batch =  200] train_perplexity =    63.81, train_loss =  4.16, learning_rate = 0.00000000


[batch =  300] train_perplexity =    63.93, train_loss =  4.16, learning_rate = 0.00000000


[batch =  400] train_perplexity =    58.60, train_loss =  4.07, learning_rate = 0.00000000


[batch =  500] train_perplexity =    58.25, train_loss =  4.06, learning_rate = 0.00000000


[batch =  600] train_perplexity =    62.16, train_loss =  4.13, learning_rate = 0.00000000


[batch =  700] train_perplexity =    59.73, train_loss =  4.09, learning_rate = 0.00000000


[batch =  800] train_perplexity =    54.77, train_loss =  4.00, learning_rate = 0.00000000


[batch =  900] train_perplexity =    59.81, train_loss =  4.09, learning_rate = 0.00000000


[batch = 1000] train_perplexity =    62.96, train_loss =  4.14, learning_rate = 0.00000000


[batch = 1100] train_perplexity =    53.61, train_loss =  3.98, learning_rate = 0.00000000


[batch = 1200] train_perplexity =    54.31, train_loss =  3.99, learning_rate = 0.00000000


[batch = 1300] train_perplexity =    54.79, train_loss =  4.00, learning_rate = 0.00000000


[epoch = 165] validation_perplexity =    95.56, validation_loss =  4.56

[epoch = 165] annealing learning_rate = 0.00000000

[batch =  100] train_perplexity =    59.82, train_loss =  4.09, learning_rate = 0.00000000


[batch =  200] train_perplexity =    63.65, train_loss =  4.15, learning_rate = 0.00000000


[batch =  300] train_perplexity =    63.72, train_loss =  4.15, learning_rate = 0.00000000


[batch =  400] train_perplexity =    58.48, train_loss =  4.07, learning_rate = 0.00000000


[batch =  500] train_perplexity =    58.25, train_loss =  4.06, learning_rate = 0.00000000


[batch =  600] train_perplexity =    62.04, train_loss =  4.13, learning_rate = 0.00000000


[batch =  700] train_perplexity =    60.06, train_loss =  4.10, learning_rate = 0.00000000


[batch =  800] train_perplexity =    55.02, train_loss =  4.01, learning_rate = 0.00000000


[batch =  900] train_perplexity =    60.10, train_loss =  4.10, learning_rate = 0.00000000


[batch = 1000] train_perplexity =    63.05, train_loss =  4.14, learning_rate = 0.00000000


[batch = 1100] train_perplexity =    53.69, train_loss =  3.98, learning_rate = 0.00000000


[batch = 1200] train_perplexity =    54.55, train_loss =  4.00, learning_rate = 0.00000000


[batch = 1300] train_perplexity =    54.83, train_loss =  4.00, learning_rate = 0.00000000


[epoch = 166] validation_perplexity =    95.56, validation_loss =  4.56

[epoch = 166] annealing learning_rate = 0.00000000

[batch =  100] train_perplexity =    59.92, train_loss =  4.09, learning_rate = 0.00000000


[batch =  200] train_perplexity =    63.75, train_loss =  4.15, learning_rate = 0.00000000


[batch =  300] train_perplexity =    63.99, train_loss =  4.16, learning_rate = 0.00000000


[batch =  400] train_perplexity =    58.48, train_loss =  4.07, learning_rate = 0.00000000


[batch =  500] train_perplexity =    58.40, train_loss =  4.07, learning_rate = 0.00000000


[batch =  600] train_perplexity =    62.30, train_loss =  4.13, learning_rate = 0.00000000


[batch =  700] train_perplexity =    59.97, train_loss =  4.09, learning_rate = 0.00000000


[batch =  800] train_perplexity =    55.13, train_loss =  4.01, learning_rate = 0.00000000


[batch =  900] train_perplexity =    60.24, train_loss =  4.10, learning_rate = 0.00000000


[batch = 1000] train_perplexity =    62.92, train_loss =  4.14, learning_rate = 0.00000000


[batch = 1100] train_perplexity =    53.71, train_loss =  3.98, learning_rate = 0.00000000


[batch = 1200] train_perplexity =    54.43, train_loss =  4.00, learning_rate = 0.00000000


[batch = 1300] train_perplexity =    54.56, train_loss =  4.00, learning_rate = 0.00000000


[epoch = 167] validation_perplexity =    95.56, validation_loss =  4.56

[epoch = 167] annealing learning_rate = 0.00000000

[batch =  100] train_perplexity =    59.70, train_loss =  4.09, learning_rate = 0.00000000


[batch =  200] train_perplexity =    63.94, train_loss =  4.16, learning_rate = 0.00000000


[batch =  300] train_perplexity =    63.79, train_loss =  4.16, learning_rate = 0.00000000


[batch =  400] train_perplexity =    58.53, train_loss =  4.07, learning_rate = 0.00000000


[batch =  500] train_perplexity =    58.17, train_loss =  4.06, learning_rate = 0.00000000


[batch =  600] train_perplexity =    62.18, train_loss =  4.13, learning_rate = 0.00000000


[batch =  700] train_perplexity =    59.65, train_loss =  4.09, learning_rate = 0.00000000


[batch =  800] train_perplexity =    54.93, train_loss =  4.01, learning_rate = 0.00000000


[batch =  900] train_perplexity =    60.18, train_loss =  4.10, learning_rate = 0.00000000


[batch = 1000] train_perplexity =    62.97, train_loss =  4.14, learning_rate = 0.00000000


[batch = 1100] train_perplexity =    53.64, train_loss =  3.98, learning_rate = 0.00000000


[batch = 1200] train_perplexity =    54.68, train_loss =  4.00, learning_rate = 0.00000000


[batch = 1300] train_perplexity =    54.77, train_loss =  4.00, learning_rate = 0.00000000


[epoch = 168] validation_perplexity =    95.56, validation_loss =  4.56

[epoch = 168] annealing learning_rate = 0.00000000

[batch =  100] train_perplexity =    59.80, train_loss =  4.09, learning_rate = 0.00000000


[batch =  200] train_perplexity =    63.95, train_loss =  4.16, learning_rate = 0.00000000


[batch =  300] train_perplexity =    63.91, train_loss =  4.16, learning_rate = 0.00000000


[batch =  400] train_perplexity =    58.73, train_loss =  4.07, learning_rate = 0.00000000


[batch =  500] train_perplexity =    58.36, train_loss =  4.07, learning_rate = 0.00000000


[batch =  600] train_perplexity =    62.16, train_loss =  4.13, learning_rate = 0.00000000


[batch =  700] train_perplexity =    60.19, train_loss =  4.10, learning_rate = 0.00000000


[batch =  800] train_perplexity =    55.12, train_loss =  4.01, learning_rate = 0.00000000


[batch =  900] train_perplexity =    60.20, train_loss =  4.10, learning_rate = 0.00000000


[batch = 1000] train_perplexity =    63.16, train_loss =  4.15, learning_rate = 0.00000000


[batch = 1100] train_perplexity =    53.94, train_loss =  3.99, learning_rate = 0.00000000


[batch = 1200] train_perplexity =    54.31, train_loss =  3.99, learning_rate = 0.00000000


[batch = 1300] train_perplexity =    54.84, train_loss =  4.00, learning_rate = 0.00000000


[epoch = 169] validation_perplexity =    95.56, validation_loss =  4.56

[epoch = 169] annealing learning_rate = 0.00000000

[batch =  100] train_perplexity =    60.07, train_loss =  4.10, learning_rate = 0.00000000


[batch =  200] train_perplexity =    63.95, train_loss =  4.16, learning_rate = 0.00000000


[batch =  300] train_perplexity =    64.27, train_loss =  4.16, learning_rate = 0.00000000


[batch =  400] train_perplexity =    58.52, train_loss =  4.07, learning_rate = 0.00000000


[batch =  500] train_perplexity =    58.49, train_loss =  4.07, learning_rate = 0.00000000


[batch =  600] train_perplexity =    62.10, train_loss =  4.13, learning_rate = 0.00000000


[batch =  700] train_perplexity =    59.86, train_loss =  4.09, learning_rate = 0.00000000


[batch =  800] train_perplexity =    54.76, train_loss =  4.00, learning_rate = 0.00000000


[batch =  900] train_perplexity =    60.01, train_loss =  4.09, learning_rate = 0.00000000


[batch = 1000] train_perplexity =    62.61, train_loss =  4.14, learning_rate = 0.00000000


[batch = 1100] train_perplexity =    53.45, train_loss =  3.98, learning_rate = 0.00000000


[batch = 1200] train_perplexity =    54.30, train_loss =  3.99, learning_rate = 0.00000000


[batch = 1300] train_perplexity =    54.67, train_loss =  4.00, learning_rate = 0.00000000


[epoch = 170] validation_perplexity =    95.56, validation_loss =  4.56

[epoch = 170] annealing learning_rate = 0.00000000

[batch =  100] train_perplexity =    60.06, train_loss =  4.10, learning_rate = 0.00000000


[batch =  200] train_perplexity =    63.86, train_loss =  4.16, learning_rate = 0.00000000


[batch =  300] train_perplexity =    63.85, train_loss =  4.16, learning_rate = 0.00000000


[batch =  400] train_perplexity =    58.62, train_loss =  4.07, learning_rate = 0.00000000


[batch =  500] train_perplexity =    58.20, train_loss =  4.06, learning_rate = 0.00000000


[batch =  600] train_perplexity =    62.33, train_loss =  4.13, learning_rate = 0.00000000


[batch =  700] train_perplexity =    59.83, train_loss =  4.09, learning_rate = 0.00000000


[batch =  800] train_perplexity =    54.80, train_loss =  4.00, learning_rate = 0.00000000


[batch =  900] train_perplexity =    59.91, train_loss =  4.09, learning_rate = 0.00000000


[batch = 1000] train_perplexity =    62.65, train_loss =  4.14, learning_rate = 0.00000000


[batch = 1100] train_perplexity =    53.72, train_loss =  3.98, learning_rate = 0.00000000


[batch = 1200] train_perplexity =    54.28, train_loss =  3.99, learning_rate = 0.00000000


[batch = 1300] train_perplexity =    54.94, train_loss =  4.01, learning_rate = 0.00000000


[epoch = 171] validation_perplexity =    95.56, validation_loss =  4.56

[epoch = 171] annealing learning_rate = 0.00000000

[batch =  100] train_perplexity =    60.10, train_loss =  4.10, learning_rate = 0.00000000


[batch =  200] train_perplexity =    63.87, train_loss =  4.16, learning_rate = 0.00000000


[batch =  300] train_perplexity =    64.14, train_loss =  4.16, learning_rate = 0.00000000


[batch =  400] train_perplexity =    58.42, train_loss =  4.07, learning_rate = 0.00000000


[batch =  500] train_perplexity =    58.32, train_loss =  4.07, learning_rate = 0.00000000


[batch =  600] train_perplexity =    62.05, train_loss =  4.13, learning_rate = 0.00000000


[batch =  700] train_perplexity =    59.93, train_loss =  4.09, learning_rate = 0.00000000


[batch =  800] train_perplexity =    55.06, train_loss =  4.01, learning_rate = 0.00000000


[batch =  900] train_perplexity =    59.79, train_loss =  4.09, learning_rate = 0.00000000


[batch = 1000] train_perplexity =    62.94, train_loss =  4.14, learning_rate = 0.00000000


[batch = 1100] train_perplexity =    53.79, train_loss =  3.99, learning_rate = 0.00000000


[batch = 1200] train_perplexity =    54.11, train_loss =  3.99, learning_rate = 0.00000000


[batch = 1300] train_perplexity =    54.86, train_loss =  4.00, learning_rate = 0.00000000


[epoch = 172] validation_perplexity =    95.56, validation_loss =  4.56

[epoch = 172] annealing learning_rate = 0.00000000

[batch =  100] train_perplexity =    59.99, train_loss =  4.09, learning_rate = 0.00000000


[batch =  200] train_perplexity =    64.22, train_loss =  4.16, learning_rate = 0.00000000


[batch =  300] train_perplexity =    63.85, train_loss =  4.16, learning_rate = 0.00000000


[batch =  400] train_perplexity =    58.61, train_loss =  4.07, learning_rate = 0.00000000


[batch =  500] train_perplexity =    58.21, train_loss =  4.06, learning_rate = 0.00000000


[batch =  600] train_perplexity =    61.94, train_loss =  4.13, learning_rate = 0.00000000


[batch =  700] train_perplexity =    60.02, train_loss =  4.09, learning_rate = 0.00000000


[batch =  800] train_perplexity =    54.88, train_loss =  4.01, learning_rate = 0.00000000


[batch =  900] train_perplexity =    59.85, train_loss =  4.09, learning_rate = 0.00000000


[batch = 1000] train_perplexity =    62.82, train_loss =  4.14, learning_rate = 0.00000000


[batch = 1100] train_perplexity =    53.74, train_loss =  3.98, learning_rate = 0.00000000


[batch = 1200] train_perplexity =    54.31, train_loss =  3.99, learning_rate = 0.00000000


[batch = 1300] train_perplexity =    54.37, train_loss =  4.00, learning_rate = 0.00000000


[epoch = 173] validation_perplexity =    95.56, validation_loss =  4.56

[epoch = 173] annealing learning_rate = 0.00000000

[batch =  100] train_perplexity =    59.71, train_loss =  4.09, learning_rate = 0.00000000


[batch =  200] train_perplexity =    63.90, train_loss =  4.16, learning_rate = 0.00000000


[batch =  300] train_perplexity =    64.04, train_loss =  4.16, learning_rate = 0.00000000


[batch =  400] train_perplexity =    58.40, train_loss =  4.07, learning_rate = 0.00000000


[batch =  500] train_perplexity =    58.35, train_loss =  4.07, learning_rate = 0.00000000


[batch =  600] train_perplexity =    61.71, train_loss =  4.12, learning_rate = 0.00000000


[batch =  700] train_perplexity =    59.95, train_loss =  4.09, learning_rate = 0.00000000


[batch =  800] train_perplexity =    55.09, train_loss =  4.01, learning_rate = 0.00000000


[batch =  900] train_perplexity =    60.15, train_loss =  4.10, learning_rate = 0.00000000


[batch = 1000] train_perplexity =    62.72, train_loss =  4.14, learning_rate = 0.00000000


[batch = 1100] train_perplexity =    53.57, train_loss =  3.98, learning_rate = 0.00000000


[batch = 1200] train_perplexity =    54.34, train_loss =  4.00, learning_rate = 0.00000000


[batch = 1300] train_perplexity =    55.01, train_loss =  4.01, learning_rate = 0.00000000


[epoch = 174] validation_perplexity =    95.56, validation_loss =  4.56

[epoch = 174] annealing learning_rate = 0.00000000

[batch =  100] train_perplexity =    59.87, train_loss =  4.09, learning_rate = 0.00000000


[batch =  200] train_perplexity =    64.06, train_loss =  4.16, learning_rate = 0.00000000


[batch =  300] train_perplexity =    63.76, train_loss =  4.16, learning_rate = 0.00000000


[batch =  400] train_perplexity =    58.57, train_loss =  4.07, learning_rate = 0.00000000


[batch =  500] train_perplexity =    58.15, train_loss =  4.06, learning_rate = 0.00000000


[batch =  600] train_perplexity =    62.15, train_loss =  4.13, learning_rate = 0.00000000


[batch =  700] train_perplexity =    60.04, train_loss =  4.10, learning_rate = 0.00000000


[batch =  800] train_perplexity =    55.21, train_loss =  4.01, learning_rate = 0.00000000


[batch =  900] train_perplexity =    59.82, train_loss =  4.09, learning_rate = 0.00000000


[batch = 1000] train_perplexity =    63.21, train_loss =  4.15, learning_rate = 0.00000000


[batch = 1100] train_perplexity =    53.87, train_loss =  3.99, learning_rate = 0.00000000


[batch = 1200] train_perplexity =    54.74, train_loss =  4.00, learning_rate = 0.00000000


[batch = 1300] train_perplexity =    54.55, train_loss =  4.00, learning_rate = 0.00000000


[epoch = 175] validation_perplexity =    95.56, validation_loss =  4.56

[epoch = 175] annealing learning_rate = 0.00000000

[batch =  100] train_perplexity =    59.50, train_loss =  4.09, learning_rate = 0.00000000


[batch =  200] train_perplexity =    63.74, train_loss =  4.15, learning_rate = 0.00000000


[batch =  300] train_perplexity =    63.73, train_loss =  4.15, learning_rate = 0.00000000


[batch =  400] train_perplexity =    58.72, train_loss =  4.07, learning_rate = 0.00000000


[batch =  500] train_perplexity =    58.49, train_loss =  4.07, learning_rate = 0.00000000


[batch =  600] train_perplexity =    62.18, train_loss =  4.13, learning_rate = 0.00000000


[batch =  700] train_perplexity =    59.60, train_loss =  4.09, learning_rate = 0.00000000


[batch =  800] train_perplexity =    54.82, train_loss =  4.00, learning_rate = 0.00000000


[batch =  900] train_perplexity =    60.03, train_loss =  4.09, learning_rate = 0.00000000


[batch = 1000] train_perplexity =    62.87, train_loss =  4.14, learning_rate = 0.00000000


[batch = 1100] train_perplexity =    53.71, train_loss =  3.98, learning_rate = 0.00000000


[batch = 1200] train_perplexity =    54.55, train_loss =  4.00, learning_rate = 0.00000000


[batch = 1300] train_perplexity =    54.85, train_loss =  4.00, learning_rate = 0.00000000


[epoch = 176] validation_perplexity =    95.56, validation_loss =  4.56

[epoch = 176] annealing learning_rate = 0.00000000

[batch =  100] train_perplexity =    60.04, train_loss =  4.10, learning_rate = 0.00000000


[batch =  200] train_perplexity =    64.18, train_loss =  4.16, learning_rate = 0.00000000


[batch =  300] train_perplexity =    63.71, train_loss =  4.15, learning_rate = 0.00000000


[batch =  400] train_perplexity =    58.41, train_loss =  4.07, learning_rate = 0.00000000


[batch =  500] train_perplexity =    58.13, train_loss =  4.06, learning_rate = 0.00000000


[batch =  600] train_perplexity =    61.96, train_loss =  4.13, learning_rate = 0.00000000


[batch =  700] train_perplexity =    59.90, train_loss =  4.09, learning_rate = 0.00000000


[batch =  800] train_perplexity =    55.09, train_loss =  4.01, learning_rate = 0.00000000


[batch =  900] train_perplexity =    59.88, train_loss =  4.09, learning_rate = 0.00000000


[batch = 1000] train_perplexity =    62.88, train_loss =  4.14, learning_rate = 0.00000000


[batch = 1100] train_perplexity =    53.47, train_loss =  3.98, learning_rate = 0.00000000


[batch = 1200] train_perplexity =    54.46, train_loss =  4.00, learning_rate = 0.00000000


[batch = 1300] train_perplexity =    54.77, train_loss =  4.00, learning_rate = 0.00000000


[epoch = 177] validation_perplexity =    95.56, validation_loss =  4.56

[epoch = 177] annealing learning_rate = 0.00000000

[batch =  100] train_perplexity =    60.08, train_loss =  4.10, learning_rate = 0.00000000


[batch =  200] train_perplexity =    64.40, train_loss =  4.17, learning_rate = 0.00000000


[batch =  300] train_perplexity =    63.48, train_loss =  4.15, learning_rate = 0.00000000


[batch =  400] train_perplexity =    58.64, train_loss =  4.07, learning_rate = 0.00000000


[batch =  500] train_perplexity =    58.39, train_loss =  4.07, learning_rate = 0.00000000


[batch =  600] train_perplexity =    61.98, train_loss =  4.13, learning_rate = 0.00000000


[batch =  700] train_perplexity =    59.85, train_loss =  4.09, learning_rate = 0.00000000


[batch =  800] train_perplexity =    54.76, train_loss =  4.00, learning_rate = 0.00000000


[batch =  900] train_perplexity =    59.80, train_loss =  4.09, learning_rate = 0.00000000


[batch = 1000] train_perplexity =    62.55, train_loss =  4.14, learning_rate = 0.00000000


[batch = 1100] train_perplexity =    53.83, train_loss =  3.99, learning_rate = 0.00000000


[batch = 1200] train_perplexity =    54.36, train_loss =  4.00, learning_rate = 0.00000000


[batch = 1300] train_perplexity =    54.86, train_loss =  4.00, learning_rate = 0.00000000


[epoch = 178] validation_perplexity =    95.56, validation_loss =  4.56

[epoch = 178] annealing learning_rate = 0.00000000

[batch =  100] train_perplexity =    59.82, train_loss =  4.09, learning_rate = 0.00000000


[batch =  200] train_perplexity =    63.85, train_loss =  4.16, learning_rate = 0.00000000


[batch =  300] train_perplexity =    63.68, train_loss =  4.15, learning_rate = 0.00000000


[batch =  400] train_perplexity =    58.38, train_loss =  4.07, learning_rate = 0.00000000


[batch =  500] train_perplexity =    58.12, train_loss =  4.06, learning_rate = 0.00000000


[batch =  600] train_perplexity =    62.11, train_loss =  4.13, learning_rate = 0.00000000


[batch =  700] train_perplexity =    60.12, train_loss =  4.10, learning_rate = 0.00000000


[batch =  800] train_perplexity =    54.95, train_loss =  4.01, learning_rate = 0.00000000


[batch =  900] train_perplexity =    59.92, train_loss =  4.09, learning_rate = 0.00000000


[batch = 1000] train_perplexity =    62.66, train_loss =  4.14, learning_rate = 0.00000000


[batch = 1100] train_perplexity =    53.89, train_loss =  3.99, learning_rate = 0.00000000


[batch = 1200] train_perplexity =    54.50, train_loss =  4.00, learning_rate = 0.00000000


[batch = 1300] train_perplexity =    54.82, train_loss =  4.00, learning_rate = 0.00000000


[epoch = 179] validation_perplexity =    95.56, validation_loss =  4.56

[epoch = 179] annealing learning_rate = 0.00000000

[batch =  100] train_perplexity =    59.87, train_loss =  4.09, learning_rate = 0.00000000


[batch =  200] train_perplexity =    64.07, train_loss =  4.16, learning_rate = 0.00000000


[batch =  300] train_perplexity =    63.76, train_loss =  4.16, learning_rate = 0.00000000


[batch =  400] train_perplexity =    58.44, train_loss =  4.07, learning_rate = 0.00000000


[batch =  500] train_perplexity =    58.18, train_loss =  4.06, learning_rate = 0.00000000


[batch =  600] train_perplexity =    62.33, train_loss =  4.13, learning_rate = 0.00000000


[batch =  700] train_perplexity =    59.95, train_loss =  4.09, learning_rate = 0.00000000


[batch =  800] train_perplexity =    54.71, train_loss =  4.00, learning_rate = 0.00000000


[batch =  900] train_perplexity =    60.21, train_loss =  4.10, learning_rate = 0.00000000


[batch = 1000] train_perplexity =    62.87, train_loss =  4.14, learning_rate = 0.00000000


[batch = 1100] train_perplexity =    53.58, train_loss =  3.98, learning_rate = 0.00000000


[batch = 1200] train_perplexity =    54.25, train_loss =  3.99, learning_rate = 0.00000000


[batch = 1300] train_perplexity =    54.58, train_loss =  4.00, learning_rate = 0.00000000


[epoch = 180] validation_perplexity =    95.56, validation_loss =  4.56

[epoch = 180] annealing learning_rate = 0.00000000

[batch =  100] train_perplexity =    59.73, train_loss =  4.09, learning_rate = 0.00000000


[batch =  200] train_perplexity =    64.53, train_loss =  4.17, learning_rate = 0.00000000


[batch =  300] train_perplexity =    63.83, train_loss =  4.16, learning_rate = 0.00000000


[batch =  400] train_perplexity =    58.62, train_loss =  4.07, learning_rate = 0.00000000


[batch =  500] train_perplexity =    58.28, train_loss =  4.07, learning_rate = 0.00000000


[batch =  600] train_perplexity =    62.20, train_loss =  4.13, learning_rate = 0.00000000


[batch =  700] train_perplexity =    59.97, train_loss =  4.09, learning_rate = 0.00000000


[batch =  800] train_perplexity =    55.25, train_loss =  4.01, learning_rate = 0.00000000


[batch =  900] train_perplexity =    60.12, train_loss =  4.10, learning_rate = 0.00000000


[batch = 1000] train_perplexity =    62.77, train_loss =  4.14, learning_rate = 0.00000000


[batch = 1100] train_perplexity =    53.94, train_loss =  3.99, learning_rate = 0.00000000


[batch = 1200] train_perplexity =    54.52, train_loss =  4.00, learning_rate = 0.00000000


[batch = 1300] train_perplexity =    54.57, train_loss =  4.00, learning_rate = 0.00000000


[epoch = 181] validation_perplexity =    95.56, validation_loss =  4.56

[epoch = 181] annealing learning_rate = 0.00000000

[batch =  100] train_perplexity =    59.87, train_loss =  4.09, learning_rate = 0.00000000


[batch =  200] train_perplexity =    64.12, train_loss =  4.16, learning_rate = 0.00000000


[batch =  300] train_perplexity =    63.80, train_loss =  4.16, learning_rate = 0.00000000


[batch =  400] train_perplexity =    58.46, train_loss =  4.07, learning_rate = 0.00000000


[batch =  500] train_perplexity =    58.22, train_loss =  4.06, learning_rate = 0.00000000


[batch =  600] train_perplexity =    62.28, train_loss =  4.13, learning_rate = 0.00000000


[batch =  700] train_perplexity =    59.71, train_loss =  4.09, learning_rate = 0.00000000


[batch =  800] train_perplexity =    54.97, train_loss =  4.01, learning_rate = 0.00000000


[batch =  900] train_perplexity =    60.13, train_loss =  4.10, learning_rate = 0.00000000


[batch = 1000] train_perplexity =    62.84, train_loss =  4.14, learning_rate = 0.00000000


[batch = 1100] train_perplexity =    53.81, train_loss =  3.99, learning_rate = 0.00000000


[batch = 1200] train_perplexity =    54.43, train_loss =  4.00, learning_rate = 0.00000000


[batch = 1300] train_perplexity =    54.81, train_loss =  4.00, learning_rate = 0.00000000


[epoch = 182] validation_perplexity =    95.56, validation_loss =  4.56

[epoch = 182] annealing learning_rate = 0.00000000

[batch =  100] train_perplexity =    60.12, train_loss =  4.10, learning_rate = 0.00000000


[batch =  200] train_perplexity =    64.08, train_loss =  4.16, learning_rate = 0.00000000


[batch =  300] train_perplexity =    63.97, train_loss =  4.16, learning_rate = 0.00000000


[batch =  400] train_perplexity =    58.41, train_loss =  4.07, learning_rate = 0.00000000


[batch =  500] train_perplexity =    57.98, train_loss =  4.06, learning_rate = 0.00000000


[batch =  600] train_perplexity =    61.82, train_loss =  4.12, learning_rate = 0.00000000


[batch =  700] train_perplexity =    59.79, train_loss =  4.09, learning_rate = 0.00000000


[batch =  800] train_perplexity =    54.99, train_loss =  4.01, learning_rate = 0.00000000


[batch =  900] train_perplexity =    59.76, train_loss =  4.09, learning_rate = 0.00000000


[batch = 1000] train_perplexity =    63.21, train_loss =  4.15, learning_rate = 0.00000000


[batch = 1100] train_perplexity =    53.67, train_loss =  3.98, learning_rate = 0.00000000


[batch = 1200] train_perplexity =    54.32, train_loss =  3.99, learning_rate = 0.00000000


[batch = 1300] train_perplexity =    54.86, train_loss =  4.00, learning_rate = 0.00000000


[epoch = 183] validation_perplexity =    95.56, validation_loss =  4.56

[epoch = 183] annealing learning_rate = 0.00000000

[batch =  100] train_perplexity =    59.69, train_loss =  4.09, learning_rate = 0.00000000


[batch =  200] train_perplexity =    64.09, train_loss =  4.16, learning_rate = 0.00000000


[batch =  300] train_perplexity =    63.93, train_loss =  4.16, learning_rate = 0.00000000


[batch =  400] train_perplexity =    58.72, train_loss =  4.07, learning_rate = 0.00000000


[batch =  500] train_perplexity =    57.95, train_loss =  4.06, learning_rate = 0.00000000


[batch =  600] train_perplexity =    62.18, train_loss =  4.13, learning_rate = 0.00000000


[batch =  700] train_perplexity =    59.48, train_loss =  4.09, learning_rate = 0.00000000


[batch =  800] train_perplexity =    55.12, train_loss =  4.01, learning_rate = 0.00000000


[batch =  900] train_perplexity =    59.90, train_loss =  4.09, learning_rate = 0.00000000


[batch = 1000] train_perplexity =    62.66, train_loss =  4.14, learning_rate = 0.00000000


[batch = 1100] train_perplexity =    53.65, train_loss =  3.98, learning_rate = 0.00000000


[batch = 1200] train_perplexity =    54.24, train_loss =  3.99, learning_rate = 0.00000000


[batch = 1300] train_perplexity =    54.91, train_loss =  4.01, learning_rate = 0.00000000


[epoch = 184] validation_perplexity =    95.56, validation_loss =  4.56

[epoch = 184] annealing learning_rate = 0.00000000

[batch =  100] train_perplexity =    59.87, train_loss =  4.09, learning_rate = 0.00000000


[batch =  200] train_perplexity =    63.61, train_loss =  4.15, learning_rate = 0.00000000


[batch =  300] train_perplexity =    63.91, train_loss =  4.16, learning_rate = 0.00000000


[batch =  400] train_perplexity =    58.04, train_loss =  4.06, learning_rate = 0.00000000


[batch =  500] train_perplexity =    58.63, train_loss =  4.07, learning_rate = 0.00000000


[batch =  600] train_perplexity =    62.10, train_loss =  4.13, learning_rate = 0.00000000


[batch =  700] train_perplexity =    59.73, train_loss =  4.09, learning_rate = 0.00000000


[batch =  800] train_perplexity =    54.84, train_loss =  4.00, learning_rate = 0.00000000


[batch =  900] train_perplexity =    60.22, train_loss =  4.10, learning_rate = 0.00000000


[batch = 1000] train_perplexity =    63.11, train_loss =  4.14, learning_rate = 0.00000000


[batch = 1100] train_perplexity =    53.62, train_loss =  3.98, learning_rate = 0.00000000


[batch = 1200] train_perplexity =    54.39, train_loss =  4.00, learning_rate = 0.00000000


[batch = 1300] train_perplexity =    54.75, train_loss =  4.00, learning_rate = 0.00000000


[epoch = 185] validation_perplexity =    95.56, validation_loss =  4.56

[epoch = 185] annealing learning_rate = 0.00000000

[batch =  100] train_perplexity =    59.51, train_loss =  4.09, learning_rate = 0.00000000


[batch =  200] train_perplexity =    64.25, train_loss =  4.16, learning_rate = 0.00000000


[batch =  300] train_perplexity =    63.89, train_loss =  4.16, learning_rate = 0.00000000


[batch =  400] train_perplexity =    58.68, train_loss =  4.07, learning_rate = 0.00000000


[batch =  500] train_perplexity =    58.58, train_loss =  4.07, learning_rate = 0.00000000


[batch =  600] train_perplexity =    62.20, train_loss =  4.13, learning_rate = 0.00000000


[batch =  700] train_perplexity =    59.90, train_loss =  4.09, learning_rate = 0.00000000


[batch =  800] train_perplexity =    54.91, train_loss =  4.01, learning_rate = 0.00000000


[batch =  900] train_perplexity =    60.02, train_loss =  4.09, learning_rate = 0.00000000


[batch = 1000] train_perplexity =    63.03, train_loss =  4.14, learning_rate = 0.00000000


[batch = 1100] train_perplexity =    53.47, train_loss =  3.98, learning_rate = 0.00000000


[batch = 1200] train_perplexity =    54.33, train_loss =  4.00, learning_rate = 0.00000000


[batch = 1300] train_perplexity =    55.02, train_loss =  4.01, learning_rate = 0.00000000


[epoch = 186] validation_perplexity =    95.56, validation_loss =  4.56

[epoch = 186] annealing learning_rate = 0.00000000

[batch =  100] train_perplexity =    59.93, train_loss =  4.09, learning_rate = 0.00000000


[batch =  200] train_perplexity =    64.19, train_loss =  4.16, learning_rate = 0.00000000


[batch =  300] train_perplexity =    64.04, train_loss =  4.16, learning_rate = 0.00000000


[batch =  400] train_perplexity =    58.58, train_loss =  4.07, learning_rate = 0.00000000


[batch =  500] train_perplexity =    58.38, train_loss =  4.07, learning_rate = 0.00000000


[batch =  600] train_perplexity =    62.20, train_loss =  4.13, learning_rate = 0.00000000


[batch =  700] train_perplexity =    59.89, train_loss =  4.09, learning_rate = 0.00000000


[batch =  800] train_perplexity =    55.06, train_loss =  4.01, learning_rate = 0.00000000


[batch =  900] train_perplexity =    60.43, train_loss =  4.10, learning_rate = 0.00000000


[batch = 1000] train_perplexity =    63.07, train_loss =  4.14, learning_rate = 0.00000000


[batch = 1100] train_perplexity =    53.43, train_loss =  3.98, learning_rate = 0.00000000


[batch = 1200] train_perplexity =    54.17, train_loss =  3.99, learning_rate = 0.00000000


[batch = 1300] train_perplexity =    54.68, train_loss =  4.00, learning_rate = 0.00000000


[epoch = 187] validation_perplexity =    95.56, validation_loss =  4.56

[epoch = 187] annealing learning_rate = 0.00000000

[batch =  100] train_perplexity =    59.71, train_loss =  4.09, learning_rate = 0.00000000


[batch =  200] train_perplexity =    63.69, train_loss =  4.15, learning_rate = 0.00000000


[batch =  300] train_perplexity =    63.78, train_loss =  4.16, learning_rate = 0.00000000


[batch =  400] train_perplexity =    58.60, train_loss =  4.07, learning_rate = 0.00000000


[batch =  500] train_perplexity =    58.34, train_loss =  4.07, learning_rate = 0.00000000


[batch =  600] train_perplexity =    61.98, train_loss =  4.13, learning_rate = 0.00000000


[batch =  700] train_perplexity =    59.90, train_loss =  4.09, learning_rate = 0.00000000


[batch =  800] train_perplexity =    55.07, train_loss =  4.01, learning_rate = 0.00000000


[batch =  900] train_perplexity =    59.82, train_loss =  4.09, learning_rate = 0.00000000


[batch = 1000] train_perplexity =    62.34, train_loss =  4.13, learning_rate = 0.00000000


[batch = 1100] train_perplexity =    53.65, train_loss =  3.98, learning_rate = 0.00000000


[batch = 1200] train_perplexity =    54.32, train_loss =  3.99, learning_rate = 0.00000000


[batch = 1300] train_perplexity =    54.92, train_loss =  4.01, learning_rate = 0.00000000


[epoch = 188] validation_perplexity =    95.56, validation_loss =  4.56

[epoch = 188] annealing learning_rate = 0.00000000

[batch =  100] train_perplexity =    59.59, train_loss =  4.09, learning_rate = 0.00000000


[batch =  200] train_perplexity =    63.77, train_loss =  4.16, learning_rate = 0.00000000


[batch =  300] train_perplexity =    64.03, train_loss =  4.16, learning_rate = 0.00000000


[batch =  400] train_perplexity =    58.78, train_loss =  4.07, learning_rate = 0.00000000


[batch =  500] train_perplexity =    58.24, train_loss =  4.06, learning_rate = 0.00000000


[batch =  600] train_perplexity =    62.14, train_loss =  4.13, learning_rate = 0.00000000


[batch =  700] train_perplexity =    59.88, train_loss =  4.09, learning_rate = 0.00000000


[batch =  800] train_perplexity =    54.85, train_loss =  4.00, learning_rate = 0.00000000


[batch =  900] train_perplexity =    60.14, train_loss =  4.10, learning_rate = 0.00000000


[batch = 1000] train_perplexity =    62.93, train_loss =  4.14, learning_rate = 0.00000000


[batch = 1100] train_perplexity =    54.08, train_loss =  3.99, learning_rate = 0.00000000


[batch = 1200] train_perplexity =    54.41, train_loss =  4.00, learning_rate = 0.00000000


[batch = 1300] train_perplexity =    54.84, train_loss =  4.00, learning_rate = 0.00000000


[epoch = 189] validation_perplexity =    95.56, validation_loss =  4.56

[epoch = 189] annealing learning_rate = 0.00000000

[batch =  100] train_perplexity =    59.72, train_loss =  4.09, learning_rate = 0.00000000


[batch =  200] train_perplexity =    63.86, train_loss =  4.16, learning_rate = 0.00000000


[batch =  300] train_perplexity =    63.74, train_loss =  4.15, learning_rate = 0.00000000


[batch =  400] train_perplexity =    58.61, train_loss =  4.07, learning_rate = 0.00000000


[batch =  500] train_perplexity =    58.77, train_loss =  4.07, learning_rate = 0.00000000


[batch =  600] train_perplexity =    62.05, train_loss =  4.13, learning_rate = 0.00000000


[batch =  700] train_perplexity =    60.05, train_loss =  4.10, learning_rate = 0.00000000


[batch =  800] train_perplexity =    55.20, train_loss =  4.01, learning_rate = 0.00000000


[batch =  900] train_perplexity =    60.04, train_loss =  4.09, learning_rate = 0.00000000


[batch = 1000] train_perplexity =    62.94, train_loss =  4.14, learning_rate = 0.00000000


[batch = 1100] train_perplexity =    53.64, train_loss =  3.98, learning_rate = 0.00000000


[batch = 1200] train_perplexity =    54.23, train_loss =  3.99, learning_rate = 0.00000000


[batch = 1300] train_perplexity =    54.66, train_loss =  4.00, learning_rate = 0.00000000


[epoch = 190] validation_perplexity =    95.56, validation_loss =  4.56

[epoch = 190] annealing learning_rate = 0.00000000

[batch =  100] train_perplexity =    59.73, train_loss =  4.09, learning_rate = 0.00000000


[batch =  200] train_perplexity =    63.99, train_loss =  4.16, learning_rate = 0.00000000


[batch =  300] train_perplexity =    64.21, train_loss =  4.16, learning_rate = 0.00000000


[batch =  400] train_perplexity =    58.28, train_loss =  4.07, learning_rate = 0.00000000


[batch =  500] train_perplexity =    58.04, train_loss =  4.06, learning_rate = 0.00000000


[batch =  600] train_perplexity =    62.28, train_loss =  4.13, learning_rate = 0.00000000


[batch =  700] train_perplexity =    59.47, train_loss =  4.09, learning_rate = 0.00000000


[batch =  800] train_perplexity =    54.81, train_loss =  4.00, learning_rate = 0.00000000


[batch =  900] train_perplexity =    60.00, train_loss =  4.09, learning_rate = 0.00000000


[batch = 1000] train_perplexity =    62.68, train_loss =  4.14, learning_rate = 0.00000000


[batch = 1100] train_perplexity =    53.99, train_loss =  3.99, learning_rate = 0.00000000


[batch = 1200] train_perplexity =    54.26, train_loss =  3.99, learning_rate = 0.00000000


[batch = 1300] train_perplexity =    54.85, train_loss =  4.00, learning_rate = 0.00000000


[epoch = 191] validation_perplexity =    95.56, validation_loss =  4.56

[epoch = 191] annealing learning_rate = 0.00000000

[batch =  100] train_perplexity =    59.86, train_loss =  4.09, learning_rate = 0.00000000


[batch =  200] train_perplexity =    63.79, train_loss =  4.16, learning_rate = 0.00000000


[batch =  300] train_perplexity =    63.84, train_loss =  4.16, learning_rate = 0.00000000


[batch =  400] train_perplexity =    58.58, train_loss =  4.07, learning_rate = 0.00000000


[batch =  500] train_perplexity =    58.62, train_loss =  4.07, learning_rate = 0.00000000


[batch =  600] train_perplexity =    62.34, train_loss =  4.13, learning_rate = 0.00000000


[batch =  700] train_perplexity =    60.06, train_loss =  4.10, learning_rate = 0.00000000


[batch =  800] train_perplexity =    54.95, train_loss =  4.01, learning_rate = 0.00000000


[batch =  900] train_perplexity =    60.25, train_loss =  4.10, learning_rate = 0.00000000


[batch = 1000] train_perplexity =    62.90, train_loss =  4.14, learning_rate = 0.00000000


[batch = 1100] train_perplexity =    53.82, train_loss =  3.99, learning_rate = 0.00000000


[batch = 1200] train_perplexity =    54.55, train_loss =  4.00, learning_rate = 0.00000000


[batch = 1300] train_perplexity =    55.02, train_loss =  4.01, learning_rate = 0.00000000


[epoch = 192] validation_perplexity =    95.56, validation_loss =  4.56

[epoch = 192] annealing learning_rate = 0.00000000

[batch =  100] train_perplexity =    59.79, train_loss =  4.09, learning_rate = 0.00000000


[batch =  200] train_perplexity =    64.15, train_loss =  4.16, learning_rate = 0.00000000


[batch =  300] train_perplexity =    63.70, train_loss =  4.15, learning_rate = 0.00000000


[batch =  400] train_perplexity =    58.41, train_loss =  4.07, learning_rate = 0.00000000


[batch =  500] train_perplexity =    58.36, train_loss =  4.07, learning_rate = 0.00000000


[batch =  600] train_perplexity =    62.20, train_loss =  4.13, learning_rate = 0.00000000


[batch =  700] train_perplexity =    59.54, train_loss =  4.09, learning_rate = 0.00000000


[batch =  800] train_perplexity =    54.83, train_loss =  4.00, learning_rate = 0.00000000


[batch =  900] train_perplexity =    59.95, train_loss =  4.09, learning_rate = 0.00000000


[batch = 1000] train_perplexity =    63.00, train_loss =  4.14, learning_rate = 0.00000000


[batch = 1100] train_perplexity =    53.66, train_loss =  3.98, learning_rate = 0.00000000


[batch = 1200] train_perplexity =    54.55, train_loss =  4.00, learning_rate = 0.00000000


[batch = 1300] train_perplexity =    54.78, train_loss =  4.00, learning_rate = 0.00000000


[epoch = 193] validation_perplexity =    95.56, validation_loss =  4.56

[epoch = 193] annealing learning_rate = 0.00000000

[batch =  100] train_perplexity =    59.63, train_loss =  4.09, learning_rate = 0.00000000


[batch =  200] train_perplexity =    64.03, train_loss =  4.16, learning_rate = 0.00000000


[batch =  300] train_perplexity =    64.05, train_loss =  4.16, learning_rate = 0.00000000


[batch =  400] train_perplexity =    58.60, train_loss =  4.07, learning_rate = 0.00000000


[batch =  500] train_perplexity =    58.11, train_loss =  4.06, learning_rate = 0.00000000


[batch =  600] train_perplexity =    62.23, train_loss =  4.13, learning_rate = 0.00000000


[batch =  700] train_perplexity =    59.71, train_loss =  4.09, learning_rate = 0.00000000


[batch =  800] train_perplexity =    55.16, train_loss =  4.01, learning_rate = 0.00000000


[batch =  900] train_perplexity =    59.71, train_loss =  4.09, learning_rate = 0.00000000


[batch = 1000] train_perplexity =    62.89, train_loss =  4.14, learning_rate = 0.00000000


[batch = 1100] train_perplexity =    53.82, train_loss =  3.99, learning_rate = 0.00000000


[batch = 1200] train_perplexity =    54.45, train_loss =  4.00, learning_rate = 0.00000000


[batch = 1300] train_perplexity =    54.68, train_loss =  4.00, learning_rate = 0.00000000


[epoch = 194] validation_perplexity =    95.56, validation_loss =  4.56

[epoch = 194] annealing learning_rate = 0.00000000

[batch =  100] train_perplexity =    59.93, train_loss =  4.09, learning_rate = 0.00000000


[batch =  200] train_perplexity =    64.26, train_loss =  4.16, learning_rate = 0.00000000


[batch =  300] train_perplexity =    64.14, train_loss =  4.16, learning_rate = 0.00000000


[batch =  400] train_perplexity =    58.53, train_loss =  4.07, learning_rate = 0.00000000


[batch =  500] train_perplexity =    58.23, train_loss =  4.06, learning_rate = 0.00000000


[batch =  600] train_perplexity =    62.55, train_loss =  4.14, learning_rate = 0.00000000


[batch =  700] train_perplexity =    60.20, train_loss =  4.10, learning_rate = 0.00000000


[batch =  800] train_perplexity =    55.08, train_loss =  4.01, learning_rate = 0.00000000


[batch =  900] train_perplexity =    60.00, train_loss =  4.09, learning_rate = 0.00000000


[batch = 1000] train_perplexity =    62.80, train_loss =  4.14, learning_rate = 0.00000000


[batch = 1100] train_perplexity =    53.53, train_loss =  3.98, learning_rate = 0.00000000


[batch = 1200] train_perplexity =    54.37, train_loss =  4.00, learning_rate = 0.00000000


[batch = 1300] train_perplexity =    54.84, train_loss =  4.00, learning_rate = 0.00000000


[epoch = 195] validation_perplexity =    95.56, validation_loss =  4.56

[epoch = 195] annealing learning_rate = 0.00000000

[batch =  100] train_perplexity =    59.85, train_loss =  4.09, learning_rate = 0.00000000


[batch =  200] train_perplexity =    64.09, train_loss =  4.16, learning_rate = 0.00000000


[batch =  300] train_perplexity =    63.99, train_loss =  4.16, learning_rate = 0.00000000


[batch =  400] train_perplexity =    58.57, train_loss =  4.07, learning_rate = 0.00000000


[batch =  500] train_perplexity =    58.21, train_loss =  4.06, learning_rate = 0.00000000


[batch =  600] train_perplexity =    62.10, train_loss =  4.13, learning_rate = 0.00000000


[batch =  700] train_perplexity =    59.86, train_loss =  4.09, learning_rate = 0.00000000


[batch =  800] train_perplexity =    55.05, train_loss =  4.01, learning_rate = 0.00000000


[batch =  900] train_perplexity =    59.83, train_loss =  4.09, learning_rate = 0.00000000


[batch = 1000] train_perplexity =    62.77, train_loss =  4.14, learning_rate = 0.00000000


[batch = 1100] train_perplexity =    53.79, train_loss =  3.99, learning_rate = 0.00000000


[batch = 1200] train_perplexity =    54.23, train_loss =  3.99, learning_rate = 0.00000000


[batch = 1300] train_perplexity =    54.97, train_loss =  4.01, learning_rate = 0.00000000


[epoch = 196] validation_perplexity =    95.56, validation_loss =  4.56

[epoch = 196] annealing learning_rate = 0.00000000

[batch =  100] train_perplexity =    59.53, train_loss =  4.09, learning_rate = 0.00000000


[batch =  200] train_perplexity =    63.93, train_loss =  4.16, learning_rate = 0.00000000


[batch =  300] train_perplexity =    63.74, train_loss =  4.15, learning_rate = 0.00000000


[batch =  400] train_perplexity =    58.44, train_loss =  4.07, learning_rate = 0.00000000


[batch =  500] train_perplexity =    58.26, train_loss =  4.06, learning_rate = 0.00000000


[batch =  600] train_perplexity =    61.87, train_loss =  4.13, learning_rate = 0.00000000


[batch =  700] train_perplexity =    59.77, train_loss =  4.09, learning_rate = 0.00000000


[batch =  800] train_perplexity =    54.92, train_loss =  4.01, learning_rate = 0.00000000


[batch =  900] train_perplexity =    59.75, train_loss =  4.09, learning_rate = 0.00000000


[batch = 1000] train_perplexity =    62.87, train_loss =  4.14, learning_rate = 0.00000000


[batch = 1100] train_perplexity =    53.74, train_loss =  3.98, learning_rate = 0.00000000


[batch = 1200] train_perplexity =    54.24, train_loss =  3.99, learning_rate = 0.00000000


[batch = 1300] train_perplexity =    54.78, train_loss =  4.00, learning_rate = 0.00000000


[epoch = 197] validation_perplexity =    95.56, validation_loss =  4.56

[epoch = 197] annealing learning_rate = 0.00000000

[batch =  100] train_perplexity =    59.74, train_loss =  4.09, learning_rate = 0.00000000


[batch =  200] train_perplexity =    64.00, train_loss =  4.16, learning_rate = 0.00000000


[batch =  300] train_perplexity =    63.81, train_loss =  4.16, learning_rate = 0.00000000


[batch =  400] train_perplexity =    58.47, train_loss =  4.07, learning_rate = 0.00000000


[batch =  500] train_perplexity =    58.38, train_loss =  4.07, learning_rate = 0.00000000


[batch =  600] train_perplexity =    62.28, train_loss =  4.13, learning_rate = 0.00000000


[batch =  700] train_perplexity =    60.00, train_loss =  4.09, learning_rate = 0.00000000


[batch =  800] train_perplexity =    55.05, train_loss =  4.01, learning_rate = 0.00000000


[batch =  900] train_perplexity =    59.96, train_loss =  4.09, learning_rate = 0.00000000


[batch = 1000] train_perplexity =    62.70, train_loss =  4.14, learning_rate = 0.00000000


[batch = 1100] train_perplexity =    53.68, train_loss =  3.98, learning_rate = 0.00000000


[batch = 1200] train_perplexity =    54.60, train_loss =  4.00, learning_rate = 0.00000000


[batch = 1300] train_perplexity =    54.98, train_loss =  4.01, learning_rate = 0.00000000


[epoch = 198] validation_perplexity =    95.56, validation_loss =  4.56

[epoch = 198] annealing learning_rate = 0.00000000

[batch =  100] train_perplexity =    59.58, train_loss =  4.09, learning_rate = 0.00000000


[batch =  200] train_perplexity =    63.93, train_loss =  4.16, learning_rate = 0.00000000


[batch =  300] train_perplexity =    63.53, train_loss =  4.15, learning_rate = 0.00000000


[batch =  400] train_perplexity =    58.61, train_loss =  4.07, learning_rate = 0.00000000


[batch =  500] train_perplexity =    58.54, train_loss =  4.07, learning_rate = 0.00000000


[batch =  600] train_perplexity =    62.16, train_loss =  4.13, learning_rate = 0.00000000


[batch =  700] train_perplexity =    60.04, train_loss =  4.10, learning_rate = 0.00000000


[batch =  800] train_perplexity =    54.86, train_loss =  4.00, learning_rate = 0.00000000


[batch =  900] train_perplexity =    59.84, train_loss =  4.09, learning_rate = 0.00000000


[batch = 1000] train_perplexity =    62.89, train_loss =  4.14, learning_rate = 0.00000000


[batch = 1100] train_perplexity =    53.64, train_loss =  3.98, learning_rate = 0.00000000


[batch = 1200] train_perplexity =    54.34, train_loss =  4.00, learning_rate = 0.00000000


[batch = 1300] train_perplexity =    54.92, train_loss =  4.01, learning_rate = 0.00000000


[epoch = 199] validation_perplexity =    95.56, validation_loss =  4.56

[epoch = 199] annealing learning_rate = 0.00000000

[batch =  100] train_perplexity =    59.57, train_loss =  4.09, learning_rate = 0.00000000


[batch =  200] train_perplexity =    64.38, train_loss =  4.16, learning_rate = 0.00000000


[batch =  300] train_perplexity =    63.89, train_loss =  4.16, learning_rate = 0.00000000


[batch =  400] train_perplexity =    58.33, train_loss =  4.07, learning_rate = 0.00000000


[batch =  500] train_perplexity =    58.29, train_loss =  4.07, learning_rate = 0.00000000


[batch =  600] train_perplexity =    62.18, train_loss =  4.13, learning_rate = 0.00000000


[batch =  700] train_perplexity =    59.86, train_loss =  4.09, learning_rate = 0.00000000


[batch =  800] train_perplexity =    54.80, train_loss =  4.00, learning_rate = 0.00000000


[batch =  900] train_perplexity =    60.11, train_loss =  4.10, learning_rate = 0.00000000


[batch = 1000] train_perplexity =    62.82, train_loss =  4.14, learning_rate = 0.00000000


[batch = 1100] train_perplexity =    53.94, train_loss =  3.99, learning_rate = 0.00000000


[batch = 1200] train_perplexity =    54.36, train_loss =  4.00, learning_rate = 0.00000000


[batch = 1300] train_perplexity =    54.82, train_loss =  4.00, learning_rate = 0.00000000


[epoch = 200] validation_perplexity =    95.56, validation_loss =  4.56

[epoch = 200] annealing learning_rate = 0.00000000
Test validation:

test_perplexity =    91.59, test_loss =  4.52
